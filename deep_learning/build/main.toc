\contentsline {chapter}{\numberline {第一章\hspace {.3em}}神经网络与深度学习基础}{6}{chapter.1}%
\contentsline {section}{\numberline {1.1}启发性思考}{6}{section.1.1}%
\contentsline {section}{\numberline {1.2}神经元：深度学习的基石}{6}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}数学模型}{6}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}关键组件详解}{6}{subsection.1.2.2}%
\contentsline {section}{\numberline {1.3}激活函数：从Sigmoid到ReLU}{7}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}经典激活函数}{7}{subsection.1.3.1}%
\contentsline {subsubsection}{Sigmoid函数}{7}{subsubsection*.2}%
\contentsline {subsubsection}{双曲正切函数（tanh）}{7}{subsubsection*.3}%
\contentsline {subsubsection}{线性整流单元（ReLU）}{8}{subsubsection*.4}%
\contentsline {subsection}{\numberline {1.3.2}激活函数对比与进阶选择}{8}{subsection.1.3.2}%
\contentsline {subsubsection}{ReLU的改进变体}{8}{subsubsection*.6}%
\contentsline {section}{\numberline {1.4}前馈神经网络：层叠的威力}{9}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}网络结构}{9}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}优雅高效的矩阵表示}{9}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Softmax函数：多分类的“裁判”}{9}{subsection.1.4.3}%
\contentsline {subsubsection}{标准Softmax}{9}{subsubsection*.7}%
\contentsline {subsubsection}{温度系数（Temperature）}{9}{subsubsection*.8}%
\contentsline {section}{\numberline {1.5}神经网络的训练：让模型学会思考}{10}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}步骤一：定义网络结构}{10}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}步骤二：定义损失函数（Loss Function）}{10}{subsection.1.5.2}%
\contentsline {subsection}{\numberline {1.5.3}步骤三：梯度下降优化}{11}{subsection.1.5.3}%
\contentsline {section}{\numberline {1.6}反向传播：深度学习的引擎}{11}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}误差项（Error Term）的定义}{11}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}反向传播的四步流程}{11}{subsection.1.6.2}%
\contentsline {subsubsection}{1. 前向传播}{11}{subsubsection*.9}%
\contentsline {subsubsection}{2. 计算输出层误差}{11}{subsubsection*.10}%
\contentsline {subsubsection}{3. 反向传播误差（关键递推公式）}{12}{subsubsection*.11}%
\contentsline {subsubsection}{4. 计算参数梯度}{12}{subsubsection*.12}%
\contentsline {subsubsection}{5. 更新参数}{12}{subsubsection*.13}%
\contentsline {subsubsection}{6. 批处理与梯度累加}{12}{subsubsection*.14}%
\contentsline {subsection}{\numberline {1.6.3}反向传播推导（选读）}{12}{subsection.1.6.3}%
\contentsline {section}{\numberline {1.7}引言：从拟合数据到泛化世界}{13}{section.1.7}%
\contentsline {section}{\numberline {1.8}模型性能评估：不只是看分数}{13}{section.1.8}%
\contentsline {subsection}{\numberline {1.8.1}数据集划分的艺术}{13}{subsection.1.8.1}%
\contentsline {subsection}{\numberline {1.8.2}K-折交叉验证：小数据集的救星}{14}{subsection.1.8.2}%
\contentsline {section}{\numberline {1.9}诊断模型问题：欠拟合与过拟合}{14}{section.1.9}%
\contentsline {subsection}{\numberline {1.9.1}欠拟合（Underfitting）}{14}{subsection.1.9.1}%
\contentsline {subsection}{\numberline {1.9.2}过拟合（Overfitting）}{15}{subsection.1.9.2}%
\contentsline {subsection}{\numberline {1.9.3}偏差-方差权衡：统计学习理论的视角}{15}{subsection.1.9.3}%
\contentsline {section}{\numberline {1.10}正则化技术：防止过拟合的利器}{15}{section.1.10}%
\contentsline {subsection}{\numberline {1.10.1}L1和L2正则化}{15}{subsection.1.10.1}%
\contentsline {subsubsection}{几何解释}{16}{subsubsection*.17}%
\contentsline {subsection}{\numberline {1.10.2}Dropout：训练时随机丢弃神经元}{16}{subsection.1.10.2}%
\contentsline {subsection}{\numberline {1.10.3}早停法（Early Stopping）}{16}{subsection.1.10.3}%
\contentsline {section}{\numberline {1.11}从神经网络到深度学习：历史脉络}{17}{section.1.11}%
\contentsline {subsection}{\numberline {1.11.1}发展历程}{17}{subsection.1.11.1}%
\contentsline {subsubsection}{早期探索（1940s-1980s）}{17}{subsubsection*.20}%
\contentsline {subsubsection}{复兴前夜（1980s-2006）}{17}{subsubsection*.21}%
\contentsline {subsubsection}{复兴之年（2006）}{17}{subsubsection*.22}%
\contentsline {subsubsection}{爆发之年（2012）}{17}{subsubsection*.23}%
\contentsline {subsection}{\numberline {1.11.2}深度学习的四大支柱}{18}{subsection.1.11.2}%
\contentsline {subsection}{\numberline {1.11.3}为什么叫"深度学习"？}{18}{subsection.1.11.3}%
\contentsline {section}{\numberline {1.12}关键模型架构}{19}{section.1.12}%
\contentsline {subsection}{\numberline {1.12.1}自编码器（Autoencoders, AEs）}{19}{subsection.1.12.1}%
\contentsline {subsection}{\numberline {1.12.2}卷积神经网络（CNN）}{19}{subsection.1.12.2}%
\contentsline {subsubsection}{核心组件}{19}{subsubsection*.28}%
\contentsline {subsubsection}{经典架构}{20}{subsubsection*.29}%
\contentsline {subsection}{\numberline {1.12.3}循环神经网络（RNN）与LSTM}{20}{subsection.1.12.3}%
\contentsline {subsubsection}{标准RNN}{20}{subsubsection*.30}%
\contentsline {subsubsection}{LSTM：长短期记忆网络}{20}{subsubsection*.32}%
\contentsline {subsection}{\numberline {1.12.4}生成对抗网络（GANs）}{21}{subsection.1.12.4}%
\contentsline {subsection}{\numberline {1.12.5}扩散模型（Diffusion Models）}{22}{subsection.1.12.5}%
\contentsline {section}{\numberline {1.13}Transformer：注意力就是全部}{22}{section.1.13}%
\contentsline {subsection}{\numberline {1.13.1}自注意力机制（Self-Attention）}{22}{subsection.1.13.1}%
\contentsline {subsection}{\numberline {1.13.2}多头注意力（Multi-Head Attention）}{23}{subsection.1.13.2}%
\contentsline {subsection}{\numberline {1.13.3}Transformer架构}{23}{subsection.1.13.3}%
\contentsline {subsubsection}{编码器（Encoder）}{23}{subsubsection*.38}%
\contentsline {subsubsection}{解码器（Decoder）}{23}{subsubsection*.39}%
\contentsline {subsection}{\numberline {1.13.4}位置编码（Positional Encoding）}{24}{subsection.1.13.4}%
\contentsline {subsection}{\numberline {1.13.5}Transformer vs RNN/LSTM}{24}{subsection.1.13.5}%
\contentsline {section}{\numberline {1.14}为什么Transformer能够"通吃"？}{25}{section.1.14}%
\contentsline {subsection}{\numberline {1.14.1}从专家模型到通用模型}{25}{subsection.1.14.1}%
\contentsline {subsection}{\numberline {1.14.2}Transformer的核心优势}{25}{subsection.1.14.2}%
\contentsline {section}{\numberline {1.15}总结与思考}{25}{section.1.15}%
\contentsline {chapter}{\numberline {第二章\hspace {.3em}}深度强化学习之基础}{26}{chapter.2}%
\contentsline {section}{\numberline {2.1}引言：从有答案的学习到探索的学习}{26}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}监督学习的辉煌与局限}{26}{subsection.2.1.1}%
\contentsline {subsubsection}{监督学习的核心思想}{26}{subsubsection*.41}%
\contentsline {subsubsection}{监督学习的成功案例}{26}{subsubsection*.42}%
\contentsline {subsubsection}{监督学习的局限性}{26}{subsubsection*.43}%
\contentsline {subsection}{\numberline {2.1.2}强化学习：一种新的学习范式}{27}{subsection.2.1.2}%
\contentsline {subsubsection}{从"知道答案"到"探索答案"}{27}{subsubsection*.45}%
\contentsline {subsubsection}{人类学习与强化学习的类比}{28}{subsubsection*.47}%
\contentsline {section}{\numberline {2.2}马尔可夫决策过程：强化学习的数学基础}{28}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}马尔可夫决策过程的基本概念}{28}{subsection.2.2.1}%
\contentsline {subsubsection}{MDP的五个核心要素}{28}{subsubsection*.50}%
\contentsline {subsection}{\numberline {2.2.2}MDP的核心概念}{29}{subsection.2.2.2}%
\contentsline {subsubsection}{策略函数 $\pi $}{29}{subsubsection*.51}%
\contentsline {subsubsection}{轨迹与回报}{30}{subsubsection*.52}%
\contentsline {subsubsection}{Rollout（或轨迹采样）}{30}{subsubsection*.53}%
\contentsline {section}{\numberline {2.3}价值函数：评估策略的优劣}{30}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}价值函数的定义与意义}{30}{subsection.2.3.1}%
\contentsline {subsubsection}{状态价值函数 $V_\pi (s)$}{30}{subsubsection*.54}%
\contentsline {subsubsection}{动作价值函数 $Q_\pi (s, a)$}{31}{subsubsection*.55}%
\contentsline {subsection}{\numberline {2.3.2}价值函数的关系}{31}{subsection.2.3.2}%
\contentsline {subsubsection}{从 $Q_\pi $ 到 $V_\pi $}{31}{subsubsection*.56}%
\contentsline {subsubsection}{从 $V_\pi $ 到 $Q_\pi $}{31}{subsubsection*.57}%
\contentsline {subsection}{\numberline {2.3.3}最优价值函数}{31}{subsection.2.3.3}%
\contentsline {subsubsection}{最优状态价值函数}{32}{subsubsection*.58}%
\contentsline {subsubsection}{最优动作价值函数}{32}{subsubsection*.59}%
\contentsline {subsubsection}{最优价值函数的关系}{32}{subsubsection*.60}%
\contentsline {section}{\numberline {2.4}贝尔曼方程：动态规划的核心}{32}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}贝尔曼期望方程}{32}{subsection.2.4.1}%
\contentsline {subsubsection}{状态价值函数的贝尔曼方程}{33}{subsubsection*.61}%
\contentsline {subsubsection}{动作价值函数的贝尔曼方程}{33}{subsubsection*.62}%
\contentsline {subsection}{\numberline {2.4.2}贝尔曼最优方程}{33}{subsection.2.4.2}%
\contentsline {subsubsection}{状态价值函数的最优贝尔曼方程}{33}{subsubsection*.63}%
\contentsline {subsubsection}{动作价值函数的最优贝尔曼方程}{34}{subsubsection*.64}%
\contentsline {subsection}{\numberline {2.4.3}贝尔曼方程的矩阵形式}{34}{subsection.2.4.3}%
\contentsline {subsubsection}{状态价值函数的矩阵形式}{34}{subsubsection*.65}%
\contentsline {subsubsection}{解析解与迭代解}{34}{subsubsection*.66}%
\contentsline {section}{\numberline {2.5}强化学习算法分类}{34}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}有模型 vs 无模型}{35}{subsection.2.5.1}%
\contentsline {subsubsection}{有模型方法的进一步分类}{35}{subsubsection*.68}%
\contentsline {subsection}{\numberline {2.5.2}基于价值 vs 基于策略}{36}{subsection.2.5.2}%
\contentsline {subsubsection}{基于价值方法的策略推导}{36}{subsubsection*.70}%
\contentsline {subsubsection}{基于策略方法的目标函数}{37}{subsubsection*.71}%
\contentsline {subsection}{\numberline {2.5.3}蒙特卡洛 vs 时序差分}{37}{subsection.2.5.3}%
\contentsline {subsubsection}{蒙特卡洛方法的偏差与方差分析}{37}{subsubsection*.73}%
\contentsline {subsubsection}{时序差分方法的偏差与方差分析}{38}{subsubsection*.74}%
\contentsline {subsection}{\numberline {2.5.4}在线策略 vs 离线策略}{38}{subsection.2.5.4}%
\contentsline {subsubsection}{SARSA：在线策略TD控制算法}{38}{subsubsection*.76}%
\contentsline {subsubsection}{Q-learning：离线策略TD控制算法}{39}{subsubsection*.77}%
\contentsline {subsubsection}{经验回放（Experience Replay）}{39}{subsubsection*.78}%
\contentsline {section}{\numberline {2.6}Actor-Critic方法：价值与策略的结合}{39}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Actor-Critic框架}{39}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}优势函数（Advantage Function）}{40}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}策略梯度与Actor-Critic}{40}{subsection.2.6.3}%
\contentsline {subsection}{\numberline {2.6.4}现代Actor-Critic算法}{40}{subsection.2.6.4}%
\contentsline {subsubsection}{优势Actor-Critic（A2C/A3C）}{40}{subsubsection*.80}%
\contentsline {subsubsection}{近端策略优化（PPO）}{41}{subsubsection*.81}%
\contentsline {subsubsection}{深度确定性策略梯度（DDPG）}{41}{subsubsection*.82}%
\contentsline {section}{\numberline {2.7}总结与展望}{41}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1}强化学习算法分类总结}{41}{subsection.2.7.1}%
\contentsline {subsection}{\numberline {2.7.2}强化学习的挑战与前沿}{42}{subsection.2.7.2}%
\contentsline {subsubsection}{主要挑战}{42}{subsubsection*.84}%
\contentsline {subsubsection}{前沿方向}{42}{subsubsection*.85}%
\contentsline {chapter}{\numberline {第三章\hspace {.3em}}深度强化学习之价值学习}{43}{chapter.3}%
\contentsline {section}{\numberline {3.1}价值学习：从评估到决策}{43}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}价值学习的核心思想}{43}{subsection.3.1.1}%
\contentsline {section}{\numberline {3.2}Q-Learning：经典的价值学习算法}{44}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}算法原理}{44}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}表格Q-Learning}{44}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}探索与利用的平衡：$\epsilon $-贪婪策略}{45}{subsection.3.2.3}%
\contentsline {subsection}{\numberline {3.2.4}Q-Learning与SARSA的比较}{46}{subsection.3.2.4}%
\contentsline {section}{\numberline {3.3}深度Q网络：当Q-Learning遇见深度学习}{46}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}表格方法的局限性}{46}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}深度Q网络的基本思想}{46}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}DQN的训练目标与损失函数}{47}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}DQN的训练流程}{48}{subsection.3.3.4}%
\contentsline {section}{\numberline {3.4}DQN的核心技术}{48}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}经验回放（Experience Replay）}{48}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}目标网络（Target Network）}{49}{subsection.3.4.2}%
\contentsline {section}{\numberline {3.5}DQN的改进与扩展}{51}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}优先经验回放（Prioritized Experience Replay）}{51}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Double DQN}{51}{subsection.3.5.2}%
\contentsline {subsection}{\numberline {3.5.3}Dueling DQN}{52}{subsection.3.5.3}%
\contentsline {subsection}{\numberline {3.5.4}Multi-Step DQN}{53}{subsection.3.5.4}%
\contentsline {subsection}{\numberline {3.5.5}Noisy DQN}{54}{subsection.3.5.5}%
\contentsline {subsection}{\numberline {3.5.6}Distributional DQN}{54}{subsection.3.5.6}%
\contentsline {subsubsection}{C51算法}{55}{subsubsection*.95}%
\contentsline {subsubsection}{QR-DQN和IQN}{56}{subsubsection*.96}%
\contentsline {section}{\numberline {3.6}Rainbow：集成多种改进}{56}{section.3.6}%
\contentsline {section}{\numberline {3.7}总结与比较}{57}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}DQN变种对比}{57}{subsection.3.7.1}%
\contentsline {subsection}{\numberline {3.7.2}实际应用建议}{57}{subsection.3.7.2}%
\contentsline {subsection}{\numberline {3.7.3}未来方向}{57}{subsection.3.7.3}%
\contentsline {chapter}{\numberline {第四章\hspace {.3em}}深度强化学习之策略学习}{59}{chapter.4}%
\contentsline {section}{\numberline {4.1}引言：为什么需要策略学习？}{59}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}价值学习方法的局限性}{59}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}策略学习的优势}{60}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}策略学习的基本框架}{61}{subsection.4.1.3}%
\contentsline {section}{\numberline {4.2}策略梯度定理：理论基础}{61}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}目标函数的定义}{61}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}轨迹概率的分解}{61}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}策略梯度推导}{62}{subsection.4.2.3}%
\contentsline {subsection}{\numberline {4.2.4}直观理解}{63}{subsection.4.2.4}%
\contentsline {section}{\numberline {4.3}REINFORCE算法：蒙特卡洛策略梯度}{63}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}基本REINFORCE算法}{63}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}REINFORCE的改进：因果关系修正}{63}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}REINFORCE的改进：引入基线}{64}{subsection.4.3.3}%
\contentsline {subsection}{\numberline {4.3.4}REINFORCE的优缺点}{65}{subsection.4.3.4}%
\contentsline {section}{\numberline {4.4}Actor-Critic方法：结合价值函数}{66}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Actor-Critic基本思想}{66}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}优势函数（Advantage Function）}{66}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Actor-Critic算法}{67}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}Actor-Critic的优缺点}{67}{subsection.4.4.4}%
\contentsline {section}{\numberline {4.5}A2C和A3C：并行化Actor-Critic}{67}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}优势Actor-Critic（A2C）}{67}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}A2C算法}{68}{subsection.4.5.2}%
\contentsline {subsection}{\numberline {4.5.3}异步优势Actor-Critic（A3C）}{68}{subsection.4.5.3}%
\contentsline {subsection}{\numberline {4.5.4}A2C vs A3C比较}{69}{subsection.4.5.4}%
\contentsline {section}{\numberline {4.6}TRPO：置信域策略优化}{70}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}TRPO的基本思想}{70}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2}TRPO的数学形式}{70}{subsection.4.6.2}%
\contentsline {subsection}{\numberline {4.6.3}重要性采样（Importance Sampling）}{70}{subsection.4.6.3}%
\contentsline {subsection}{\numberline {4.6.4}代理目标函数（Surrogate Objective）}{71}{subsection.4.6.4}%
\contentsline {subsection}{\numberline {4.6.5}TRPO的求解}{71}{subsection.4.6.5}%
\contentsline {subsection}{\numberline {4.6.6}TRPO算法}{71}{subsection.4.6.6}%
\contentsline {subsection}{\numberline {4.6.7}TRPO的优缺点}{72}{subsection.4.6.7}%
\contentsline {section}{\numberline {4.7}PPO：近端策略优化}{72}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}PPO的基本思想}{72}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}PPO-Clip目标函数}{73}{subsection.4.7.2}%
\contentsline {subsection}{\numberline {4.7.3}PPO-Clip的直观理解}{73}{subsection.4.7.3}%
\contentsline {subsection}{\numberline {4.7.4}PPO-Penalty目标函数}{74}{subsection.4.7.4}%
\contentsline {subsection}{\numberline {4.7.5}广义优势估计（GAE）}{74}{subsection.4.7.5}%
\contentsline {subsection}{\numberline {4.7.6}PPO算法}{75}{subsection.4.7.6}%
\contentsline {subsection}{\numberline {4.7.7}联合损失函数}{75}{subsection.4.7.7}%
\contentsline {subsection}{\numberline {4.7.8}PPO的优缺点}{76}{subsection.4.7.8}%
\contentsline {section}{\numberline {4.8}策略学习方法的比较与应用}{76}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}方法对比}{76}{subsection.4.8.1}%
\contentsline {subsection}{\numberline {4.8.2}选择指南}{76}{subsection.4.8.2}%
\contentsline {subsection}{\numberline {4.8.3}实际应用建议}{76}{subsection.4.8.3}%
\contentsline {section}{\numberline {4.9}总结与展望}{77}{section.4.9}%
\contentsline {subsection}{\numberline {4.9.1}策略学习的发展历程}{77}{subsection.4.9.1}%
\contentsline {subsection}{\numberline {4.9.2}关键技术创新}{77}{subsection.4.9.2}%
\contentsline {subsection}{\numberline {4.9.3}未来发展方向}{78}{subsection.4.9.3}%
\contentsline {subsection}{\numberline {4.9.4}学习建议}{78}{subsection.4.9.4}%
