\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {第一章\hspace  {.3em}}神经网络与深度学习基础}{6}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10.0pt}}
\@writefile{lot}{\addvspace {10.0pt}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}启发性思考}{6}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}神经元：深度学习的基石}{6}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}数学模型}{6}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}关键组件详解}{6}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}激活函数：从Sigmoid到ReLU}{7}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}经典激活函数}{7}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid函数}{7}{subsubsection*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{双曲正切函数（tanh）}{7}{subsubsection*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{线性整流单元（ReLU）}{8}{subsubsection*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}激活函数对比与进阶选择}{8}{subsection.1.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces 激活函数对比}}{8}{table.caption.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ReLU的改进变体}{8}{subsubsection*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}前馈神经网络：层叠的威力}{9}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}网络结构}{9}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}优雅高效的矩阵表示}{9}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Softmax函数：多分类的“裁判”}{9}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{标准Softmax}{9}{subsubsection*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{温度系数（Temperature）}{9}{subsubsection*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}神经网络的训练：让模型学会思考}{10}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}步骤一：定义网络结构}{10}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}步骤二：定义损失函数（Loss Function）}{10}{subsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}步骤三：梯度下降优化}{11}{subsection.1.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}反向传播：深度学习的引擎}{11}{section.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}误差项（Error Term）的定义}{11}{subsection.1.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}反向传播的四步流程}{11}{subsection.1.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{1. 前向传播}{11}{subsubsection*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{2. 计算输出层误差}{11}{subsubsection*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{3. 反向传播误差（关键递推公式）}{12}{subsubsection*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{4. 计算参数梯度}{12}{subsubsection*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{5. 更新参数}{12}{subsubsection*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{6. 批处理与梯度累加}{12}{subsubsection*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.3}反向传播推导（选读）}{12}{subsection.1.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.7}引言：从拟合数据到泛化世界}{13}{section.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.8}模型性能评估：不只是看分数}{13}{section.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.1}数据集划分的艺术}{13}{subsection.1.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.2}K-折交叉验证：小数据集的救星}{14}{subsection.1.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.9}诊断模型问题：欠拟合与过拟合}{14}{section.1.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces 欠拟合与过拟合的图示}}{14}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.1}欠拟合（Underfitting）}{14}{subsection.1.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.2}过拟合（Overfitting）}{15}{subsection.1.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.3}偏差-方差权衡：统计学习理论的视角}{15}{subsection.1.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.10}正则化技术：防止过拟合的利器}{15}{section.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10.1}L1和L2正则化}{15}{subsection.1.10.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces L1 vs L2正则化对比}}{16}{table.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{几何解释}{16}{subsubsection*.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces L1和L2正则化的几何解释}}{16}{figure.caption.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10.2}Dropout：训练时随机丢弃神经元}{16}{subsection.1.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10.3}早停法（Early Stopping）}{16}{subsection.1.10.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Dropout示意图}}{17}{figure.caption.19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.11}从神经网络到深度学习：历史脉络}{17}{section.1.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11.1}发展历程}{17}{subsection.1.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{早期探索（1940s-1980s）}{17}{subsubsection*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{复兴前夜（1980s-2006）}{17}{subsubsection*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{复兴之年（2006）}{17}{subsubsection*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{爆发之年（2012）}{17}{subsubsection*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11.2}深度学习的四大支柱}{18}{subsection.1.11.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces 深度学习的四大支柱}}{18}{table.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11.3}为什么叫"深度学习"？}{18}{subsection.1.11.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.4}{\ignorespaces 传统机器学习 vs 深度学习}}{18}{table.caption.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces 自编码器结构示意图}}{19}{figure.caption.26}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.12}关键模型架构}{19}{section.1.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.1}自编码器（Autoencoders, AEs）}{19}{subsection.1.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.2}卷积神经网络（CNN）}{19}{subsection.1.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{核心组件}{19}{subsubsection*.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces CNN基本操作：卷积与池化}}{20}{figure.caption.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{经典架构}{20}{subsubsection*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.3}循环神经网络（RNN）与LSTM}{20}{subsection.1.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{标准RNN}{20}{subsubsection*.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces RNN的表示}}{20}{figure.caption.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{LSTM：长短期记忆网络}{20}{subsubsection*.32}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces LSTM单元结构}}{21}{figure.caption.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.4}生成对抗网络（GANs）}{21}{subsection.1.12.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces GAN的基本结构}}{22}{figure.caption.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.5}扩散模型（Diffusion Models）}{22}{subsection.1.12.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces 扩散模型的前向与反向过程}}{22}{figure.caption.35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.13}Transformer：注意力就是全部}{22}{section.1.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13.1}自注意力机制（Self-Attention）}{22}{subsection.1.13.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces 自注意力机制示例："吃"与"鱼"有强关联}}{23}{figure.caption.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13.2}多头注意力（Multi-Head Attention）}{23}{subsection.1.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13.3}Transformer架构}{23}{subsection.1.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{编码器（Encoder）}{23}{subsubsection*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{解码器（Decoder）}{23}{subsubsection*.39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Transformer架构}}{24}{figure.caption.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13.4}位置编码（Positional Encoding）}{24}{subsection.1.13.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13.5}Transformer vs RNN/LSTM}{24}{subsection.1.13.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.5}{\ignorespaces Transformer与RNN/LSTM对比}}{24}{table.caption.40}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.14}为什么Transformer能够"通吃"？}{25}{section.1.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.14.1}从专家模型到通用模型}{25}{subsection.1.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.14.2}Transformer的核心优势}{25}{subsection.1.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.15}总结与思考}{25}{section.1.15}\protected@file@percent }
\@setckpt{chapters/chap1}{
\setcounter{page}{26}
\setcounter{equation}{0}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{15}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{5}
\setcounter{parentequation}{0}
\setcounter{section@level}{0}
\setcounter{Item}{45}
\setcounter{Hfootnote}{0}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{52}
\setcounter{lstnumber}{1}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{nlinenum}{0}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{remark}{0}
\setcounter{example}{0}
\setcounter{lstlisting}{0}
}
