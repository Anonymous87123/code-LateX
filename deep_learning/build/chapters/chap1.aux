\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {第一章\hspace  {.3em}}神经网络与深度学习基础}{12}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10.0pt}}
\@writefile{lot}{\addvspace {10.0pt}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}启发性思考}{12}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}神经元：深度学习的基石}{12}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}数学模型}{12}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}关键组件详解}{12}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}激活函数：从Sigmoid到ReLU}{13}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}经典激活函数}{13}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid函数}{13}{subsubsection*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{双曲正切函数（tanh）}{13}{subsubsection*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{线性整流单元（ReLU）}{14}{subsubsection*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}激活函数对比与进阶选择}{14}{subsection.1.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces 激活函数对比}}{14}{table.caption.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ReLU的改进变体}{14}{subsubsection*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}前馈神经网络：层叠的威力}{15}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}网络结构}{15}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}优雅高效的矩阵表示}{15}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Softmax函数：多分类的“裁判”}{15}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{标准Softmax}{15}{subsubsection*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{温度系数（Temperature）}{15}{subsubsection*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}神经网络的训练：让模型学会思考}{16}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}步骤一：定义网络结构}{16}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}步骤二：定义损失函数（Loss Function）}{16}{subsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}步骤三：梯度下降优化}{17}{subsection.1.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}反向传播：深度学习的引擎}{17}{section.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}误差项（Error Term）的定义}{17}{subsection.1.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}反向传播的四步流程}{17}{subsection.1.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{1. 前向传播}{17}{subsubsection*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{2. 计算输出层误差}{17}{subsubsection*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{3. 反向传播误差（关键递推公式）}{18}{subsubsection*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{4. 计算参数梯度}{18}{subsubsection*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{5. 更新参数}{18}{subsubsection*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{6. 批处理与梯度累加}{18}{subsubsection*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.3}反向传播推导（选读）}{18}{subsection.1.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.7}引言：从拟合数据到泛化世界}{19}{section.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.8}模型性能评估：不只是看分数}{19}{section.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.1}数据集划分的艺术}{19}{subsection.1.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.2}K-折交叉验证：小数据集的救星}{20}{subsection.1.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.9}诊断模型问题：欠拟合与过拟合}{20}{section.1.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces 欠拟合与过拟合的图示}}{20}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.1}欠拟合（Underfitting）}{20}{subsection.1.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.2}过拟合（Overfitting）}{21}{subsection.1.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.3}偏差-方差权衡：统计学习理论的视角}{21}{subsection.1.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.10}正则化技术：防止过拟合的利器}{21}{section.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10.1}L1和L2正则化}{21}{subsection.1.10.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces L1 vs L2正则化对比}}{22}{table.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{几何解释}{22}{subsubsection*.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces L1和L2正则化的几何解释}}{22}{figure.caption.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10.2}Dropout：训练时随机丢弃神经元}{22}{subsection.1.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10.3}早停法（Early Stopping）}{22}{subsection.1.10.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Dropout示意图}}{23}{figure.caption.19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.11}从神经网络到深度学习：历史脉络}{23}{section.1.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11.1}发展历程}{23}{subsection.1.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{早期探索（1940s-1980s）}{23}{subsubsection*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{复兴前夜（1980s-2006）}{23}{subsubsection*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{复兴之年（2006）}{23}{subsubsection*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{爆发之年（2012）}{23}{subsubsection*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11.2}深度学习的四大支柱}{24}{subsection.1.11.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces 深度学习的四大支柱}}{24}{table.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11.3}为什么叫"深度学习"？}{24}{subsection.1.11.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.4}{\ignorespaces 传统机器学习 vs 深度学习}}{24}{table.caption.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces 自编码器结构示意图}}{25}{figure.caption.26}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.12}关键模型架构}{25}{section.1.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.1}自编码器（Autoencoders, AEs）}{25}{subsection.1.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.2}卷积神经网络（CNN）}{25}{subsection.1.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{核心组件}{25}{subsubsection*.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces CNN基本操作：卷积与池化}}{26}{figure.caption.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{经典架构}{26}{subsubsection*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.3}循环神经网络（RNN）与LSTM}{26}{subsection.1.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{标准RNN}{26}{subsubsection*.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces RNN的表示}}{26}{figure.caption.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{LSTM：长短期记忆网络}{26}{subsubsection*.32}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces LSTM单元结构}}{27}{figure.caption.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.4}生成对抗网络（GANs）}{27}{subsection.1.12.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces GAN的基本结构}}{28}{figure.caption.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.5}扩散模型（Diffusion Models）}{28}{subsection.1.12.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces 扩散模型的前向与反向过程}}{28}{figure.caption.35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.13}Transformer：注意力就是全部}{28}{section.1.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13.1}自注意力机制（Self-Attention）}{28}{subsection.1.13.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces 自注意力机制示例："吃"与"鱼"有强关联}}{29}{figure.caption.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13.2}多头注意力（Multi-Head Attention）}{29}{subsection.1.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13.3}Transformer架构}{29}{subsection.1.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{编码器（Encoder）}{29}{subsubsection*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{解码器（Decoder）}{29}{subsubsection*.39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Transformer架构}}{30}{figure.caption.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13.4}位置编码（Positional Encoding）}{30}{subsection.1.13.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13.5}Transformer vs RNN/LSTM}{30}{subsection.1.13.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.5}{\ignorespaces Transformer与RNN/LSTM对比}}{30}{table.caption.40}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.14}为什么Transformer能够"通吃"？}{31}{section.1.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.14.1}从专家模型到通用模型}{31}{subsection.1.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.14.2}Transformer的核心优势}{31}{subsection.1.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.15}总结与思考}{31}{section.1.15}\protected@file@percent }
\@setckpt{chapters/chap1}{
\setcounter{page}{32}
\setcounter{equation}{0}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{15}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{5}
\setcounter{parentequation}{0}
\setcounter{section@level}{0}
\setcounter{Item}{45}
\setcounter{Hfootnote}{0}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{52}
\setcounter{lstnumber}{1}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{nlinenum}{0}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{remark}{0}
\setcounter{example}{0}
\setcounter{lemma}{0}
\setcounter{lstlisting}{0}
}
