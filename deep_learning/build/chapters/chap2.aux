\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {第二章\hspace  {.3em}}深度强化学习之基础}{26}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10.0pt}}
\@writefile{lot}{\addvspace {10.0pt}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}引言：从有答案的学习到探索的学习}{26}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}监督学习的辉煌与局限}{26}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{监督学习的核心思想}{26}{subsubsection*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{监督学习的成功案例}{26}{subsubsection*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{监督学习的局限性}{26}{subsubsection*.43}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces 监督学习的局限性}}{27}{table.caption.44}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}强化学习：一种新的学习范式}{27}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{从"知道答案"到"探索答案"}{27}{subsubsection*.45}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces 从监督学习到强化学习的范式转变}}{27}{figure.caption.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{人类学习与强化学习的类比}{28}{subsubsection*.47}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces 人类学习与强化学习的对比}}{28}{table.caption.48}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}马尔可夫决策过程：强化学习的数学基础}{28}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}马尔可夫决策过程的基本概念}{28}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces 马尔可夫决策过程的基本流程}}{28}{figure.caption.49}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MDP的五个核心要素}{28}{subsubsection*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}MDP的核心概念}{29}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{策略函数 $\pi $}{29}{subsubsection*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{轨迹与回报}{30}{subsubsection*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Rollout（或轨迹采样）}{30}{subsubsection*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}价值函数：评估策略的优劣}{30}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}价值函数的定义与意义}{30}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{状态价值函数 $V_\pi (s)$}{30}{subsubsection*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{动作价值函数 $Q_\pi (s, a)$}{31}{subsubsection*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}价值函数的关系}{31}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{从 $Q_\pi $ 到 $V_\pi $}{31}{subsubsection*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{从 $V_\pi $ 到 $Q_\pi $}{31}{subsubsection*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}最优价值函数}{31}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{最优状态价值函数}{32}{subsubsection*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{最优动作价值函数}{32}{subsubsection*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{最优价值函数的关系}{32}{subsubsection*.60}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}贝尔曼方程：动态规划的核心}{32}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}贝尔曼期望方程}{32}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{状态价值函数的贝尔曼方程}{33}{subsubsection*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{动作价值函数的贝尔曼方程}{33}{subsubsection*.62}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}贝尔曼最优方程}{33}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{状态价值函数的最优贝尔曼方程}{33}{subsubsection*.63}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{动作价值函数的最优贝尔曼方程}{34}{subsubsection*.64}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}贝尔曼方程的矩阵形式}{34}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{状态价值函数的矩阵形式}{34}{subsubsection*.65}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{解析解与迭代解}{34}{subsubsection*.66}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}强化学习算法分类}{34}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}有模型 vs 无模型}{35}{subsection.2.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces 有模型方法与无模型方法的对比}}{35}{table.caption.67}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{有模型方法的进一步分类}{35}{subsubsection*.68}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}基于价值 vs 基于策略}{36}{subsection.2.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces 基于价值方法与基于策略方法的对比}}{36}{table.caption.69}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{基于价值方法的策略推导}{36}{subsubsection*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{基于策略方法的目标函数}{37}{subsubsection*.71}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}蒙特卡洛 vs 时序差分}{37}{subsection.2.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces 蒙特卡洛方法与时序差分方法的对比}}{37}{table.caption.72}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{蒙特卡洛方法的偏差与方差分析}{37}{subsubsection*.73}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{时序差分方法的偏差与方差分析}{38}{subsubsection*.74}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}在线策略 vs 离线策略}{38}{subsection.2.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces 在线策略方法与离线策略方法的对比}}{38}{table.caption.75}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{SARSA：在线策略TD控制算法}{38}{subsubsection*.76}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Q-learning：离线策略TD控制算法}{39}{subsubsection*.77}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{经验回放（Experience Replay）}{39}{subsubsection*.78}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Actor-Critic方法：价值与策略的结合}{39}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Actor-Critic框架}{39}{subsection.2.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Actor-Critic框架示意图}}{39}{figure.caption.79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}优势函数（Advantage Function）}{40}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}策略梯度与Actor-Critic}{40}{subsection.2.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}现代Actor-Critic算法}{40}{subsection.2.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{优势Actor-Critic（A2C/A3C）}{40}{subsubsection*.80}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{近端策略优化（PPO）}{41}{subsubsection*.81}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{深度确定性策略梯度（DDPG）}{41}{subsubsection*.82}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}总结与展望}{41}{section.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}强化学习算法分类总结}{41}{subsection.2.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces 强化学习算法分类树}}{41}{figure.caption.83}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}强化学习的挑战与前沿}{42}{subsection.2.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{主要挑战}{42}{subsubsection*.84}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{前沿方向}{42}{subsubsection*.85}\protected@file@percent }
\@setckpt{chapters/chap2}{
\setcounter{page}{43}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{3}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{6}
\setcounter{parentequation}{0}
\setcounter{section@level}{0}
\setcounter{Item}{80}
\setcounter{Hfootnote}{0}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{80}
\setcounter{lstnumber}{1}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{nlinenum}{0}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{remark}{0}
\setcounter{example}{0}
\setcounter{lstlisting}{0}
}
