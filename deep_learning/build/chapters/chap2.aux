\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {第二章\hspace  {.3em}}深度强化学习之基础}{32}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10.0pt}}
\@writefile{lot}{\addvspace {10.0pt}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}引言：从有答案的学习到探索的学习}{32}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}监督学习的辉煌与局限}{32}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{监督学习的核心思想}{32}{subsubsection*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{监督学习的成功案例}{32}{subsubsection*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{监督学习的局限性}{32}{subsubsection*.43}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces 监督学习的局限性}}{33}{table.caption.44}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}强化学习：一种新的学习范式}{33}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{从"知道答案"到"探索答案"}{33}{subsubsection*.45}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces 从监督学习到强化学习的范式转变}}{33}{figure.caption.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{人类学习与强化学习的类比}{34}{subsubsection*.47}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces 人类学习与强化学习的对比}}{34}{table.caption.48}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}马尔可夫决策过程：强化学习的数学基础}{34}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}马尔可夫决策过程的基本概念}{34}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces 马尔可夫决策过程的基本流程}}{34}{figure.caption.49}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MDP的五个核心要素}{34}{subsubsection*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}MDP的核心概念}{35}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{策略函数 $\pi $}{35}{subsubsection*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{轨迹与回报}{36}{subsubsection*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Rollout（或轨迹采样）}{36}{subsubsection*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}价值函数：评估策略的优劣}{36}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}价值函数的定义与意义}{36}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{状态价值函数 $V_\pi (s)$}{36}{subsubsection*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{动作价值函数 $Q_\pi (s, a)$}{37}{subsubsection*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}价值函数的关系}{37}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{从 $Q_\pi $ 到 $V_\pi $}{37}{subsubsection*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{从 $V_\pi $ 到 $Q_\pi $}{37}{subsubsection*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}最优价值函数}{37}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{最优状态价值函数}{38}{subsubsection*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{最优动作价值函数}{38}{subsubsection*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{最优价值函数的关系}{38}{subsubsection*.60}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}贝尔曼方程：动态规划的核心}{38}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}贝尔曼期望方程}{38}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{状态价值函数的贝尔曼方程}{39}{subsubsection*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{动作价值函数的贝尔曼方程}{39}{subsubsection*.62}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}贝尔曼最优方程}{39}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{状态价值函数的最优贝尔曼方程}{39}{subsubsection*.63}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{动作价值函数的最优贝尔曼方程}{40}{subsubsection*.64}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}贝尔曼方程的矩阵形式}{40}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{状态价值函数的矩阵形式}{40}{subsubsection*.65}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{解析解与迭代解}{40}{subsubsection*.66}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}强化学习算法分类}{40}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}有模型 vs 无模型}{41}{subsection.2.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces 有模型方法与无模型方法的对比}}{41}{table.caption.67}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{有模型方法的进一步分类}{41}{subsubsection*.68}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}基于价值 vs 基于策略}{42}{subsection.2.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces 基于价值方法与基于策略方法的对比}}{42}{table.caption.69}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{基于价值方法的策略推导}{42}{subsubsection*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{基于策略方法的目标函数}{43}{subsubsection*.71}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}蒙特卡洛 vs 时序差分}{43}{subsection.2.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces 蒙特卡洛方法与时序差分方法的对比}}{43}{table.caption.72}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{蒙特卡洛方法的偏差与方差分析}{43}{subsubsection*.73}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{时序差分方法的偏差与方差分析}{44}{subsubsection*.74}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}在线策略 vs 离线策略}{44}{subsection.2.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces 在线策略方法与离线策略方法的对比}}{44}{table.caption.75}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{SARSA：在线策略TD控制算法}{44}{subsubsection*.76}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Q-learning：离线策略TD控制算法}{45}{subsubsection*.77}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{经验回放（Experience Replay）}{45}{subsubsection*.78}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Actor-Critic方法：价值与策略的结合}{45}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Actor-Critic框架}{45}{subsection.2.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Actor-Critic框架示意图}}{45}{figure.caption.79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}优势函数（Advantage Function）}{46}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}策略梯度与Actor-Critic}{46}{subsection.2.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}现代Actor-Critic算法}{46}{subsection.2.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{优势Actor-Critic（A2C/A3C）}{46}{subsubsection*.80}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{近端策略优化（PPO）}{47}{subsubsection*.81}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{深度确定性策略梯度（DDPG）}{47}{subsubsection*.82}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}总结与展望}{47}{section.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}强化学习算法分类总结}{47}{subsection.2.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces 强化学习算法分类树}}{47}{figure.caption.83}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}强化学习的挑战与前沿}{48}{subsection.2.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{主要挑战}{48}{subsubsection*.84}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{前沿方向}{48}{subsubsection*.85}\protected@file@percent }
\@setckpt{chapters/chap2}{
\setcounter{page}{49}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{3}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{6}
\setcounter{parentequation}{0}
\setcounter{section@level}{0}
\setcounter{Item}{80}
\setcounter{Hfootnote}{0}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{80}
\setcounter{lstnumber}{1}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{nlinenum}{0}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{remark}{0}
\setcounter{example}{0}
\setcounter{lemma}{0}
\setcounter{lstlisting}{0}
}
