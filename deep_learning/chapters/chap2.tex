\chapter{深度强化学习之基础}
\section{引言：从有答案的学习到探索的学习}

\subsection{监督学习的辉煌与局限}

\subsubsection{监督学习的核心思想}
监督学习（Supervised Learning）是机器学习中最经典、最直观的范式。它的核心思想可以概括为：

\textbf{从"有标签"的数据中学习。}

\begin{itemize}
    \item \textbf{学习方式}：模型通过学习大量的（输入，正确输出）样本对，来建立输入到输出的映射关系。
    \item \textbf{类比}：就像学生跟着老师学习，老师为每个问题提供标准答案。
    \item \textbf{数学模型}：给定训练集 $\{(x^{(i)}, y^{(i)})\}_{i=1}^m$，学习函数 $f: \mathcal{X} \rightarrow \mathcal{Y}$，使得 $f(x^{(i)}) \approx y^{(i)}$。
\end{itemize}

\subsubsection{监督学习的成功案例}
\begin{enumerate}
    \item \textbf{图像分类}：输入图片，输出类别标签（猫/狗/车等）。
    \item \textbf{语音识别}：输入音频波形，输出对应文本。
    \item \textbf{机器翻译}：输入源语言句子，输出目标语言句子。
\end{enumerate}

\subsubsection{监督学习的局限性}
尽管监督学习非常强大，但它并非万能。在某些场景下会面临严峻挑战：

\begin{table}[H]
\centering
\caption{监督学习的局限性}
\begin{tabular}{p{0.3\textwidth}p{0.65\textwidth}}
\toprule
\textbf{挑战类型} & \textbf{具体描述与示例} \\
\midrule
\textbf{缺乏"标准答案"} & 
\begin{itemize}
    \item 许多复杂决策问题没有唯一的"正确答案"
    \item \textbf{示例}：围棋中每一步的好坏难以用简单的对错衡量
    \item \textbf{示例}：自动驾驶中的决策权衡（安全vs效率）
\end{itemize} \\
\hline
\textbf{序贯决策需求} & 
\begin{itemize}
    \item 需要做一系列相关决策，而每一步的优劣要到很久之后才能判断
    \item \textbf{示例}：下棋时需要规划多步之后的局面
    \item \textbf{示例}：机器人需要完成多步动作才能达到目标
\end{itemize} \\
\hline
\textbf{动态变化的环境} & 
\begin{itemize}
    \item 环境不断变化，今天的"最优解"明天可能失效
    \item \textbf{示例}：金融市场交易策略
    \item \textbf{示例}：游戏AI应对不同玩家风格
\end{itemize} \\
\hline
\textbf{探索与利用的权衡} & 
\begin{itemize}
    \item 如何在已知的好方法和尝试新方法之间取得平衡
    \item \textbf{示例}：推荐系统：推荐用户已知喜欢的内容vs尝试新内容
\end{itemize} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{强化学习：一种新的学习范式}

\subsubsection{从"知道答案"到"探索答案"}
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
\draw[fill=blue!20] (0,0) rectangle (5,3);
\node at (2.5,2.5) {\large 监督学习};
\node at (2.5,2) {有标准答案};
\node at (2.5,1.5) {被动接收};
\node at (2.5,1) {静态环境};

\draw[fill=green!20] (7,0) rectangle (12,3);
\node at (9.5,2.5) {\large 强化学习};
\node at (9.5,2) {探索最优解};
\node at (9.5,1.5) {主动交互};
\node at (9.5,1) {动态环境};

\draw[->, thick] (5,1.5) -- node[above] {范式转变} (7,1.5);
\end{tikzpicture}
\caption{从监督学习到强化学习的范式转变}
\end{figure}

\textbf{强化学习（Reinforcement Learning, RL）}的核心思想：

\textbf{通过与环境的"互动"和"试错"来学习。}

\begin{itemize}
    \item \textbf{没有现成答案}：学习者（智能体）通过行动获得奖励或惩罚。
    \item \textbf{学习目标}：学会采取能最大化\textbf{长期累积奖励}的策略。
    \item \textbf{类比}：就像婴儿学习走路，通过尝试和摔倒（惩罚）来学会平衡和前进（奖励）。
\end{itemize}

\subsubsection{人类学习与强化学习的类比}

\begin{table}[H]
\centering
\caption{人类学习与强化学习的对比}
\begin{tabular}{p{0.3\textwidth}p{0.3\textwidth}p{0.3\textwidth}}
\toprule
\textbf{学习要素} & \textbf{人类学习} & \textbf{强化学习} \\
\midrule
\textbf{交互} & 与环境互动（如触摸物体） & 智能体与环境互动 \\
\hline
\textbf{反馈} & 
\begin{itemize}
    \item 打碎杯子 → 受到惩罚
    \item 成绩提升 → 获得奖励
\end{itemize} & 
\begin{itemize}
    \item 错误动作 → 负奖励
    \item 正确动作 → 正奖励
\end{itemize} \\
\hline
\textbf{分析} & 
\begin{itemize}
    \item "打碎杯子"的收益小
    \item "好成绩"的收益大
\end{itemize} & 
\begin{itemize}
    \item 评估动作的期望回报
    \item 更新策略以最大化回报
\end{itemize} \\
\hline
\textbf{学习} & 
\begin{itemize}
    \item 减小力度
    \item 努力学习
\end{itemize} & 
\begin{itemize}
    \item 调整策略参数
    \item 更新价值函数
\end{itemize} \\
\bottomrule
\end{tabular}
\end{table}

\section{马尔可夫决策过程：强化学习的数学基础}

\subsection{马尔可夫决策过程的基本概念}

马尔可夫决策过程（Markov Decision Process, MDP）是强化学习的标准数学框架，它形式化了智能体与环境交互的过程。

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=2cm,
    state/.style={circle, draw, minimum size=1.2cm, fill=blue!20},
    action/.style={rectangle, draw, minimum width=1.5cm, minimum height=0.8cm, fill=green!20},
    reward/.style={rectangle, draw, minimum width=1.5cm, minimum height=0.8cm, fill=red!20}
]
\node[state] (s) {$s_t$};
\node[action, below left=1cm and 0.5cm of s] (a) {$a_t$};
\node[state, right=2cm of s] (sp) {$s_{t+1}$};
\node[reward, below right=1cm and 0.5cm of s] (r) {$r_t$};

\draw[->, thick] (s) -- node[left] {选择动作} (a);
\draw[->, thick] (a) -- node[below right] {} (s);
\draw[->, thick] (s) -- node[above] {状态转移} (sp);
\draw[->, thick] (sp) -- node[right] {获得奖励} (r);
\draw[->, thick] (s) -- node[below left] {环境反馈} (r);

\node[above=0.2cm of s] {状态};
\node[below=0.2cm of a] {动作};
\node[above=0.2cm of sp] {新状态};
\node[below=0.2cm of r] {奖励};
\end{tikzpicture}
\caption{马尔可夫决策过程的基本流程}
\end{figure}

\subsubsection{MDP的五个核心要素}
一个MDP由以下五元组定义：$(\mathcal{S}, \mathcal{A}, P, R, \gamma)$

\begin{definition}[状态空间 $\mathcal{S}$]
状态（State）是对当前环境的完整描述。状态空间是所有可能状态的集合。
\begin{itemize}
    \item \textbf{理性理解}：状态是环境的"快照"，包含了做出决策所需的全部信息。
    \item \textbf{感性理解}：就像下棋时的棋盘局面，包含了所有棋子的位置信息。
    \item \textbf{示例}：在自动驾驶中，状态可能包括车辆位置、速度、周围车辆位置等。
\end{itemize}
\end{definition}

\begin{definition}[动作空间 $\mathcal{A}$]
动作（Action）是智能体可以执行的操作。动作空间是所有可能动作的集合。
\begin{itemize}
    \item \textbf{理性理解}：动作是智能体对环境施加的控制信号。
    \item \textbf{感性理解}：就像棋手可以走的合法着法。
    \item \textbf{分类}：
    \begin{enumerate}
        \item \textbf{离散动作空间}：动作数量有限（如上/下/左/右）
        \item \textbf{连续动作空间}：动作是连续的（如转向角度、油门大小）
    \end{enumerate}
\end{itemize}
\end{definition}

\begin{definition}[状态转移函数 $P$]
状态转移函数定义了环境动力学：$p(s' \mid s, a) = \mathbb{P}(S_{t+1} = s' \mid S_t = s, A_t = a)$
\begin{itemize}
    \item \textbf{理性理解}：在状态$s$执行动作$a$后，环境转移到状态$s'$的概率。
    \item \textbf{感性理解}：就像知道在某个棋局走某步后，出现各种可能局面的概率。
    \item \textbf{马尔可夫性}：下一状态$s'$只依赖于当前状态$s$和动作$a$，不依赖于更早的历史。
    \[
    \mathbb{P}(S_{t+1} = s' \mid S_t = s, A_t = a, S_{t-1}, A_{t-1}, \ldots) = \mathbb{P}(S_{t+1} = s' \mid S_t = s, A_t = a)
    \]
\end{itemize}
\end{definition}

\begin{definition}[奖励函数 $R$]
奖励函数定义了立即奖励：$R(s, a) = \mathbb{E}[R_{t+1} \mid S_t = s, A_t = a]$
\begin{itemize}
    \item \textbf{理性理解}：在状态$s$执行动作$a$后，期望获得的立即奖励。
    \item \textbf{感性理解}：就像游戏中的得分或扣分。
    \item \textbf{设计原则}：奖励函数的设计是强化学习成功的关键，需要准确反映任务目标。
    \begin{itemize}
        \item \textbf{稀疏奖励}：只在关键时刻给予奖励（如获胜时+1，失败时-1）
        \item \textbf{稠密奖励}：每一步都有小奖励（如离目标越近奖励越大）
    \end{itemize}
\end{itemize}
\end{definition}

\begin{definition}[折扣因子 $\gamma$]
折扣因子 $\gamma \in [0, 1]$ 权衡即时奖励与未来奖励的重要性。
\[
U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots
\]
\begin{itemize}
    \item $\gamma = 0$：只关心即时奖励（短视）
    \item $\gamma \rightarrow 1$：几乎平等对待所有未来奖励（远见）
    \item \textbf{数学意义}：确保无限序列的回报有界（当$\gamma < 1$时）
    \item \textbf{实际意义}：反映未来奖励的不确定性（"一鸟在手胜过双鸟在林"）
\end{itemize}
\end{definition}

\subsection{MDP的核心概念}

\subsubsection{策略函数 $\pi$}

\begin{definition}[策略函数]
策略 $\pi(a \mid s)$ 是在状态 $s$ 下选择动作 $a$ 的概率分布：
\[
\pi(a \mid s) = \mathbb{P}(A_t = a \mid S_t = s)
\]
\begin{itemize}
    \item \textbf{确定性策略}：$\pi(s)$ 输出一个确定的动作（如 $\pi(s) = \text{"向右移动"}$）
    \item \textbf{随机性策略}：$\pi(a \mid s)$ 输出动作的概率分布（如 $\pi(\text{"右"} \mid s) = 0.8, \pi(\text{"左"} \mid s) = 0.2$）
    \item \textbf{理性理解}：策略是智能体的"行为准则"或"决策规则"。
    \item \textbf{感性理解}：就像一个人的性格或习惯，决定了在什么情况下会做什么选择。
\end{itemize}
\end{definition}

\subsubsection{轨迹与回报}

\begin{definition}[轨迹（Trajectory）]
一个轨迹（或回合，Episode）是从初始状态到终止状态的完整交互序列：
\[
\tau = (s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3, \ldots)
\]
其中 $s_0$ 是初始状态，智能体根据策略选择动作，环境根据转移函数和奖励函数给出新状态和奖励。
\end{definition}

\begin{definition}[回报（Return）]
从时刻 $t$ 开始的累计折扣奖励称为回报：
\[
U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
\]
\begin{itemize}
    \item \textbf{理性理解}：回报是未来所有奖励的加权和，权重随时间指数衰减。
    \item \textbf{感性理解}：就像投资回报，近期的收益比远期的收益更有价值。
\end{itemize}
\end{definition}

\subsubsection{Rollout（或轨迹采样）}
\begin{itemize}
    \item \textbf{定义}：从当前状态开始，按照某个策略 $\pi$ 进行交互，生成一段轨迹的过程。
    \item \textbf{目的}：评估策略在当前状态下的表现，或收集训练数据。
    \item \textbf{示例}：在蒙特卡洛树搜索中，从当前棋局开始，随机走棋直到终局，得到胜负结果。
\end{itemize}

\section{价值函数：评估策略的优劣}

\subsection{价值函数的定义与意义}

由于未来的奖励具有随机性（来自策略的随机性和环境的随机性），我们关心的是期望回报。

\subsubsection{状态价值函数 $V_\pi(s)$}

\begin{definition}[状态价值函数]
状态价值函数 $V_\pi(s)$ 表示从状态 $s$ 开始，遵循策略 $\pi$ 所能获得的期望回报：
\[
V_\pi(s) = \mathbb{E}_\pi[U_t \mid S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^\infty \gamma^k R_{t+k} \mid S_t = s\right]
\]
\begin{itemize}
    \item \textbf{理性理解}：$V_\pi(s)$ 量化了状态 $s$ 的"好坏"程度。
    \item \textbf{感性理解}：就像评估一个棋局的"优势程度"，数值越大表示局面越好。
    \item \textbf{性质}：$V_\pi(s)$ 只依赖于状态 $s$ 和策略 $\pi$，不依赖于具体采取的动作。
\end{itemize}
\end{definition}

\subsubsection{动作价值函数 $Q_\pi(s, a)$}

\begin{definition}[动作价值函数（Q函数）]
动作价值函数 $Q_\pi(s, a)$ 表示在状态 $s$ 下执行动作 $a$，然后遵循策略 $\pi$ 所能获得的期望回报：
\[
Q_\pi(s, a) = \mathbb{E}_\pi[U_t \mid S_t = s, A_t = a] = \mathbb{E}_\pi\left[\sum_{k=0}^\infty \gamma^k R_{t+k} \mid S_t = s, A_t = a\right]
\]
\begin{itemize}
    \item \textbf{理性理解}：$Q_\pi(s, a)$ 量化了在状态 $s$ 下选择动作 $a$ 的"好坏"程度。
    \item \textbf{感性理解}：就像评估在某个棋局下走某一步的"优劣程度"。
    \item \textbf{关键区别}：$Q_\pi(s, a)$ 额外考虑了当前选择的动作 $a$。
\end{itemize}
\end{definition}

\subsection{价值函数的关系}

状态价值函数和动作价值函数之间存在密切关系：

\subsubsection{从 $Q_\pi$ 到 $V_\pi$}
状态价值是该状态下所有可能动作的期望价值：
\[
V_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \cdot Q_\pi(s, a)
\]
\begin{itemize}
    \item \textbf{解释}：在状态 $s$ 下，按照策略 $\pi$ 随机选择动作，$V_\pi(s)$ 是 $Q_\pi(s, a)$ 的加权平均。
    \item \textbf{示例}：如果策略 $\pi$ 在状态 $s$ 下以0.7概率选择动作 $a_1$（$Q=10$），以0.3概率选择动作 $a_2$（$Q=5$），则 $V_\pi(s) = 0.7 \times 10 + 0.3 \times 5 = 8.5$。
\end{itemize}

\subsubsection{从 $V_\pi$ 到 $Q_\pi$}
动作价值可以分解为立即奖励加上折扣后的下一个状态价值的期望：
\[
Q_\pi(s, a) = R(s, a) + \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s, a) \cdot V_\pi(s')
\]
\begin{itemize}
    \item \textbf{解释}：执行动作 $a$ 后，获得立即奖励 $R(s, a)$，然后环境转移到状态 $s'$，从 $s'$ 开始遵循策略 $\pi$ 的期望回报是 $V_\pi(s')$。
    \item \textbf{示例}：在状态 $s$ 执行动作 $a$，有0.8概率转移到 $s_1$（$V=20$），0.2概率转移到 $s_2$（$V=5$），立即奖励为2，$\gamma=0.9$，则 $Q_\pi(s, a) = 2 + 0.9 \times (0.8 \times 20 + 0.2 \times 5) = 2 + 0.9 \times 17 = 17.3$。
\end{itemize}

\subsection{最优价值函数}

强化学习的最终目标是找到最优策略 $\pi^*$，使得从任何状态出发都能获得最大期望回报。

\subsubsection{最优状态价值函数}
\begin{definition}[最优状态价值函数]
最优状态价值函数 $V^*(s)$ 是所有可能策略中能获得的最大状态价值：
\[
V^*(s) = \max_\pi V_\pi(s), \quad \forall s \in \mathcal{S}
\]
\begin{itemize}
    \item \textbf{理性理解}：$V^*(s)$ 是从状态 $s$ 出发，采用最优策略所能获得的最大期望回报。
    \item \textbf{感性理解}：就像"棋神"在当前局面下能获得的最佳结果。
\end{itemize}
\end{definition}

\subsubsection{最优动作价值函数}
\begin{definition}[最优动作价值函数]
最优动作价值函数 $Q^*(s, a)$ 是所有可能策略中能获得的最大动作价值：
\[
Q^*(s, a) = \max_\pi Q_\pi(s, a), \quad \forall s \in \mathcal{S}, a \in \mathcal{A}
\]
\begin{itemize}
    \item \textbf{理性理解}：$Q^*(s, a)$ 是在状态 $s$ 下执行动作 $a$，然后采用最优策略所能获得的最大期望回报。
    \item \textbf{感性理解}：就像知道在某个局面下走某一步，后续采用最佳着法能获得的最好结果。
\end{itemize}
\end{definition}

\subsubsection{最优价值函数的关系}
最优状态价值和最优动作价值之间也存在类似关系：

\[
V^*(s) = \max_{a \in \mathcal{A}} Q^*(s, a)
\]
\[
Q^*(s, a) = R(s, a) + \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s, a) \cdot V^*(s')
\]

将两式结合，得到：
\[
V^*(s) = \max_{a \in \mathcal{A}} \left\{ R(s, a) + \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s, a) \cdot V^*(s') \right\}
\]
\[
Q^*(s, a) = R(s, a) + \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s, a) \cdot \max_{a' \in \mathcal{A}} Q^*(s', a')
\]

\section{贝尔曼方程：动态规划的核心}

\subsection{贝尔曼期望方程}

贝尔曼期望方程（Bellman Expectation Equation）建立了当前价值与未来价值之间的关系。

\subsubsection{状态价值函数的贝尔曼方程}
\begin{theorem}[状态价值函数的贝尔曼方程]
对于任意策略 $\pi$ 和状态 $s$，有：
\[
V_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \left[ R(s, a) + \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s, a) \cdot V_\pi(s') \right]
\]
\end{theorem}

\textbf{推导过程}：
\begin{align*}
V_\pi(s) &= \mathbb{E}_\pi[U_t \mid S_t = s] \\
&= \mathbb{E}_\pi\left[R_t + \gamma U_{t+1} \mid S_t = s\right] \\
&= \sum_{a \in \mathcal{A}} \pi(a \mid s) \mathbb{E}_\pi\left[R_t + \gamma U_{t+1} \mid S_t = s, A_t = a\right] \\
&= \sum_{a \in \mathcal{A}} \pi(a \mid s) \left[ R(s, a) + \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s, a) \cdot \mathbb{E}_\pi[U_{t+1} \mid S_{t+1} = s'] \right] \\
&= \sum_{a \in \mathcal{A}} \pi(a \mid s) \left[ R(s, a) + \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s, a) \cdot V_\pi(s') \right]
\end{align*}

\textbf{直观理解}：当前状态的价值 = 当前动作的期望立即奖励 + 折扣后的下一状态价值的期望。

\subsubsection{动作价值函数的贝尔曼方程}
\begin{theorem}[动作价值函数的贝尔曼方程]
对于任意策略 $\pi$、状态 $s$ 和动作 $a$，有：
\[
Q_\pi(s, a) = R(s, a) + \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s, a) \cdot \sum_{a' \in \mathcal{A}} \pi(a' \mid s') \cdot Q_\pi(s', a')
\]
\end{theorem}

\subsection{贝尔曼最优方程}

贝尔曼最优方程（Bellman Optimality Equation）描述了最优价值函数必须满足的条件。

\subsubsection{状态价值函数的最优贝尔曼方程}
\begin{theorem}[状态价值函数的最优贝尔曼方程]
最优状态价值函数满足：
\[
V^*(s) = \max_{a \in \mathcal{A}} \left\{ R(s, a) + \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s, a) \cdot V^*(s') \right\}
\]
\end{theorem}

\textbf{直观理解}：最优状态价值 = 所有可能动作中，能获得最大"立即奖励+折扣后最优价值"的那个动作对应的值。

\subsubsection{动作价值函数的最优贝尔曼方程}
\begin{theorem}[动作价值函数的最优贝尔曼方程]
最优动作价值函数满足：
\[
Q^*(s, a) = R(s, a) + \gamma \sum_{s' \in \mathcal{S}} p(s' \mid s, a) \cdot \max_{a' \in \mathcal{A}} Q^*(s', a')
\]
\end{theorem}

\textbf{直观理解}：最优动作价值 = 立即奖励 + 折扣后下一状态的最优动作价值的最大值。

\subsection{贝尔曼方程的矩阵形式}

对于有限状态空间，贝尔曼方程可以写成矩阵形式，便于理论分析和计算。

\subsubsection{状态价值函数的矩阵形式}
设状态空间 $\mathcal{S} = \{s_1, s_2, \ldots, s_N\}$，定义：
\begin{itemize}
    \item 状态价值向量：$\mathbf{V}_\pi = [V_\pi(s_1), V_\pi(s_2), \ldots, V_\pi(s_N)]^\top$
    \item 立即奖励向量：$\mathbf{R}_\pi = [R_\pi(s_1), R_\pi(s_2), \ldots, R_\pi(s_N)]^\top$，其中 $R_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) R(s, a)$
    \item 状态转移矩阵：$\mathbf{P}_\pi$，其中 $[\mathbf{P}_\pi]_{ij} = \sum_{a \in \mathcal{A}} \pi(a \mid s_i) p(s_j \mid s_i, a)$
\end{itemize}

则贝尔曼期望方程可写为：
\[
\mathbf{V}_\pi = \mathbf{R}_\pi + \gamma \mathbf{P}_\pi \mathbf{V}_\pi
\]

\subsubsection{解析解与迭代解}
\begin{enumerate}
    \item \textbf{解析解}：理论上可以直接求解：
    \[
    \mathbf{V}_\pi = (\mathbf{I} - \gamma \mathbf{P}_\pi)^{-1} \mathbf{R}_\pi
    \]
    但矩阵求逆的复杂度为 $O(N^3)$，当状态数 $N$ 很大时计算不可行。
    
    \item \textbf{迭代解}：实际中常用迭代方法：
    \[
    \mathbf{V}_\pi^{(k+1)} = \mathbf{R}_\pi + \gamma \mathbf{P}_\pi \mathbf{V}_\pi^{(k)}
    \]
    当 $k \to \infty$ 时，$\mathbf{V}_\pi^{(k)} \to \mathbf{V}_\pi$。
\end{enumerate}

\section{强化学习算法分类}

强化学习算法可以从多个维度进行分类，理解这些分类有助于我们选择合适的算法解决特定问题。

\subsection{有模型 vs 无模型}

\begin{table}[H]
\centering
\caption{有模型方法与无模型方法的对比}
\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
\toprule
\textbf{有模型方法（Model-Based）} & \textbf{无模型方法（Model-Free）} \\
\midrule
\textbf{核心思想}：拥有环境模型（$P$ 和 $R$），可以通过规划（Planning）求解最优策略 & \textbf{核心思想}：不依赖环境模型，通过与环境的直接交互学习最优策略 \\
\hline
\textbf{环境知识}：需要知道状态转移概率 $p(s' \mid s, a)$ 和奖励函数 $R(s, a)$ & \textbf{环境知识}：不需要知道 $P$ 和 $R$，只需与环境交互获得样本 $(s, a, r, s')$ \\
\hline
\textbf{典型算法}：
\begin{itemize}
    \item 值迭代（Value Iteration）
    \item 策略迭代（Policy Iteration）
    \item 蒙特卡洛树搜索（MCTS）
\end{itemize} & \textbf{典型算法}：
\begin{itemize}
    \item Q-learning
    \item SARSA
    \item DQN
    \item 策略梯度方法
\end{itemize} \\
\hline
\textbf{优点}：
\begin{itemize}
    \item 样本效率高（可用模型生成虚拟数据）
    \item 可进行规划，无需实际交互
\end{itemize} & \textbf{优点}：
\begin{itemize}
    \item 更通用，不依赖环境模型
    \item 适用于复杂、未知环境
\end{itemize} \\
\hline
\textbf{缺点}：
\begin{itemize}
    \item 需要准确的环境模型
    \item 模型误差会累积影响策略
\end{itemize} & \textbf{缺点}：
\begin{itemize}
    \item 样本效率较低
    \item 需要大量交互数据
\end{itemize} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{有模型方法的进一步分类}
\begin{enumerate}
    \item \textbf{给定环境模型}：环境模型已知且准确
    \begin{itemize}
        \item \textbf{示例}：棋类游戏（规则明确）
        \item \textbf{应用}：AlphaGo中的蒙特卡洛树搜索
    \end{itemize}
    
    \item \textbf{学习环境模型}：从交互数据中学习环境模型
    \begin{itemize}
        \item \textbf{方法}：先用数据学习 $p(s' \mid s, a)$ 和 $R(s, a)$ 的估计
        \item \textbf{算法}：Dyna框架（Sutton, 1991）
        \item \textbf{流程}：
        \begin{enumerate}
            \item 用真实交互数据更新模型
            \item 用模型生成虚拟数据
            \item 用虚拟数据更新价值函数和策略
        \end{enumerate}
    \end{itemize}
\end{enumerate}

\subsection{基于价值 vs 基于策略}

\begin{table}[H]
\centering
\caption{基于价值方法与基于策略方法的对比}
\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
\toprule
\textbf{基于价值方法（Value-Based）} & \textbf{基于策略方法（Policy-Based）} \\
\midrule
\textbf{核心思想}：学习价值函数（$V$ 或 $Q$），通过价值函数导出策略 & \textbf{核心思想}：直接学习策略函数 $\pi(a \mid s; \theta)$，优化策略参数 $\theta$ \\
\hline
\textbf{策略表示}：通常是确定性的，$\pi(s) = \arg\max_a Q(s, a)$ & \textbf{策略表示}：可以是确定性的或随机性的，直接参数化 \\
\hline
\textbf{典型算法}：
\begin{itemize}
    \item Q-learning
    \item DQN
    \item SARSA
\end{itemize} & \textbf{典型算法}：
\begin{itemize}
    \item REINFORCE
    \item 策略梯度
    \item TRPO/PPO
\end{itemize} \\
\hline
\textbf{优点}：
\begin{itemize}
    \item 通常更稳定，收敛性较好
    \item 样本效率相对较高
\end{itemize} & \textbf{优点}：
\begin{itemize}
    \item 能处理连续动作空间
    \item 能学习随机策略
    \item 收敛到局部最优
\end{itemize} \\
\hline
\textbf{缺点}：
\begin{itemize}
    \item 难以处理连续动作空间
    \item 通常得到确定性策略
\end{itemize} & \textbf{缺点}：
\begin{itemize}
    \item 方差大，训练不稳定
    \item 样本效率较低
\end{itemize} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{基于价值方法的策略推导}
在基于价值的方法中，策略通常从价值函数中推导出来：

\begin{enumerate}
    \item \textbf{贪婪策略}：总是选择价值最高的动作
    \[
    \pi(s) = \arg\max_{a \in \mathcal{A}} Q(s, a)
    \]
    
    \item \textbf{$\epsilon$-贪婪策略}：以 $1-\epsilon$ 的概率选择最优动作，以 $\epsilon$ 的概率随机选择动作
    \[
    \pi(a \mid s) = 
    \begin{cases}
    1 - \epsilon + \frac{\epsilon}{|\mathcal{A}|} & \text{if } a = \arg\max_{a'} Q(s, a') \\
    \frac{\epsilon}{|\mathcal{A}|} & \text{otherwise}
    \end{cases}
    \]
    
    \item \textbf{Boltzmann探索（Softmax）}：按价值大小分配选择概率
    \[
    \pi(a \mid s) = \frac{\exp(Q(s, a)/\tau)}{\sum_{a' \in \mathcal{A}} \exp(Q(s, a')/\tau)}
    \]
    其中 $\tau > 0$ 是温度参数，控制探索程度。
\end{enumerate}

\subsubsection{基于策略方法的目标函数}
基于策略的方法直接优化策略参数 $\theta$ 以最大化期望回报：
\[
J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}[R(\tau)] = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[\sum_{t=0}^T \gamma^t r_t\right]
\]
其中 $p_\theta(\tau)$ 是由策略 $\pi_\theta$ 生成轨迹 $\tau$ 的概率。

使用策略梯度定理，可以得到梯度：
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t \mid s_t) \cdot G_t\right]
\]
其中 $G_t = \sum_{k=t}^T \gamma^{k-t} r_k$ 是从时刻 $t$ 开始的累计回报。

\subsection{蒙特卡洛 vs 时序差分}

\begin{table}[H]
\centering
\caption{蒙特卡洛方法与时序差分方法的对比}
\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
\toprule
\textbf{蒙特卡洛方法（Monte Carlo, MC）} & \textbf{时序差分方法（Temporal Difference, TD）} \\
\midrule
\textbf{更新时机}：必须等到回合结束才能更新 & \textbf{更新时机}：每一步都可以立即更新 \\
\hline
\textbf{目标值}：使用实际回报 $G_t = \sum_{k=t}^T \gamma^{k-t} r_k$ & \textbf{目标值}：使用TD目标 $r_t + \gamma V(s_{t+1})$ \\
\hline
\textbf{偏差-方差}：无偏但高方差 & \textbf{偏差-方差}：有偏但低方差 \\
\hline
\textbf{更新公式（状态价值）}：
$V(s_t) \leftarrow V(s_t) + \alpha [G_t - V(s_t)]$ & \textbf{更新公式（状态价值）}：
$V(s_t) \leftarrow V(s_t) + \alpha [r_t + \gamma V(s_{t+1}) - V(s_t)]$ \\
\hline
\textbf{收敛性}：保证收敛到真实价值 & \textbf{收敛性}：在一定条件下收敛 \\
\hline
\textbf{适用场景}：
\begin{itemize}
    \item 回合制任务
    \item 需要无偏估计
\end{itemize} & \textbf{适用场景}：
\begin{itemize}
    \item 连续任务
    \item 在线学习
    \item 需要快速更新
\end{itemize} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{蒙特卡洛方法的偏差与方差分析}
\begin{itemize}
    \item \textbf{无偏性}：蒙特卡洛估计是真实期望的无偏估计，因为 $G_t$ 是 $V(s_t)$ 的无偏估计。
    \[
    \mathbb{E}[G_t \mid S_t = s] = V_\pi(s)
    \]
    
    \item \textbf{高方差}：$G_t$ 依赖于整个后续轨迹，随机性积累导致高方差。
    \[
    \text{Var}[G_t] = \text{Var}\left[\sum_{k=t}^T \gamma^{k-t} R_k\right] = \sum_{k=t}^T \gamma^{2(k-t)} \text{Var}[R_k] + \text{交叉项}
    \]
\end{itemize}

\subsubsection{时序差分方法的偏差与方差分析}
\begin{itemize}
    \item \textbf{有偏性}：TD目标 $r_t + \gamma V(s_{t+1})$ 是 $V(s_t)$ 的有偏估计，因为 $V(s_{t+1})$ 本身是估计值。
    \[
    \mathbb{E}[r_t + \gamma V(s_{t+1}) \mid S_t = s] \neq V_\pi(s) \quad \text{除非 } V(s_{t+1}) = V_\pi(s_{t+1})
    \]
    
    \item \textbf{低方差}：TD目标只依赖于单步奖励和下一个状态的价值估计，随机性较少。
    \[
    \text{Var}[r_t + \gamma V(s_{t+1})] \approx \text{Var}[r_t] + \gamma^2 \text{Var}[V(s_{t+1})]
    \]
\end{itemize}

\subsection{在线策略 vs 离线策略}

\begin{table}[H]
\centering
\caption{在线策略方法与离线策略方法的对比}
\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
\toprule
\textbf{在线策略方法（On-Policy）} & \textbf{离线策略方法（Off-Policy）} \\
\midrule
\textbf{行为策略与目标策略}：相同 & \textbf{行为策略与目标策略}：可以不同 \\
\hline
\textbf{数据收集}：使用当前策略收集数据 & \textbf{数据收集}：可以使用任意策略收集数据 \\
\hline
\textbf{数据利用}：只能使用当前策略产生的数据 & \textbf{数据利用}：可以使用历史数据、专家数据等 \\
\hline
\textbf{探索方式}：策略本身必须有一定的探索性 & \textbf{探索方式}：行为策略负责探索，目标策略可以更贪婪 \\
\hline
\textbf{典型算法}：
\begin{itemize}
    \item SARSA
    \item REINFORCE
    \item TRPO
    \item PPO
\end{itemize} & \textbf{典型算法}：
\begin{itemize}
    \item Q-learning
    \item DQN
    \item DDPG
    \item SAC
\end{itemize} \\
\hline
\textbf{优点}：
\begin{itemize}
    \item 理论分析相对简单
    \item 通常更稳定
\end{itemize} & \textbf{优点}：
\begin{itemize}
    \item 数据重用，样本效率高
    \item 可以学习多个策略
    \item 支持从示范中学习
\end{itemize} \\
\hline
\textbf{缺点}：
\begin{itemize}
    \item 样本效率低
    \item 探索与利用需要平衡
\end{itemize} & \textbf{缺点}：
\begin{itemize}
    \item 理论分析复杂
    \item 可能存在收敛问题
\end{itemize} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{SARSA：在线策略TD控制算法}
SARSA（State-Action-Reward-State-Action）是经典的在线策略TD控制算法：

\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_t + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right]
\]

\begin{itemize}
    \item \textbf{名称来源}：使用五元组 $(S_t, A_t, R_t, S_{t+1}, A_{t+1})$
    \item \textbf{在线策略}：$A_t$ 和 $A_{t+1}$ 都来自当前策略（通常为 $\epsilon$-贪婪策略）
    \item \textbf{更新目标}：$R_t + \gamma Q(S_{t+1}, A_{t+1})$，其中 $A_{t+1}$ 是实际采取的动作
\end{itemize}

\subsubsection{Q-learning：离线策略TD控制算法}
Q-learning是最著名的离线策略TD控制算法：

\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_t + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \right]
\]

\begin{itemize}
    \item \textbf{离线策略}：$A_t$ 来自行为策略（如 $\epsilon$-贪婪），但更新时使用目标策略（贪婪策略）
    \item \textbf{更新目标}：$R_t + \gamma \max_{a'} Q(S_{t+1}, a')$，使用最优动作的价值
    \item \textbf{收敛性}：在一定条件下，Q-learning能收敛到最优Q函数
\end{itemize}

\subsubsection{经验回放（Experience Replay）}
经验回放是深度Q网络（DQN）中的关键技术，属于离线策略方法：

\begin{enumerate}
    \item \textbf{存储经验}：将经验元组 $(s_t, a_t, r_t, s_{t+1})$ 存入回放缓冲区（Replay Buffer）
    \item \textbf{随机采样}：训练时从缓冲区中随机采样小批量经验
    \item \textbf{打破相关性}：随机采样打破了经验之间的时间相关性，提高稳定性
    \item \textbf{数据重用}：同一份经验可以多次用于训练，提高样本效率
\end{enumerate}

\section{Actor-Critic方法：价值与策略的结合}

\subsection{Actor-Critic框架}

Actor-Critic方法结合了基于价值方法和基于策略方法的优点：

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=2cm,
    actor/.style={rectangle, draw, minimum width=2.5cm, minimum height=1.5cm, fill=blue!20},
    critic/.style={rectangle, draw, minimum width=2.5cm, minimum height=1.5cm, fill=red!20},
    env/.style={rectangle, draw, minimum width=2cm, minimum height=1cm, fill=green!20}
]
\node[actor] (actor) {Actor (策略)};
\node[critic, below=of actor] (critic) {Critic (价值)};
\node[env, right=of actor] (env) {环境};

\draw[->, thick] (actor) -- node[above] {动作 $a_t$} (env);
\draw[->, thick] (env) -- node[below] {状态 $s_t$, 奖励 $r_t$} (actor);
\draw[<->, thick, dashed] (actor) -- node[right] {评估与指导} (critic);
\draw[->, thick] (env.south) |- node[pos=0.7, left] {状态 $s_t$} (critic.east);
\end{tikzpicture}
\caption{Actor-Critic框架示意图}
\end{figure}

\begin{itemize}
    \item \textbf{Actor（执行者）}：负责选择动作，即策略 $\pi_\theta(a \mid s)$
    \item \textbf{Critic（评论者）}：负责评估状态或状态-动作对的价值，即价值函数 $V_\phi(s)$ 或 $Q_\phi(s, a)$
    \item \textbf{协同工作}：
    \begin{enumerate}
        \item Actor根据当前策略选择动作
        \item Critic评估Actor选择的动作的好坏
        \item Critic的评估指导Actor更新策略
    \end{enumerate}
\end{itemize}

\subsection{优势函数（Advantage Function）}

优势函数是Actor-Critic方法中的核心概念，用于衡量一个动作相对于平均水平的优势：

\[
A_\pi(s, a) = Q_\pi(s, a) - V_\pi(s)
\]

\begin{itemize}
    \item \textbf{直观理解}：在状态 $s$ 下选择动作 $a$ 比随机选择动作平均好多少
    \item \textbf{性质}：
    \begin{itemize}
        \item 如果 $A_\pi(s, a) > 0$，说明动作 $a$ 优于平均水平
        \item 如果 $A_\pi(s, a) < 0$，说明动作 $a$ 劣于平均水平
        \item $\mathbb{E}_{a \sim \pi(\cdot \mid s)}[A_\pi(s, a)] = 0$（所有动作的优势平均为0）
    \end{itemize}
\end{itemize}

\subsection{策略梯度与Actor-Critic}

结合优势函数的策略梯度定理：

\[
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t \mid s_t) \cdot A_\pi(s_t, a_t)\right]
\]

\begin{itemize}
    \item \textbf{REINFORCE算法}：使用蒙特卡洛回报 $G_t$ 作为优势估计
    \[
    \nabla_\theta J(\theta) \approx \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t \mid s_t) \cdot (G_t - b(s_t))
    \]
    其中 $b(s_t)$ 是基线（baseline），通常取 $V(s_t)$ 以减少方差。
    
    \item \textbf{Actor-Critic算法}：使用Critic估计的优势函数
    \[
    \nabla_\theta J(\theta) \approx \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t \mid s_t) \cdot A_\phi(s_t, a_t)
    \]
    其中 $A_\phi(s_t, a_t)$ 是Critic估计的优势函数。
\end{itemize}

\subsection{现代Actor-Critic算法}

\subsubsection{优势Actor-Critic（A2C/A3C）}
\begin{itemize}
    \item \textbf{A2C（同步优势Actor-Critic）}：同步更新多个Worker
    \item \textbf{A3C（异步优势Actor-Critic）}：异步更新多个Worker，提高训练速度
    \item \textbf{优势估计}：使用n步TD误差估计优势
    \[
    A(s_t, a_t) = \sum_{k=0}^{n-1} \gamma^k r_{t+k} + \gamma^n V(s_{t+n}) - V(s_t)
    \]
\end{itemize}

\subsubsection{近端策略优化（PPO）}
PPO通过限制策略更新的幅度来提高训练稳定性：

\[
J^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\right)\right]
\]

其中 $r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}$ 是概率比，$\hat{A}_t$ 是优势估计，$\epsilon$ 是超参数。

\subsubsection{深度确定性策略梯度（DDPG）}
DDPG结合了DQN和确定性策略梯度，适用于连续动作空间：

\begin{itemize}
    \item \textbf{Actor}：确定性策略 $\mu_\theta(s)$，输出连续动作
    \item \textbf{Critic}：动作价值函数 $Q_\phi(s, a)$
    \item \textbf{关键技术}：
    \begin{enumerate}
        \item 经验回放
        \item 目标网络（Target Network）
        \item 确定性策略梯度定理：
        \[
        \nabla_\theta J(\theta) \approx \mathbb{E}_{s \sim \mathcal{D}}\left[\nabla_\theta \mu_\theta(s) \nabla_a Q_\phi(s, a)\big|_{a=\mu_\theta(s)}\right]
        \]
    \end{enumerate}
\end{itemize}

\section{总结与展望}

\subsection{强化学习算法分类总结}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
\node[draw, rectangle, rounded corners, fill=blue!10, minimum width=3cm, minimum height=1cm] (rl) at (0,0) {强化学习};
\node[draw, rectangle, rounded corners, fill=green!10, minimum width=2.5cm, minimum height=0.8cm] (model) at (-3, -2) {有模型};
\node[draw, rectangle, rounded corners, fill=green!10, minimum width=2.5cm, minimum height=0.8cm] (modelfree) at (3, -2) {无模型};

\node[draw, rectangle, rounded corners, fill=yellow!10, minimum width=2cm, minimum height=0.6cm] (dp) at (-3, -3.5) {动态规划};
\node[draw, rectangle, rounded corners, fill=yellow!10, minimum width=2cm, minimum height=0.6cm] (value) at (1.5, -3.5) {基于价值};
\node[draw, rectangle, rounded corners, fill=yellow!10, minimum width=2cm, minimum height=0.6cm] (policy) at (4.5, -3.5) {基于策略};

\node[draw, rectangle, rounded corners, fill=orange!10, minimum width=1.8cm, minimum height=0.5cm] (mc) at (0.5, -5) {蒙特卡洛};
\node[draw, rectangle, rounded corners, fill=orange!10, minimum width=1.8cm, minimum height=0.5cm] (td) at (3.5, -5) {时序差分};

\node[draw, rectangle, rounded corners, fill=red!10, minimum width=1.6cm, minimum height=0.4cm] (on) at (3.5, -6) {在线策略};
\node[draw, rectangle, rounded corners, fill=red!10, minimum width=1.6cm, minimum height=0.4cm] (off) at (7, -6) {离线策略};

\draw[->] (rl) -- (model);
\draw[->] (rl) -- (modelfree);
\draw[->] (model) -- (dp);
\draw[->] (modelfree) -- (value);
\draw[->] (modelfree) -- (policy);
\draw[->] (value) -- (mc);
\draw[->] (value) -- (td);
\draw[->] (td) -- (on);
\draw[->] (td) -- (off);

\node at (0, -7) {典型算法：};
\node at (-3.5, -7.5) {值迭代、策略迭代};
\node at (3.5, -7.5) {Q-learning、DQN};
\node at (10, -7.5) {REINFORCE、PPO};
\end{tikzpicture}
\caption{强化学习算法分类树}
\end{figure}

\subsection{强化学习的挑战与前沿}

\subsubsection{主要挑战}
\begin{enumerate}
    \item \textbf{样本效率}：强化学习通常需要大量交互数据
    \begin{itemize}
        \item \textbf{解决方法}：经验回放、模型学习、模仿学习
    \end{itemize}
    
    \item \textbf{探索与利用的平衡}
    \begin{itemize}
        \item \textbf{解决方法}：$\epsilon$-贪婪、UCB、Thompson采样、好奇心驱动探索
    \end{itemize}
    
    \item \textbf{稳定性与收敛性}
    \begin{itemize}
        \item \textbf{解决方法}：目标网络、软更新、信任域方法
    \end{itemize}
    
    \item \textbf{稀疏奖励问题}
    \begin{itemize}
        \item \textbf{解决方法}：课程学习、分层强化学习、内在动机
    \end{itemize}
    
    \item \textbf{安全性}
    \begin{itemize}
        \item \textbf{解决方法}：约束强化学习、安全探索
    \end{itemize}
\end{enumerate}

\subsubsection{前沿方向}
\begin{enumerate}
    \item \textbf{深度强化学习（Deep RL）}：将深度学习与强化学习结合
    \begin{itemize}
        \item \textbf{DQN}：深度Q网络，开启深度强化学习时代
        \item \textbf{AlphaGo}：结合蒙特卡洛树搜索与深度神经网络
        \item \textbf{AlphaZero}：从零开始自我对弈学习
    \end{itemize}
    
    \item \textbf{多智能体强化学习（Multi-Agent RL）}
    \begin{itemize}
        \item \textbf{协作}：智能体协作完成共同任务
        \item \textbf{竞争}：智能体在竞争环境中学习
        \item \textbf{混合}：既有协作又有竞争
    \end{itemize}
    
    \item \textbf{元强化学习（Meta-RL）}
    \begin{itemize}
        \item \textbf{目标}：学会如何快速学习新任务
        \item \textbf{应用}：Few-shot学习、快速适应
    \end{itemize}
    
    \item \textbf{离线强化学习（Offline RL）}
    \begin{itemize}
        \item \textbf{特点}：从固定的数据集学习，不与环境交互
        \item \textbf{应用}：医疗、自动驾驶等高风险领域
    \end{itemize}
    
    \item \textbf{强化学习与大型语言模型结合}
    \begin{itemize}
        \item \textbf{RLHF}：基于人类反馈的强化学习
        \item \textbf{应用}：ChatGPT等对话系统的对齐
    \end{itemize}
\end{enumerate}

强化学习是一门理论与实践并重的学科，需要在理解数学原理的基础上，通过实际编码和调参来积累经验。随着计算能力的提升和算法的进步，强化学习正在越来越多的领域展现出强大的潜力。