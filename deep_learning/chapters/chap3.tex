\chapter{深度强化学习之价值学习}
\section{价值学习：从评估到决策}

\subsection{价值学习的核心思想}

在强化学习中，价值学习（Value-Based Learning）是一类重要的方法。与直接学习策略（策略学习）不同，价值学习的核心思想是学习一个\textbf{价值函数}（Value Function），这个函数用于评估在某个状态（或状态-动作对）下的"好坏"程度。

\begin{definition}[最优动作价值函数]
最优动作价值函数 $Q^*(s, a)$ 定义为在所有可能的策略中，从状态 $s$ 开始、执行动作 $a$ 后能获得的最大期望回报：
\[
Q^*(s, a) = \max_{\pi} Q_{\pi}(s, a), \quad \forall s \in \mathcal{S}, a \in \mathcal{A}
\]
其中 $Q_{\pi}(s, a)$ 是在策略 $\pi$ 下的动作价值函数。
\end{definition}

\textbf{关键洞察}：一旦我们知道了最优动作价值函数 $Q^*(s, a)$，最优策略就可以直接得到：在每个状态 $s$ 下，选择使 $Q^*(s, a)$ 最大的动作：
\[
\pi^*(s) = \arg\max_{a \in \mathcal{A}} Q^*(s, a)
\]

\textbf{价值学习的优点}：
\begin{itemize}
    \item \textbf{直观性}：Q值直接反映了动作的"好坏"。
    \item \textbf{稳定性}：相比策略学习方法，价值学习通常更稳定。
    \item \textbf{可解释性}：Q值表或Q网络提供了对决策过程的直观理解。
\end{itemize}

\textbf{价值学习的挑战}：
\begin{itemize}
    \item \textbf{维度灾难}：当状态空间或动作空间很大时，存储所有状态-动作对的Q值变得不可行。
    \item \textbf{连续空间}：对于连续状态或动作空间，无法枚举所有可能情况。
    \item \textbf{泛化能力}：需要从有限的经验中泛化到未见过的状态。
\end{itemize}

\section{Q-Learning：经典的价值学习算法}

\subsection{算法原理}

Q-Learning是强化学习中最经典、最重要的算法之一。它是一种\textbf{离线策略}（off-policy）的\textbf{时序差分}（Temporal Difference, TD）学习方法。

\begin{definition}[Q-Learning更新规则]
Q-Learning通过以下规则更新Q值估计：
\[
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
\]
其中：
\begin{itemize}
    \item $\alpha \in (0, 1]$ 是学习率（步长）
    \item $\gamma \in [0, 1]$ 是折扣因子
    \item $r$ 是执行动作 $a$ 后获得的即时奖励
    \item $s'$ 是转移后的新状态
    \item $\max_{a'} Q(s', a')$ 是在新状态 $s'$ 下所有可能动作的最大Q值
\end{itemize}
\end{definition}

\textbf{直观理解}：
\begin{itemize}
    \item $Q(s, a)$ 是当前对在状态 $s$ 下执行动作 $a$ 的价值的估计。
    \item $r + \gamma \max_{a'} Q(s', a')$ 是TD目标，包含两部分：
    \begin{enumerate}
        \item 即时奖励 $r$
        \item 折扣后的未来最大可能价值 $\gamma \max_{a'} Q(s', a')$
    \end{enumerate}
    \item $r + \gamma \max_{a'} Q(s', a') - Q(s, a)$ 是TD误差，表示当前估计与目标之间的差距。
    \item 学习率 $\alpha$ 控制更新步长：$\alpha$ 越大，更新越快，但可能不稳定；$\alpha$ 越小，更新越慢，但更稳定。
\end{itemize}

\subsection{表格Q-Learning}

在状态空间和动作空间都很小的情况下，我们可以使用表格形式存储所有状态-动作对的Q值。

\begin{table}[H]
\centering
\caption{表格Q-Learning中的Q值表示例}
\begin{tabular}{c|cccc}
\hline
\textbf{状态} & \textbf{动作1} & \textbf{动作2} & \textbf{动作3} & \textbf{动作4} \\
\hline
状态0 & $Q(0,1)$ & $Q(0,2)$ & $Q(0,3)$ & $Q(0,4)$ \\
状态1 & $Q(1,1)$ & $Q(1,2)$ & $Q(1,3)$ & $Q(1,4)$ \\
状态2 & $Q(2,1)$ & $Q(2,2)$ & $Q(2,3)$ & $Q(2,4)$ \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
状态m & $Q(m,1)$ & $Q(m,2)$ & $Q(m,3)$ & $Q(m,4)$ \\
\hline
\end{tabular}
\end{table}

\begin{algorithm}[H]
\caption{表格Q-Learning算法}
\begin{algorithmic}[1]
\REQUIRE 学习率 $\alpha$，折扣因子 $\gamma$，探索率 $\epsilon$
\ENSURE 最优动作价值函数 $Q^*(s, a)$
\STATE 初始化 $Q(s, a)$ 为任意值（通常为0或随机小值）
\FOR{每个回合（episode）}
    \STATE 初始化状态 $s$
    \WHILE{$s$ 不是终止状态}
        \STATE 根据 $\epsilon$-贪婪策略从 $s$ 选择动作 $a$：
        \[
        a = \begin{cases}
        \text{随机动作} & \text{以概率 } \epsilon \\
        \arg\max_{a'} Q(s, a') & \text{以概率 } 1-\epsilon
        \end{cases}
        \]
        \STATE 执行动作 $a$，观察奖励 $r$ 和新状态 $s'$
        \STATE 更新Q值：$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$
        \STATE $s \leftarrow s'$
    \ENDWHILE
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{探索与利用的平衡：$\epsilon$-贪婪策略}

在Q-Learning中，$\epsilon$-贪婪策略用于平衡探索（尝试新动作）和利用（选择已知最佳动作）：

\[
\pi(a|s) = \begin{cases}
1 - \epsilon + \frac{\epsilon}{|\mathcal{A}|} & \text{如果 } a = \arg\max_{a'} Q(s, a') \\
\frac{\epsilon}{|\mathcal{A}|} & \text{其他动作}
\end{cases}
\]

\begin{itemize}
    \item $\epsilon \in [0, 1]$ 是探索率
    \item 以概率 $1-\epsilon$ 选择当前认为最好的动作（利用）
    \item 以概率 $\epsilon$ 随机选择动作（探索）
    \item 通常随着训练进行，$\epsilon$ 会逐渐减小（从高探索到高利用）
\end{itemize}

\subsection{Q-Learning与SARSA的比较}

\begin{table}[H]
\centering
\caption{Q-Learning与SARSA的比较}
\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
\toprule
\textbf{Q-Learning（离线策略）} & \textbf{SARSA（在线策略）} \\
\midrule
\textbf{更新公式}：$Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$ & \textbf{更新公式}：$Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma Q(s',a') - Q(s,a)]$ \\
\hline
\textbf{目标策略}：贪婪策略 $\pi(s) = \arg\max_{a} Q(s,a)$ & \textbf{目标策略}：与行为策略相同（通常为$\epsilon$-贪婪） \\
\hline
\textbf{行为策略}：$\epsilon$-贪婪策略 & \textbf{行为策略}：$\epsilon$-贪婪策略 \\
\hline
\textbf{TD目标}：$r + \gamma \max_{a'} Q(s',a')$ & \textbf{TD目标}：$r + \gamma Q(s',a')$ \\
\hline
\textbf{学习对象}：最优Q函数 $Q^*$ & \textbf{学习对象}：当前策略的Q函数 $Q_\pi$ \\
\hline
\textbf{探索影响}：行为策略探索不影响目标策略 & \textbf{探索影响}：探索直接影响学习策略 \\
\hline
\textbf{收敛性}：收敛到最优策略（在适当条件下） & \textbf{收敛性}：收敛到$\epsilon$-贪婪策略的最优策略 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{关键区别}：
\begin{itemize}
    \item Q-Learning在计算TD目标时使用$\max_{a'} Q(s',a')$，假设下一个动作是最优的。
    \item SARSA在计算TD目标时使用实际采取的下一个动作$Q(s',a')$，更"谨慎"。
    \item 这导致Q-Learning更"乐观"，SARSA更"保守"。
\end{itemize}

\section{深度Q网络：当Q-Learning遇见深度学习}

\subsection{表格方法的局限性}

表格Q-Learning虽然直观，但面临严重限制：

\begin{enumerate}
    \item \textbf{维度灾难}：状态空间随维度指数增长。例如：
    \begin{itemize}
        \item 围棋：$10^{170}$ 个状态，无法存储
        \item Atari游戏：$256^{84\times84}$ 个可能状态，天文数字
        \item 连续状态空间：无限多个状态
    \end{itemize}
    
    \item \textbf{泛化能力差}：每个状态-动作对独立学习，无法泛化到相似状态。
    
    \item \textbf{内存需求巨大}：存储所有Q值需要大量内存。
    
    \item \textbf{学习效率低}：每个状态需要单独学习，无法利用状态间的相似性。
\end{enumerate}

\subsection{深度Q网络的基本思想}

深度Q网络（Deep Q-Network, DQN）的核心思想是用神经网络来近似Q函数，从而解决维度灾难问题。

\begin{definition}[深度Q网络]
深度Q网络是一个参数化的函数 $Q(s, a; \theta)$，其中 $\theta$ 是神经网络的参数。对于给定的状态 $s$，网络输出所有可能动作的Q值：
\[
f(s; \theta) \approx [Q(s, a_1), Q(s, a_2), \ldots, Q(s, a_n)]
\]
其中 $n$ 是动作空间的大小。
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{D:/code LateX/deep_learning/picture/net.png}
\caption{深度Q网络结构示意图：输入状态，输出每个动作的Q值}
\end{figure}

\textbf{网络架构}：
\begin{itemize}
    \item \textbf{输入}：状态表示（如图像像素、特征向量等）
    \item \textbf{隐藏层}：多个全连接层或卷积层，用于提取特征
    \item \textbf{输出层}：每个输出节点对应一个动作的Q值
\end{itemize}

\subsection{DQN的训练目标与损失函数}

DQN的训练目标是找到参数 $\theta$，使得 $Q(s, a; \theta)$ 近似最优Q函数 $Q^*(s, a)$。

\begin{definition}[DQN损失函数]
使用均方误差（MSE）作为损失函数：
\[
L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( Q(s, a; \theta) - y \right)^2 \right]
\]
其中：
\begin{itemize}
    \item $(s, a, r, s')$ 是从经验回放缓冲区 $\mathcal{D}$ 中采样的转移
    \item $y$ 是TD目标：$y = r + \gamma \max_{a'} Q(s', a'; \theta^-)$
    \item $\theta^-$ 是目标网络的参数（与在线网络 $\theta$ 不同）
\end{itemize}
\end{definition}

\textbf{梯度计算}：
\[
\nabla_{\theta} L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( Q(s, a; \theta) - y \right) \cdot \nabla_{\theta} Q(s, a; \theta) \right]
\]

\textbf{参数更新}：
\[
\theta \leftarrow \theta - \alpha \cdot \nabla_{\theta} L(\theta)
\]
其中 $\alpha$ 是学习率。

\subsection{DQN的训练流程}

\begin{algorithm}[H]
\caption{深度Q网络（DQN）算法}
\begin{algorithmic}[1]
\REQUIRE 经验回放缓冲区容量 $N$，目标网络更新频率 $C$，折扣因子 $\gamma$，探索率 $\epsilon$
\STATE 初始化在线网络 $Q(\cdot; \theta)$ 的参数 $\theta$ 随机
\STATE 初始化目标网络 $Q(\cdot; \theta^-)$ 的参数 $\theta^- \leftarrow \theta$
\STATE 初始化经验回放缓冲区 $\mathcal{D}$ 为空，容量为 $N$
\FOR{每个回合（episode）}
    \STATE 初始化状态 $s_1$ 和预处理 $\phi_1 = \phi(s_1)$
    \FOR{$t = 1$ 到 $T$}
        \STATE 以概率 $\epsilon$ 选择随机动作 $a_t$，否则 $a_t = \arg\max_{a} Q(\phi(s_t), a; \theta)$
        \STATE 执行动作 $a_t$，观察奖励 $r_t$ 和下一状态 $s_{t+1}$
        \STATE 预处理 $\phi_{t+1} = \phi(s_{t+1})$
        \STATE 存储转移 $(\phi_t, a_t, r_t, \phi_{t+1})$ 到 $\mathcal{D}$
        \STATE 从 $\mathcal{D}$ 中随机采样小批量转移 $(\phi_j, a_j, r_j, \phi_{j+1})$
        \STATE 计算TD目标：
        \[
        y_j = \begin{cases}
        r_j & \text{如果 } \phi_{j+1} \text{ 是终止状态} \\
        r_j + \gamma \max_{a'} Q(\phi_{j+1}, a'; \theta^-) & \text{否则}
        \end{cases}
        \]
        \STATE 计算损失：$L = \frac{1}{m} \sum_{j=1}^{m} (y_j - Q(\phi_j, a_j; \theta))^2$
        \STATE 使用梯度下降更新 $\theta$
        \STATE 每 $C$ 步更新目标网络：$\theta^- \leftarrow \theta$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{DQN的核心技术}

\subsection{经验回放（Experience Replay）}

经验回放是DQN成功的关键技术之一，解决了两个核心问题：

\begin{enumerate}
    \item \textbf{数据效率}：每个转移可以被多次使用，提高数据利用率。
    \item \textbf{相关性破坏}：随机采样打破了连续状态之间的相关性，使训练更稳定。
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{D:/code LateX/deep_learning/picture/recall.png}
\caption{经验回放机制：存储历史转移并随机采样用于训练}
\end{figure}

\begin{definition}[经验回放缓冲区]
经验回放缓冲区 $\mathcal{D}$ 是一个固定大小的循环缓冲区，存储转移元组 $(s, a, r, s', \text{done})$，其中：
\begin{itemize}
    \item $s$：当前状态
    \item $a$：执行的动作
    \item $r$：获得的奖励
    \item $s'$：转移后的新状态
    \item $\text{done}$：是否到达终止状态
\end{itemize}
\end{definition}

\textbf{经验回放的工作原理}：
\begin{enumerate}
    \item \textbf{收集}：智能体与环境交互，将每个转移存储到缓冲区。
    \item \textbf{采样}：训练时随机从缓冲区采样小批量转移。
    \item \textbf{学习}：使用采样到的转移计算损失并更新网络。
\end{enumerate}

\textbf{经验回放的优势}：
\begin{itemize}
    \item \textbf{打破相关性}：连续的状态高度相关，直接用于训练会导致梯度更新方向高度相关，训练不稳定。
    \item \textbf{数据重用}：每个转移可以被多次用于训练，提高数据效率。
    \item \textbf{平滑分布}：随机采样使数据分布更平稳，减少方差。
\end{itemize}

\subsection{目标网络（Target Network）}

目标网络是DQN的另一项关键技术，解决了训练中的不稳定性问题。

\begin{definition}[目标网络]
目标网络 $Q(\cdot; \theta^-)$ 是与在线网络 $Q(\cdot; \theta)$ 结构相同的网络，但参数更新更慢：
\begin{itemize}
    \item \textbf{硬更新}：每 $C$ 步将在线网络的参数复制给目标网络：$\theta^- \leftarrow \theta$
    \item \textbf{软更新}：每次迭代按比例更新：$\theta^- \leftarrow \tau \theta + (1-\tau) \theta^-$，其中 $\tau \ll 1$
\end{itemize}
\end{definition}

\textbf{目标网络的作用}：
\begin{enumerate}
    \item \textbf{稳定训练}：TD目标 $y = r + \gamma \max_{a'} Q(s', a'; \theta^-)$ 在一段时间内固定，减少了目标的波动。
    \item \textbf{避免发散}：防止Q值估计的"追逐尾巴"现象（目标随估计不断变化，导致训练发散）。
    \item \textbf{缓解过估计}：目标网络使用旧参数，减少了最大化操作带来的过估计问题。
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{D:/code LateX/deep_learning/picture/dqnloss.png}
\caption{DQN损失计算：使用目标网络计算TD目标}
\end{figure}

\textbf{为什么需要目标网络？}

考虑没有目标网络的情况：
\[
y = r + \gamma \max_{a'} Q(s', a'; \theta)
\]
损失函数：
\[
L(\theta) = \mathbb{E}[(Q(s, a; \theta) - y)^2]
\]
问题：目标 $y$ 依赖于正在优化的参数 $\theta$，导致：
\begin{enumerate}
    \item 目标不断变化，训练不稳定。
    \item Q值可能发散（"追逐尾巴"问题）。
\end{enumerate}

使用目标网络后：
\[
y = r + \gamma \max_{a'} Q(s', a'; \theta^-)
\]
目标网络参数 $\theta^-$ 更新较慢，提供了更稳定的学习目标。

\section{DQN的改进与扩展}

\subsection{优先经验回放（Prioritized Experience Replay）}

标准经验回放均匀采样，但不同转移的重要性不同。优先经验回放根据TD误差的绝对值赋予不同采样优先级。

\begin{definition}[采样优先级]
转移 $i$ 的采样概率为：
\[
P(i) = \frac{p_i^{\alpha}}{\sum_k p_k^{\alpha}}
\]
其中：
\begin{itemize}
    \item $p_i$ 是转移 $i$ 的优先级
    \item $\alpha \in [0, 1]$ 控制优先程度的强度（$\alpha=0$ 时退化为均匀采样）
\end{itemize}
\end{definition}

\textbf{优先级计算方法}：
\begin{enumerate}
    \item \textbf{基于TD误差}：$p_i = |\delta_i| + \epsilon$，其中 $\delta_i$ 是转移 $i$ 的TD误差，$\epsilon$ 是小的正常数。
    \item \textbf{基于排名}：$p_i = \frac{1}{\text{rank}(i)}$，其中 $\text{rank}(i)$ 是基于 $|\delta_i|$ 的排名。
\end{enumerate}

\textbf{重要性采样权重}：
由于非均匀采样引入偏差，需要重要性采样权重校正：
\[
w_i = \left( \frac{1}{N} \cdot \frac{1}{P(i)} \right)^{\beta}
\]
其中 $\beta \in [0, 1]$ 控制校正程度，训练中从 $\beta_{\text{初始}}$ 线性增加到1。

\textbf{损失函数}：
\[
L(\theta) = \sum_{i=1}^{m} w_i \cdot (y_i - Q(s_i, a_i; \theta))^2
\]

\textbf{为什么重要性采样有效？}

考虑两种学习方式：
\begin{enumerate}
    \item \textbf{均匀采样}：学习率 $\alpha$，用样本 $(s_j, a_j, r_j, s_{j+1})$ 更新一次。
    \item \textbf{优先采样}：学习率 $\alpha_j = w_j \cdot \alpha$，用重要样本更新多次。
\end{enumerate}

虽然学习率减小了，但重要样本被更频繁地使用，相当于用更多计算量从重要样本中提取更多信息。

\subsection{Double DQN}

标准DQN存在过估计（overestimation）问题：$\max$ 操作倾向于选择被高估的动作。

\begin{theorem}[过估计定理]
设 $X_1, \ldots, X_n$ 是独立同分布的随机变量，均值为 $\mu$，方差为 $\sigma^2$。则：
\[
\mathbb{E}[\max(X_1, \ldots, X_n)] \geq \mu
\]
等号成立当且仅当 $n=1$ 或 $\sigma^2=0$。
\end{theorem}

\textbf{证明}：由Jensen不等式，$\max$ 是凸函数，所以 $\mathbb{E}[\max(X_i)] \geq \max(\mathbb{E}[X_i]) = \mu$。

在DQN中，Q值估计包含噪声（来自函数近似、环境随机性等），$\max$ 操作会放大正误差，导致过估计。

\begin{definition}[Double DQN]
Double DQN（DDQN）解耦动作选择和价值评估：
\[
y = r + \gamma Q\left(s', \arg\max_{a'} Q(s', a'; \theta); \theta^-\right)
\]
其中：
\begin{itemize}
    \item 使用在线网络 $\theta$ 选择动作：$a^* = \arg\max_{a'} Q(s', a'; \theta)$
    \item 使用目标网络 $\theta^-$ 评估价值：$Q(s', a^*; \theta^-)$
\end{itemize}
\end{definition}

\begin{table}[H]
\centering
\caption{不同Q-Learning变种的比较}
\begin{tabular}{p{0.25\textwidth}p{0.25\textwidth}p{0.2\textwidth}p{0.2\textwidth}}
\toprule
\textbf{算法} & \textbf{动作选择} & \textbf{价值评估} & \textbf{过估计程度} \\
\midrule
原始Q-Learning & DQN网络 & DQN网络 & 严重 \\
DQN+目标网络 & 目标网络 & 目标网络 & 中等 \\
Double DQN & DQN网络 & 目标网络 & 轻微 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Double DQN的优势}：
\begin{enumerate}
    \item 减少过估计，提高价值估计的准确性。
    \item 在某些任务上性能显著优于标准DQN。
    \item 实现简单，只需修改TD目标计算方式。
\end{enumerate}

\subsection{Dueling DQN}

Dueling DQN改进了网络结构，将Q值分解为状态价值 $V(s)$ 和优势函数 $A(s, a)$。

\begin{definition}[Dueling架构]
\[
Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a'; \theta, \alpha)
\]
其中：
\begin{itemize}
    \item $V(s; \theta, \beta)$：状态价值函数，评估状态 $s$ 的好坏
    \item $A(s, a; \theta, \alpha)$：优势函数，评估动作 $a$ 相对于平均水平的优势
    \item 减去的项确保优势函数的平均值为0，解决可识别性问题
\end{itemize}
\end{definition}

\textbf{可识别性问题}：如果直接定义 $Q(s, a) = V(s) + A(s, a)$，那么对于常数 $c$，有：
\[
Q(s, a) = [V(s) + c] + [A(s, a) - c]
\]
即 $V(s)$ 和 $A(s, a)$ 不唯一。通过减去优势函数的平均值解决这一问题。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{D:/code LateX/deep_learning/picture/2.png}
\caption{Dueling DQN网络结构：将Q值分解为状态价值和优势函数}
\end{figure}

\textbf{Dueling DQN的优势}：
\begin{enumerate}
    \item \textbf{更好的泛化}：可以学习哪些状态有价值，而不需要了解每个动作的影响。
    \item \textbf{更稳定的学习}：状态价值提供基线，减少方差。
    \item \textbf{更好的策略评估}：在状态价值相似但动作价值不同的情况下表现更好。
\end{enumerate}

\subsection{Multi-Step DQN}

标准DQN使用单步TD目标，只考虑一步奖励。Multi-Step DQN考虑多步奖励，减少短视行为。

\begin{definition}[n步TD目标]
n步TD目标考虑未来n步的奖励：
\[
y_t^{(n)} = \sum_{i=0}^{n-1} \gamma^i r_{t+i} + \gamma^n \max_{a'} Q(s_{t+n}, a'; \theta^-)
\]
特别地：
\begin{itemize}
    \item $n=1$：标准DQN，$y_t^{(1)} = r_t + \gamma \max_{a'} Q(s_{t+1}, a'; \theta^-)$
    \item $n=\infty$：蒙特卡洛方法，考虑整个回合的回报
\end{itemize}
\end{definition}

\textbf{多步学习的权衡}：
\begin{itemize}
    \item \textbf{偏差-方差权衡}：n越大，偏差越小（使用更多真实奖励），方差越大（依赖更多随机步骤）。
    \item \textbf{更新延迟}：需要等待n步后才能更新，学习延迟增加。
\end{itemize}

\textbf{实现方式}：
\begin{enumerate}
    \item 存储n步转移：$(s_t, a_t, r_t, \ldots, r_{t+n-1}, s_{t+n})$
    \item 计算n步TD目标
    \item 用标准DQN方式更新
\end{enumerate}

\subsection{Noisy DQN}

传统$\epsilon$-贪婪探索在动作空间添加噪声，Noisy DQN在参数空间添加噪声，实现更高效的探索。

\begin{definition}[Noisy DQN]
Noisy DQN在网络的权重中添加可学习的噪声：
\[
\tilde{Q}(s, a, \xi; \mu, \sigma) = Q(s, a; \mu + \sigma \odot \xi)
\]
其中：
\begin{itemize}
    \item $\mu$ 和 $\sigma$ 是可学习的参数
    \item $\xi$ 是随机噪声，从标准正态分布采样：$\xi \sim \mathcal{N}(0, 1)$
    \item $\odot$ 表示逐元素乘法
\end{itemize}
\end{definition}

\textbf{噪声注入方式}：
\begin{enumerate}
    \item \textbf{因子化高斯噪声}：减少参数数量，提高效率。
    \item \textbf{独立高斯噪声}：每个参数独立添加噪声。
\end{enumerate}

\textbf{训练过程}：
\begin{enumerate}
    \item 每个回合开始时采样噪声 $\xi$，并在整个回合中固定。
    \item 使用带噪声的网络选择动作：$a_t = \arg\max_{a} \tilde{Q}(s_t, a, \xi; \mu, \sigma)$
    \item 收集经验并计算损失。
    \item 通过损失函数同时优化 $\mu$ 和 $\sigma$。
\end{enumerate}

\begin{table}[H]
\centering
\caption{$\epsilon$-贪婪与Noisy DQN的比较}
\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
\toprule
\textbf{$\epsilon$-贪婪探索} & \textbf{Noisy DQN探索} \\
\midrule
\textbf{噪声位置}：动作空间 & \textbf{噪声位置}：参数空间 \\
\hline
\textbf{更新频率}：每个时间步独立决定 & \textbf{更新频率}：每个回合固定 \\
\hline
\textbf{探索连贯性}：低（独立随机） & \textbf{探索连贯性}：高（连贯的探索方向） \\
\hline
\textbf{探索与策略耦合}：解耦 & \textbf{探索与策略耦合}：强耦合（噪声是网络一部分） \\
\hline
\textbf{学习稳定性}：较低（频繁切换） & \textbf{学习稳定性}：较高（一致的方向） \\
\hline
\textbf{超参数}：需要手动调整 $\epsilon$ & \textbf{超参数}：噪声参数自动学习 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Distributional DQN}

传统DQN学习Q值的期望，Distributional DQN学习Q值的完整分布。

\begin{definition}[价值分布]
价值分布 $Z(s, a)$ 是一个随机变量，表示从状态 $s$ 执行动作 $a$ 后获得的随机回报。
传统Q值是价值分布的期望：
\[
Q(s, a) = \mathbb{E}[Z(s, a)]
\]
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{D:/code LateX/deep_learning/picture/RL.png}
\caption{Distributional DQN：学习价值分布而非期望值}
\end{figure}

\subsubsection{C51算法}

C51（Categorical 51）是Distributional DQN的一种实现，使用51个类别的分类分布表示价值分布。

\textbf{核心思想}：
\begin{enumerate}
    \item 将价值范围 $[V_{\min}, V_{\max}]$ 离散化为 $N=51$ 个等间距的支持点（atoms）：$z_i = V_{\min} + i \cdot \Delta z$，其中 $\Delta z = \frac{V_{\max} - V_{\min}}{N-1}$。
    \item 对于每个状态-动作对，网络输出这51个点的概率：$p_i(s, a) = \mathbb{P}(Z(s, a) = z_i)$。
    \item 价值分布的期望为：$Q(s, a) = \sum_{i=1}^{N} p_i(s, a) \cdot z_i$。
\end{enumerate}

\textbf{投影贝尔曼更新}：
对于转移 $(s, a, r, s')$，目标分布为：
\[
\mathcal{T} z_j = r + \gamma z_j
\]
将 $\mathcal{T} z_j$ 投影到原始支持点上，得到目标概率分布。

\textbf{损失函数}：使用交叉熵损失：
\[
L(\theta) = - \sum_{i=1}^{N} \hat{p}_i \log p_i(s, a; \theta)
\]
其中 $\hat{p}_i$ 是目标分布在第 $i$ 个atom上的概率。

\textbf{C51的优势}：
\begin{enumerate}
    \item \textbf{更丰富的学习信号}：分布提供比标量更多的信息。
    \item \textbf{更好的风险意识}：可以区分高方差和低方差情况。
    \item \textbf{更稳定的训练}：分布学习通常更稳定。
\end{enumerate}

\subsubsection{QR-DQN和IQN}

除了C51，还有其他分布RL算法：
\begin{itemize}
    \item \textbf{QR-DQN（Quantile Regression DQN）}：学习价值分布的分位数。
    \item \textbf{IQN（Implicit Quantile Networks）}：隐式学习分位数，更灵活。
\end{itemize}

\section{Rainbow：集成多种改进}

Rainbow算法集成了DQN的六种主要改进：
\begin{enumerate}
    \item 经验回放
    \item 目标网络
    \item Double DQN
    \item Dueling网络
    \item Multi-Step学习
    \item Noisy网络
    \item Distributional RL
\end{enumerate}

\textbf{Rainbow的关键设计}：
\begin{enumerate}
    \item \textbf{多步损失}：使用n步TD目标。
    \item \textbf{优先经验回放}：使用分布TD误差作为优先级。
    \item \textbf{Noisy网络}：用于探索。
    \item \textbf{Distributional RL}：学习价值分布。
    \item \textbf{Dueling架构}：分解状态价值和优势。
    \item \textbf{Double DQN}：解耦动作选择和评估。
\end{enumerate}

\textbf{消融实验结果}：
\begin{itemize}
    \item \textbf{最大贡献}：多步学习和优先经验回放。
    \item \textbf{中等贡献}：Distributional RL和Noisy网络。
    \item \textbf{较小贡献}：Dueling网络和Double DQN。
    \item \textbf{组合效应}：所有改进组合的效果远超单个改进。
\end{itemize}

\section{总结与比较}

\subsection{DQN变种对比}

\begin{table}[H]
\centering
\caption{DQN主要变种的比较}
\begin{tabular}{p{0.2\textwidth}p{0.25\textwidth}p{0.25\textwidth}p{0.2\textwidth}}
\toprule
\textbf{算法} & \textbf{核心创新} & \textbf{解决的问题} & \textbf{实现复杂度} \\
\midrule
\textbf{DQN} & 神经网络近似Q函数 & 维度灾难，连续状态空间 & 中等 \\
\textbf{优先经验回放} & 按TD误差优先级采样 & 样本效率低 & 低 \\
\textbf{Double DQN} & 解耦动作选择与评估 & Q值过估计 & 低 \\
\textbf{Dueling DQN} & Q=V+A架构 & 状态价值与动作优势分离 & 低 \\
\textbf{Multi-Step DQN} & n步TD目标 & 短视偏差 & 低 \\
\textbf{Noisy DQN} & 参数空间噪声探索 & 探索效率低 & 中等 \\
\textbf{Distributional DQN} & 学习价值分布 & 风险意识，丰富信号 & 高 \\
\textbf{Rainbow} & 集成所有改进 & 单一改进的局限性 & 高 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{实际应用建议}

\begin{enumerate}
    \item \textbf{从标准DQN开始}：实现基础DQN，包含经验回放和目标网络。
    \item \textbf{逐步添加改进}：按优先级添加：Double DQN → 优先经验回放 → Multi-Step → 其他。
    \item \textbf{超参数调优}：
    \begin{itemize}
        \item 学习率：通常 $10^{-4}$ 到 $10^{-3}$
        \item 折扣因子 $\gamma$：0.99（长期任务）或 0.95（短期任务）
        \item 回放缓冲区大小：$10^5$ 到 $10^6$
        \item 批量大小：32 到 256
    \end{itemize}
    \item \textbf{监控训练}：
    \begin{itemize}
        \item 观察回报曲线
        \item 监控Q值范围（避免发散）
        \item 检查探索率衰减
    \end{itemize}
\end{enumerate}

\subsection{未来方向}

\begin{enumerate}
    \item \textbf{分布式强化学习}：多个智能体并行收集经验，加速训练。
    \item \textbf{元强化学习}：学习快速适应新任务的能力。
    \item \textbf{基于模型的DQN}：结合模型预测与价值学习。
    \item \textbf{多任务学习}：一个智能体学习多个相关任务。
    \item \textbf{安全约束}：在价值学习中加入安全约束。
\end{enumerate}

价值学习作为强化学习的核心方法，从经典的Q-Learning到现代的Rainbow，经历了显著的发展。理解这些算法的原理、优势和局限性，对于在实际问题中选择合适的算法至关重要。
