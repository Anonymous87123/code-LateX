\chapter{LLMs 训练经验帖}

\section{分布式训练框架选择}
多用DeepSpeed，少用Pytorch原生的torchrun。在节点数量较少的情况下，使用何种训练框架并不是特别重要；然而，一旦涉及到数百个节点，DeepSpeed显现出其强大之处，其简便的启动和便于性能分析的特点使其成为理想之选。

\section{LLMs 训练实用建议}

\subsection{弹性容错和自动重启机制}
大模型训练不是以往那种单机训个几小时就结束的任务，往往需要训练好几周甚至好几个月，这时候你就知道能稳定训练有多么重要。弹性容错能让你在机器故障的情况下依然继续重启训练；自动重启能让你在训练中断之后立刻重启训练。毕竟，大模型时代，节约时间就是节约钱。

\subsection{定期保存模型}
训练的时候每隔一段时间做个checkpointing，这样如果训练中断还能从上次的断点来恢复训练。

\subsection{规划训练目标}
训练一次大模型的成本很高的。在训练之前先想清楚这次训练的目的，记录训练参数和中间过程结果，少做重复劳动。

\subsection{关注GPU使用效率}
有时候，即使增加了多块A100 GPU，大型模型的训练速度未必会加快，这很可能是因为GPU使用效率不高，尤其在多机训练情况下更为明显。仅仅依赖nvidia-smi显示的GPU利用率并不足以准确反映实际情况，因为即使显示为100\%，实际GPU利用率也可能不是真正的100\%。要更准确地评估GPU利用率，需要关注TFLOPS和吞吐率等指标，这些监控在DeepSpeed框架中都得以整合。

\subsection{训练框架选择影响}
对于同一模型，选择不同的训练框架，对于资源的消耗情况可能存在显著差异（比如使用Huggingface Transformers和DeepSpeed训练OPT-30相对于使用Alpa对于资源的消耗会低不少）。

\subsection{环境配置注意事项}
针对已有的环境进行分布式训练环境搭建时，一定要注意之前环境的python、pip、virtualenv、setuptools的版本。不然创建的虚拟环境即使指定对了Python版本，也可能会遇到很多安装依赖库的问题（GPU服务器能够访问外网的情况下，建议使用Docker相对来说更方便）。

\subsection{系统底层库升级谨慎性}
遇到需要升级GLIBC等底层库需要升级的提示时，一定要慎重，不要轻易升级，否则，可能会造成系统宕机或很多命令无法操作等情况。

\section{模型规模选择策略}
进行大模型模型训练时，先使用小规模模型（如：OPT-125m/2.7b）进行尝试，然后再进行大规模模型（如：OPT-13b/30b...）的尝试，便于出现问题时进行排查。目前来看，业界也是基于相对较小规模参数的模型（6B/7B/13B）进行的优化，同时，13B模型经过指令精调之后的模型效果已经能够到达GPT4的90\%的效果。

\section{加速卡选择建议}
对于一些国产AI加速卡，目前来说，坑还比较多，如果时间不是时间非常充裕，还是尽量选择Nvidia的AI加速卡。


\chapter{大模型(LLMs) LangChain面}

\section{LangChain 基础概念}

\subsection{什么是LangChain？}
LangChain是一个强大的框架，旨在帮助开发人员使用语言模型构建端到端的应用程序。它提供了一套工具、组件和接口，可简化创建由大型语言模型(LLM)和聊天模型提供支持的应用程序的过程。LangChain可以轻松管理与语言模型的交互，将多个组件链接在一起，并集成额外的资源，例如API和数据库。

\subsection{LangChain Agent}
LangChain Agent是框架中驱动决策制定的实体。它可以访问一组工具，并可以根据用户的输入决定调用哪个工具。

优点：LangChain Agent帮助构建复杂的应用程序，这些应用程序需要自适应和特定于上下文的响应。当存在取决于用户输入和其他因素的未知交互链时，它们特别有用。

\section{LangChain 核心概念}

\subsection{Components and Chains}
\begin{itemize}
\item \textbf{Component}：模块化的构建块，可以组合起来创建强大的应用程序
\item \textbf{Chain}：组合在一起以完成特定任务的一系列Components(或其他Chain)
\end{itemize}

注：一个Chain可能包括一个Prompt模板、一个语言模型和一个输出解析器，它们一起工作以处理用户输入、生成响应并处理输出。

\subsection{Prompt Templates and Values}
\begin{itemize}
\item \textbf{Prompt Template作用}：负责创建PromptValue，这是最终传递给语言模型的内容
\item \textbf{Prompt Template特点}：有助于将用户输入和其他动态信息转换为适合语言模型的格式
\end{itemize}

\subsection{Example Selectors}
作用：当您想要在Prompts中动态包含示例时，Example Selectors很有用。他们接受用户输入并返回一个示例列表以在提示中使用，使其更强大和特定于上下文。

\subsection{Output Parsers}
\begin{itemize}
\item \textbf{作用}：负责将语言模型响应构建为更有用的格式
\item \textbf{实现方法}：
    \begin{itemize}
    \item 一种用于提供格式化指令
    \item 另一种用于将语言模型的响应解析为结构化格式
    \end{itemize}
\item \textbf{特点}：使得在您的应用程序中处理输出数据变得更加容易
\end{itemize}

\subsection{Indexes and Retrievers}
\begin{itemize}
\item \textbf{Index}：一种组织文档的方式，使语言模型更容易与它们交互
\item \textbf{Retrievers}：用于获取相关文档并将它们与语言模型组合的接口
\end{itemize}

注：LangChain提供了用于处理不同类型的索引和检索器的工具和功能，例如矢量数据库和文本拆分器。

\subsection{Chat Message History}
\begin{itemize}
\item \textbf{作用}：负责记住所有以前的聊天交互数据，然后可以将这些交互数据传递回模型、汇总或以其他方式组合
\item \textbf{优点}：有助于维护上下文并提高模型对对话的理解
\end{itemize}

\subsection{Agents and Toolkits}
\begin{itemize}
\item \textbf{Agent}：在LangChain中推动决策制定的实体。他们可以访问一套工具，并可以根据用户输入决定调用哪个工具
\item \textbf{Toolkits}：一组工具，当它们一起使用时，可以完成特定的任务
\end{itemize}

\section{LangChain 功能特性}

\subsection{主要功能}
\begin{itemize}
\item \textbf{针对特定文档的问答}：根据给定的文档回答问题，使用这些文档中的信息来创建答案
\item \textbf{聊天机器人}：构建可以利用LLM的功能生成文本的聊天机器人
\item \textbf{Agents}：开发可以决定行动、采取这些行动、观察结果并继续执行直到完成的代理
\end{itemize}

\subsection{LangChain 模型类型}
LangChain model是一种抽象，表示框架中使用的不同类型的模型：
\begin{itemize}
\item \textbf{LLM(大型语言模型)}：将文本字符串作为输入并返回文本字符串作为输出
\item \textbf{聊天模型(Chat Model)}：由语言模型支持，但具有更结构化的API。将聊天消息列表作为输入并返回聊天消息
\item \textbf{文本嵌入模型(Text Embedding Models)}：将文本作为输入并返回表示文本嵌入的浮点列表
\end{itemize}

\subsection{LangChain 特点}
LangChain旨在为六个主要领域的开发人员提供支持：
\begin{enumerate}
\item LLM和提示：管理提示、优化，创建通用界面
\item 链(Chain)：对LLM或其他实用程序的调用序列
\item 数据增强生成：与外部数据源交互以收集生成步骤的数据
\item Agents：让LLM做出有关行动的决定
\item 内存：维护链或代理调用之间的状态
\item 评估：使用LLM评估模型
\end{enumerate}

\section{LangChain 使用示例}

\subsection{调用LLMs生成回复}
\begin{lstlisting}[language=Python]
# 官方示例使用OPENAI接口
from langchain.llms import OpenAI
llm = OpenAI(model_name="text-davinci-003")
prompt = "你好"
response = llm(prompt)

# 用chatglm来演示该过程，封装一下即可
from transformers import AutoTokenizer, AutoModel
class chatGLM():
    def __init__(self, model_name) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True).half().cuda().eval()
    def __call__(self, prompt) -> Any:
        response, _ = self.model.chat(self.tokenizer, prompt)
        return response

llm = chatGLM(model_name="THUDM/chatglm-6b")
prompt = "你好"
response = llm(prompt)
print("response: %s" % response)
\end{lstlisting}

\subsection{修改提示模板}
\begin{lstlisting}[language=Python]
from langchain import PromptTemplate

template = """
Explain the concept of {concept} in couple of lines
"""
prompt = PromptTemplate(input_variables=["concept"], template=template)
prompt = prompt.format(concept="regularization")
print("prompt=%s" % prompt)

template = "请给我解释一下{concept}的意思"
prompt = PromptTemplate(input_variables=["concept"], template=template)
prompt = prompt.format(concept="人工智能")
print("prompt=%s" % prompt)
\end{lstlisting}

\subsection{链接多个组件处理任务}
\begin{lstlisting}[language=Python]
# chains --------
from langchain.chains import LLMChain
chain = LLMChain(llm=openAI(), prompt=promptTem)
print(chain.run("你好"))

# Chatglm对象不符合LLMChain类llm对象要求，模仿一下
class DemoChain():
    def __init__(self, llm, prompt) -> None:
        self.llm = llm
        self.prompt = prompt
    def run(self, query) -> Any:
        prompt = self.prompt.format(concept=query)
        print("query=%s->prompt=%s" % (query, prompt))
        response = self.llm(prompt)
        return response

chain = DemoChain(llm=llm, prompt=promptTem)
print(chain.run(query="天道酬勤"))
\end{lstlisting}

\subsection{Embedding \& Vector Store}
\begin{lstlisting}[language=Python]
# 官方示例代码，用的OpenAI的ada的文本Embedding模型
# 1) Embeding model
from langchain.embeddings import OpenAIEmbeddings
embeddings = OpenAIEmbeddings(model_name="ada")
query_result = embeddings.embed_query("你好")

# 2) 文本切割
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100, chunk_overlap=0
)
texts = """天道酬勤"并不是鼓励人们不劳而获，而是提醒人们要遵循自然规律..."""
texts = text_splitter.create_documents([texts])

# 3) 入库检索
import pinecone
from langchain.vectorstores import Pinecone
pinecone.init(api_key=os.getenv(""), environment=os.getenv(""))
index_name = "demo"
search = Pinecone.from_documents(texts=texts, embeddings, index_name=index_name)
query = "What is magical about an autoencoder?"
result = search.similarity_search(query)
\end{lstlisting}

\section{LangChain 问题与解决方案}

\subsection{低效的令牌使用问题}
\begin{itemize}
\item \textbf{问题}：Langchain的令牌计数功能对于小数据集来说效率很低
\item \textbf{解决方案}：Tiktoken是OpenAI开发的Python库，用于更有效地解决令牌计数问题
\end{itemize}

\subsection{文档问题}
\begin{itemize}
\item \textbf{问题}：文档不充分且经常不准确，经常有404错误页面
\item \textbf{原因}：与Langchain快速发展、版本迭代快速有关
\end{itemize}

\subsection{概念混淆问题}
\begin{itemize}
\item \textbf{问题}：代码库概念让人混淆，存在大量的"helper"函数
\item \textbf{示例}：简单的分割函数被复杂包装
\end{itemize}

\subsection{行为不一致问题}
\begin{itemize}
\item \textbf{问题}：隐藏重要细节和行为不一致，可能导致生产系统出现意想不到的问题
\item \textbf{示例}：ConversationRetrievalChain的输入问题重新措辞可能破坏对话自然流畅性
\end{itemize}

\subsection{缺乏标准数据类型问题}
\begin{itemize}
\item \textbf{问题}：缺乏表示数据的标准方法，阻碍与其他框架和工具的集成
\end{itemize}

\section{LangChain 替代方案}

\subsection{LlamaIndex}
LlamaIndex是一个数据框架，可以很容易地将大型语言模型连接到自定义数据源。它可用于存储、查询和索引数据，还提供了各种数据可视化和分析工具。

\subsection{Deepset Haystack}
Deepset Haystack是另外一个开源框架，用于使用大型语言模型构建搜索和问答应用程序。它基于Hugging Face Transformers，提供了多种查询和理解文本数据的工具。

