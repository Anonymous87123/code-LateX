

\chapter{LLM文档对话PDF解析关键问题}

\section{PDF解析的必要性}

\subsection{为什么需要进行PDF解析？}
最近在探索ChatPDF和ChatDoc等方案的思路，也就是用LLM实现文档助手。在此记录一些难题和解决方案，首先讲解主要思想，其次以问题+回答的形式展开。

\subsection{PDF解析的重要性}
当利用LLMs实现用户与文档对话时，首要工作就是对文档中内容进行解析。

由于PDF是最通用，也是最复杂的文档形式，所以对PDF进行解析变成利用LLM实现用户与文档对话的重中之重工作。

如何精确地回答用户关于文档的问题，不重也不漏？笔者认为非常重要的一点是文档内容解析。很好地组织起来，LLM只能瞎编。

\section{PDF解析方法与区别}

\subsection{PDF解析的两条技术路线}
PDF的解析大体上有两条路，一条是基于规则，一条是基于AI。

\textbf{方法一：基于规则：}
\begin{itemize}
\item \textbf{介绍：}根据文档的组织特点去"算"每部分的样式和内容
\item \textbf{存在问题：}不通用，因为PDF的类型、排版实在太多了，没办法穷举
\end{itemize}

\textbf{方法二：基于AI：}
\begin{itemize}
\item \textbf{介绍：}该方法为目标检测和OCR文字识别pipeline方法
\end{itemize}

\section{PDF解析存在的问题}

PDF转text这块存在一定的偏差，尤其是paper中包含了大量的figure和table，以及一些特殊的字符，直接调用langchain官方给的PDF解析工具，有一些信息甚至是错误的。

这里，一方面可以用arxiv的tex源码直接抽取内容，另一方面，可以尝试用各种OCR工具来提升表现。

\section{长文档关键信息提取方法}

对于长文档（书籍），如何获取其中关键信息，并构建索引：

\textbf{方法一：分块索引法}
\begin{itemize}
\item \textbf{介绍：}直接对长文档（书籍）进行分块，然后构建索引入库。后期问答，只需要从库中召回和用户query相关的内容块进行拼接成文章，输入到LLMs生成回复
\item \textbf{存在问题：}
\begin{enumerate}
\item 将文章分块，会破坏文章语义信息
\item 对于长文章，会被分割成很多块，并构建很多索引，这严重影响知识库存储空间
\item 如果内容都不能很好地组织起来，LLM只能瞎编
\end{enumerate}
\end{itemize}

\textbf{方法二：文本摘要法}
\begin{itemize}
\item \textbf{介绍：}直接利用文本摘要模型对每一篇长文档（书籍）做文本摘要，然后对文本摘要内容构建索引入库。后期问答，只需要从库中召回和用户query相关的摘要内容，输入到LLMs生成回复
\item \textbf{存在问题：}
\begin{enumerate}
\item 由于每篇长文档（书籍）内容比较多，直接利用文本摘要模型对其做文本摘要，需要比较大算力成本和时间成本
\item 生成的文本摘要存在部分内容丢失问题，不能很好的概括整篇文章
\end{enumerate}
\end{itemize}

\textbf{方法三：多级标题构建文本摘要法：}
\begin{itemize}
\item \textbf{介绍：}把多级标题提取出来，然后适当做语义扩充，或者去向量库检索相关片段，最后用LLM整合即可
\end{itemize}

\section{标题提取的重要性与方法}

\subsection{为什么要提取标题甚至是多级标题？}
没有处理过LLM文档对话的朋友可能不明白为什么要提取标题甚至是多级标题，因此我先来阐述提取标题对于LLM阅读理解的重要性有多大。

\begin{enumerate}
\item 如Q1阐述的那样，标题是快速做摘要最核心的文本
\item 对于有些问题high-level的问题，没有标题很难得到用户满意的结果
\end{enumerate}

举例：假如用户就想知道3.2节是从哪些方面讨论的（标准答案就是3个方面），如果我们没有将标题信息告诉LLM，而是把所有信息全部扔给LLM，那它大概率不会知道是3个方面（要么会少，要么会多。做过的朋友秒懂）

\subsection{如何提取文章标题？}
\textbf{第一步：PDF转图片。}用一些工具将PDF转换为图片，这里有很多开源工具可以选，笔者采用fitz，一个python库。速度很快，时间在毫秒之间。

\textbf{第二步：图片中元素识别。}采用目标检测模型识别元素（标题、文本、表格、图片、列表等元素）。

\textbf{工具介绍：}
\begin{itemize}
\item \textbf{Layout-parser：}
\begin{itemize}
\item 优点：最大的模型（约800MB）精度非常高
\item 缺点：速度慢一点
\end{itemize}

\item \textbf{PaddlePaddle-ppstructure：}
\begin{itemize}
\item 优点：模型比较小，效果也还行
\end{itemize}

\item \textbf{unstructured：}
\begin{itemize}
\item 缺点：fast模式效果很差，基本不能用，会将很多公式也识别为标题。其他模式或许可行，笔者没有尝试
\end{itemize}
\end{itemize}

利用上述工具，可以得到一个list，存储所有检测出来的标题。

\textbf{第三步：标题级别判断。}利用标题区块的高度（也就是字号）来判断哪些是一级标题，哪些是二级、三级、......N级标题。这个时候我们发现一些目标检测模型提取的区块并不是严格按照文字的边去切，导致这个idea不能实施，那怎么办呢？unstructured的fast模式就是按照文字的边去切的，同一级标题的区块高度误差在0.001之间。因此我们只需要用unstructured拿到标题的高度值即可（虽然繁琐，但是不耗时，unstructured处理也在毫秒之间）。

\section{单双栏PDF的处理}

\subsection{区分单双栏PDF与重新排序}
\textbf{动机：}很多目标检测模型识别区块之后并不是顺序返回的，因此我们需要根据坐标重新组织顺序。单栏的很好办，直接按照中心点纵坐标排序即可。双栏PDF就很棘手了，有的朋友可能不知道PDF还有双栏形式。

\textbf{问题一：首先如何区分单双栏论文？}
\begin{itemize}
\item \textbf{方法：}得到所有区块的中心点的横坐标，用这一组横坐标的极差来判断即可，双栏论文的极差远远大于单栏论文，因此可以设定一个极差阈值
\end{itemize}

\textbf{问题二：双栏论文如何确定区块的先后顺序？}
\begin{itemize}
\item \textbf{方法：}先找到中线，将左右栏的区块分开，中线横坐标可以借助上述求极差的两个横坐标x1和x2来求，也就是(x1+x2)/2。分为左右栏区块后，对于每一栏区块按照纵坐标排序即可，最后将右栏拼接到左栏后边
\end{itemize}

\section{表格和图片数据提取}

\subsection{表格和图片数据提取思路}
思路仍然是目标检测和OCR。无论是layoutparser还是PaddleOCR都有识别表格和图片的目标检测模型，而表格的数据可以直接OCR导出为excel形式数据，非常方便。

提取出表格之后喂给LLM，LLM还是可以看懂的，可以设计prompt做一些指导。关于这一块两部分demo代码都很清楚明白，这里不再赘述。

\section{基于AI的文档解析优缺点}

\subsection{基于AI的文档解析优缺点分析}
\begin{itemize}
\item \textbf{优点：}准确率高，通用性强
\item \textbf{缺点：}耗时慢，建议用GPU等加速设备，多进程、多线程去处理。耗时只在目标检测和OCR两个阶段，其他步骤均不耗时
\end{itemize}

\section{总结与建议}

\subsection{技术建议}
笔者建议按照不同类型的PDF做特定处理，例如论文、图书、财务报表、PPT都可以根据特点做一些小的专有设计。

没有GPU的话目标检测模型建议用PaddlePaddle提供的，速度很快。Layout parser只是一个框架，目标检测模型和OCR工具可以自有切换。

\subsection{实践要点总结}
\begin{itemize}
\item \textbf{预处理优化：}根据文档类型选择合适的解析策略
\item \textbf{标题提取：}多级标题提取对于LLM理解文档结构至关重要
\item \textbf{布局处理：}单双栏识别和重新排序是保证内容连贯性的关键
\item \textbf{表格处理：}结合目标检测和OCR技术提取结构化数据
\item \textbf{性能平衡：}在准确性和处理速度之间找到合适的平衡点
\end{itemize}

\subsection{未来发展方向}
\begin{itemize}
\item 更智能的文档结构理解算法
\item 多模态信息的融合处理
\item 实时处理性能的优化
\item 领域自适应能力的提升
\end{itemize}

