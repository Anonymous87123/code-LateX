\contentsline {chapter}{\numberline {第一章\hspace {.3em}}基础知识}{11}{chapter.1}%
\contentsline {section}{\numberline {1.1}大模型基础概念}{11}{section.1.1}%
\contentsline {section}{\numberline {1.2}单双向注意力}{11}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}核心概念：“阅读”和“写作”}{11}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}技术深度解析}{11}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}单双向注意力对比总结}{12}{subsection.1.2.3}%
\contentsline {section}{\numberline {1.3}主流开源模型体系}{12}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}三种主流体系}{12}{subsection.1.3.1}%
\contentsline {section}{\numberline {1.4}三种Decoder架构区别}{13}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}核心区别}{13}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Encoder-Decoder架构}{13}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Causal Decoder架构}{13}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Prefix Decoder架构}{13}{subsection.1.4.4}%
\contentsline {section}{\numberline {1.5}大模型训练目标}{13}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}语言模型}{13}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}去噪自编码器}{13}{subsection.1.5.2}%
\contentsline {section}{\numberline {1.6}涌现能力分析}{14}{section.1.6}%
\contentsline {section}{\numberline {1.7}Decoder Only架构优势}{14}{section.1.7}%
\contentsline {section}{\numberline {1.8}大模型优缺点分析}{14}{section.1.8}%
\contentsline {subsection}{\numberline {1.8.1}优点}{14}{subsection.1.8.1}%
\contentsline {subsection}{\numberline {1.8.2}缺点}{15}{subsection.1.8.2}%
\contentsline {chapter}{\numberline {第二章\hspace {.3em}}Layer Normalization 篇}{16}{chapter.2}%
\contentsline {section}{\numberline {2.1}Layer Norm 基础}{16}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Layer Norm 计算公式}{16}{subsection.2.1.1}%
\contentsline {section}{\numberline {2.2}RMS Norm（均方根 Norm）}{16}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}RMS Norm 计算公式}{16}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}RMS Norm 的特点}{16}{subsection.2.2.2}%
\contentsline {section}{\numberline {2.3}Deep Norm}{17}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Deep Norm 思路}{17}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Deep Norm 代码实现}{17}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Deep Norm 的优点}{17}{subsection.2.3.3}%
\contentsline {section}{\numberline {2.4}Layer Normalization 的位置设计}{17}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}LN 在 LLMs 中的不同位置}{17}{subsection.2.4.1}%
\contentsline {subsubsection}{Post LN}{17}{subsubsection*.3}%
\contentsline {subsubsection}{Pre-LN}{17}{subsubsection*.4}%
\contentsline {subsubsection}{Sandwich-LN}{18}{subsubsection*.5}%
\contentsline {section}{\numberline {2.5}Layer Normalization 对比分析}{18}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}各模型使用的 Normalization 方法}{18}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}特殊说明}{18}{subsection.2.5.2}%
\contentsline {chapter}{\numberline {第三章\hspace {.3em}}LLMs 激活函数篇}{19}{chapter.3}%
\contentsline {section}{\numberline {3.1}FFN 块基础}{19}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}FFN 块计算公式}{19}{subsection.3.1.1}%
\contentsline {section}{\numberline {3.2}常见激活函数}{19}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}GeLU 激活函数}{19}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Swish 激活函数}{19}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}GLU 线性门控单元}{19}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}基础 GLU 计算公式}{19}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}GeGLU 计算公式}{20}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}SwiGLU 计算公式}{20}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}参数规模说明}{20}{subsection.3.3.4}%
\contentsline {section}{\numberline {3.4}各 LLMs 激活函数使用情况}{20}{section.3.4}%
\contentsline {section}{\numberline {3.5}模型参数结构示例}{21}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}LLaMA 模型参数结构}{21}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Bloom 模型参数结构}{21}{subsection.3.5.2}%
\contentsline {subsection}{\numberline {3.5.3}特殊说明}{21}{subsection.3.5.3}%
\contentsline {chapter}{\numberline {第四章\hspace {.3em}}Attention 升级面}{23}{chapter.4}%
\contentsline {section}{\numberline {4.1}传统 Attention 的问题}{23}{section.4.1}%
\contentsline {section}{\numberline {4.2}Attention 优化方向}{23}{section.4.2}%
\contentsline {section}{\numberline {4.3}Attention 变体概述}{23}{section.4.3}%
\contentsline {section}{\numberline {4.4}Multi-Query Attention}{23}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Multi-head Attention 存在的问题}{23}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Multi-Query Attention 介绍}{23}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Multi-head Attention 与 Multi-Query Attention 对比}{24}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}各模型参数配置对比}{24}{subsection.4.4.4}%
\contentsline {subsection}{\numberline {4.4.5}Multi-Query Attention 的模型实现差异}{24}{subsection.4.4.5}%
\contentsline {subsection}{\numberline {4.4.6}Multi-Query Attention 的优势}{24}{subsection.4.4.6}%
\contentsline {subsection}{\numberline {4.4.7}使用 Multi-Query Attention 的模型}{24}{subsection.4.4.7}%
\contentsline {section}{\numberline {4.5}Grouped-query Attention}{24}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Grouped-query Attention 定义}{24}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}使用 Grouped-query Attention 的模型}{25}{subsection.4.5.2}%
\contentsline {section}{\numberline {4.6}FlashAttention}{25}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}FlashAttention 核心技术}{25}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2}FlashAttention 优点}{25}{subsection.4.6.2}%
\contentsline {subsection}{\numberline {4.6.3}使用 FlashAttention 的模型}{25}{subsection.4.6.3}%
\contentsline {section}{\numberline {4.7}并行 Transformer Block}{25}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}并行 Transformer Block 原理}{25}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}并行 Transformer Block 效果}{25}{subsection.4.7.2}%
\contentsline {subsection}{\numberline {4.7.3}使用并行 Transformer Block 的模型}{25}{subsection.4.7.3}%
\contentsline {chapter}{\numberline {第五章\hspace {.3em}}Transformers 操作篇}{26}{chapter.5}%
\contentsline {section}{\numberline {5.1}Transformers 库基础操作}{26}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}如何利用 transformers 加载 Bert 模型？}{26}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}如何利用 transformers 输出 Bert 指定 hidden\_state？}{27}{subsection.5.1.2}%
\contentsline {section}{\numberline {5.2}BERT 输出向量获取}{27}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}BERT 模型输出结构}{27}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}获取每一层网络的向量输出}{28}{subsection.5.2.2}%
\contentsline {chapter}{\numberline {第六章\hspace {.3em}}LLMs 损失函数篇}{29}{chapter.6}%
\contentsline {section}{\numberline {6.1}KL 散度}{29}{section.6.1}%
\contentsline {section}{\numberline {6.2}交叉熵损失函数}{29}{section.6.2}%
\contentsline {section}{\numberline {6.3}KL 散度与交叉熵的区别}{29}{section.6.3}%
\contentsline {section}{\numberline {6.4}多任务学习各loss差异过大处理}{30}{section.6.4}%
\contentsline {section}{\numberline {6.5}分类问题为什么用交叉熵损失函数不用均方误差(MSE)?}{30}{section.6.5}%
\contentsline {section}{\numberline {6.6}信息增益}{30}{section.6.6}%
\contentsline {section}{\numberline {6.7}多分类的分类损失函数(Softmax)}{30}{section.6.7}%
\contentsline {section}{\numberline {6.8}Softmax和交叉熵损失计算}{31}{section.6.8}%
\contentsline {section}{\numberline {6.9}Softmax数值稳定性问题}{31}{section.6.9}%
\contentsline {chapter}{\numberline {第七章\hspace {.3em}}相似度函数篇}{32}{chapter.7}%
\contentsline {section}{\numberline {7.1}相似度计算方法}{32}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}除了余弦相似度还有哪些方法}{32}{subsection.7.1.1}%
\contentsline {section}{\numberline {7.2}对比学习}{32}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}对比学习概述}{32}{subsection.7.2.1}%
\contentsline {section}{\numberline {7.3}对比学习中的负样本问题}{32}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}负样本的重要性}{32}{subsection.7.3.1}%
\contentsline {subsection}{\numberline {7.3.2}负样本构造成本过高的解决方案}{32}{subsection.7.3.2}%
\contentsline {chapter}{\numberline {第八章\hspace {.3em}}大模型(LLMs)进阶面}{34}{chapter.8}%
\contentsline {section}{\numberline {8.1}生成式大模型概述}{34}{section.8.1}%
\contentsline {subsection}{\numberline {8.1.1}什么是生成式大模型？}{34}{subsection.8.1.1}%
\contentsline {section}{\numberline {8.2}文本生成多样性机制}{34}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}大模型如何让生成的文本丰富而不单调？}{34}{subsection.8.2.1}%
\contentsline {subsubsection}{从训练角度来看}{34}{subsubsection*.11}%
\contentsline {subsubsection}{从推理角度来看}{34}{subsubsection*.12}%
\contentsline {section}{\numberline {8.3}LLMs复读机问题}{34}{section.8.3}%
\contentsline {subsection}{\numberline {8.3.1}什么是LLMs复读机问题？}{34}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}为什么会出现LLMs复读机问题？}{35}{subsection.8.3.2}%
\contentsline {subsection}{\numberline {8.3.3}如何缓解LLMs复读机问题？}{35}{subsection.8.3.3}%
\contentsline {subsubsection}{Unlikelihood Training}{35}{subsubsection*.13}%
\contentsline {subsubsection}{引入噪声}{35}{subsubsection*.14}%
\contentsline {subsubsection}{Repetition Penalty}{35}{subsubsection*.15}%
\contentsline {subsubsection}{Contrastive Search}{35}{subsubsection*.16}%
\contentsline {subsubsection}{Beam Search}{36}{subsubsection*.17}%
\contentsline {subsubsection}{TopK sampling}{36}{subsubsection*.18}%
\contentsline {subsubsection}{Nucleus sampler}{36}{subsubsection*.19}%
\contentsline {subsubsection}{Temperature}{36}{subsubsection*.20}%
\contentsline {subsubsection}{No repeat ngram size}{36}{subsubsection*.21}%
\contentsline {subsubsection}{重复率指标检测}{36}{subsubsection*.22}%
\contentsline {subsubsection}{后处理和过滤}{36}{subsubsection*.23}%
\contentsline {subsubsection}{人工干预和控制}{36}{subsubsection*.24}%
\contentsline {section}{\numberline {8.4}LLaMA系列问题}{37}{section.8.4}%
\contentsline {subsection}{\numberline {8.4.1}LLaMA输入句子长度理论上可以无限长吗？}{37}{subsection.8.4.1}%
\contentsline {section}{\numberline {8.5}模型选择指南}{37}{section.8.5}%
\contentsline {subsection}{\numberline {8.5.1}什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型？}{37}{subsection.8.5.1}%
\contentsline {section}{\numberline {8.6}专业领域大模型需求}{37}{section.8.6}%
\contentsline {subsection}{\numberline {8.6.1}各个专业领域是否需要各自的大模型来服务？}{37}{subsection.8.6.1}%
\contentsline {section}{\numberline {8.7}长文本处理技术}{37}{section.8.7}%
\contentsline {subsection}{\numberline {8.7.1}如何让大模型处理更长的文本？}{37}{subsection.8.7.1}%
\contentsline {subsubsection}{LongChat方法}{37}{subsubsection*.25}%
\contentsline {subsubsection}{其他技术方向}{37}{subsubsection*.26}%
\contentsline {chapter}{\numberline {第九章\hspace {.3em}}大模型(LLMs)微调面}{39}{chapter.9}%
\contentsline {section}{\numberline {9.1}微调基础问题}{39}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}全参数微调显存需求}{39}{subsection.9.1.1}%
\contentsline {subsection}{\numberline {9.1.2}SFT后模型性能下降原因}{39}{subsection.9.1.2}%
\contentsline {section}{\numberline {9.2}数据构建与处理}{39}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}SFT指令微调数据构建原则}{39}{subsection.9.2.1}%
\contentsline {subsection}{\numberline {9.2.2}领域模型Continue PreTrain数据选取}{40}{subsection.9.2.2}%
\contentsline {subsection}{\numberline {9.2.3}缓解模型遗忘通用能力}{40}{subsection.9.2.3}%
\contentsline {subsection}{\numberline {9.2.4}Multi-Task Instruction PreTraining}{40}{subsection.9.2.4}%
\contentsline {section}{\numberline {9.3}模型选择与配置}{40}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}基座模型选择策略}{40}{subsection.9.3.1}%
\contentsline {subsection}{\numberline {9.3.2}数据输入格式要求}{40}{subsection.9.3.2}%
\contentsline {subsection}{\numberline {9.3.3}领域评测集构建}{40}{subsection.9.3.3}%
\contentsline {subsection}{\numberline {9.3.4}词表扩增必要性}{41}{subsection.9.3.4}%
\contentsline {section}{\numberline {9.4}训练实践与经验}{41}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}训练自己的大模型步骤}{41}{subsection.9.4.1}%
\contentsline {subsection}{\numberline {9.4.2}多轮对话微调方法}{41}{subsection.9.4.2}%
\contentsline {section}{\numberline {9.5}关键技术问题}{42}{section.9.5}%
\contentsline {subsection}{\numberline {9.5.1}灾难性遗忘问题}{42}{subsection.9.5.1}%
\contentsline {subsection}{\numberline {9.5.2}微调模型显存需求}{42}{subsection.9.5.2}%
\contentsline {subsection}{\numberline {9.5.3}SFT学习内容}{42}{subsection.9.5.3}%
\contentsline {section}{\numberline {9.6}训练优化技术}{43}{section.9.6}%
\contentsline {subsection}{\numberline {9.6.1}Batch Size设置问题}{43}{subsection.9.6.1}%
\contentsline {subsection}{\numberline {9.6.2}优化器选择}{43}{subsection.9.6.2}%
\contentsline {section}{\numberline {9.7}数据构建建议}{43}{section.9.7}%
\contentsline {subsection}{\numberline {9.7.1}预训练数据集选择}{43}{subsection.9.7.1}%
\contentsline {subsection}{\numberline {9.7.2}微调数据集构建原则}{43}{subsection.9.7.2}%
\contentsline {section}{\numberline {9.8}Loss突刺问题分析}{43}{section.9.8}%
\contentsline {subsection}{\numberline {9.8.1}Loss突刺现象}{43}{subsection.9.8.1}%
\contentsline {subsection}{\numberline {9.8.2}Adam优化器与Loss突刺}{44}{subsection.9.8.2}%
\contentsline {subsection}{\numberline {9.8.3}Loss突刺解决方案}{44}{subsection.9.8.3}%
\contentsline {chapter}{\numberline {第十章\hspace {.3em}}LLMs 训练经验帖}{45}{chapter.10}%
\contentsline {section}{\numberline {10.1}分布式训练框架选择}{45}{section.10.1}%
\contentsline {section}{\numberline {10.2}LLMs 训练实用建议}{45}{section.10.2}%
\contentsline {subsection}{\numberline {10.2.1}弹性容错和自动重启机制}{45}{subsection.10.2.1}%
\contentsline {subsection}{\numberline {10.2.2}定期保存模型}{45}{subsection.10.2.2}%
\contentsline {subsection}{\numberline {10.2.3}规划训练目标}{45}{subsection.10.2.3}%
\contentsline {subsection}{\numberline {10.2.4}关注GPU使用效率}{45}{subsection.10.2.4}%
\contentsline {subsection}{\numberline {10.2.5}训练框架选择影响}{46}{subsection.10.2.5}%
\contentsline {subsection}{\numberline {10.2.6}环境配置注意事项}{46}{subsection.10.2.6}%
\contentsline {subsection}{\numberline {10.2.7}系统底层库升级谨慎性}{46}{subsection.10.2.7}%
\contentsline {section}{\numberline {10.3}模型规模选择策略}{46}{section.10.3}%
\contentsline {section}{\numberline {10.4}加速卡选择建议}{46}{section.10.4}%
\contentsline {chapter}{\numberline {第十一章\hspace {.3em}}大模型(LLMs) LangChain面}{47}{chapter.11}%
\contentsline {section}{\numberline {11.1}LangChain 基础概念}{47}{section.11.1}%
\contentsline {subsection}{\numberline {11.1.1}什么是LangChain？}{47}{subsection.11.1.1}%
\contentsline {subsection}{\numberline {11.1.2}LangChain Agent}{47}{subsection.11.1.2}%
\contentsline {section}{\numberline {11.2}LangChain 核心概念}{47}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Components and Chains}{47}{subsection.11.2.1}%
\contentsline {subsection}{\numberline {11.2.2}Prompt Templates and Values}{47}{subsection.11.2.2}%
\contentsline {subsection}{\numberline {11.2.3}Example Selectors}{48}{subsection.11.2.3}%
\contentsline {subsection}{\numberline {11.2.4}Output Parsers}{48}{subsection.11.2.4}%
\contentsline {subsection}{\numberline {11.2.5}Indexes and Retrievers}{48}{subsection.11.2.5}%
\contentsline {subsection}{\numberline {11.2.6}Chat Message History}{48}{subsection.11.2.6}%
\contentsline {subsection}{\numberline {11.2.7}Agents and Toolkits}{48}{subsection.11.2.7}%
\contentsline {section}{\numberline {11.3}LangChain 功能特性}{48}{section.11.3}%
\contentsline {subsection}{\numberline {11.3.1}主要功能}{48}{subsection.11.3.1}%
\contentsline {subsection}{\numberline {11.3.2}LangChain 模型类型}{49}{subsection.11.3.2}%
\contentsline {subsection}{\numberline {11.3.3}LangChain 特点}{49}{subsection.11.3.3}%
\contentsline {section}{\numberline {11.4}LangChain 使用示例}{49}{section.11.4}%
\contentsline {subsection}{\numberline {11.4.1}调用LLMs生成回复}{49}{subsection.11.4.1}%
\contentsline {subsection}{\numberline {11.4.2}修改提示模板}{50}{subsection.11.4.2}%
\contentsline {subsection}{\numberline {11.4.3}链接多个组件处理任务}{50}{subsection.11.4.3}%
\contentsline {subsection}{\numberline {11.4.4}Embedding \& Vector Store}{51}{subsection.11.4.4}%
\contentsline {section}{\numberline {11.5}LangChain 问题与解决方案}{51}{section.11.5}%
\contentsline {subsection}{\numberline {11.5.1}低效的令牌使用问题}{51}{subsection.11.5.1}%
\contentsline {subsection}{\numberline {11.5.2}文档问题}{52}{subsection.11.5.2}%
\contentsline {subsection}{\numberline {11.5.3}概念混淆问题}{52}{subsection.11.5.3}%
\contentsline {subsection}{\numberline {11.5.4}行为不一致问题}{52}{subsection.11.5.4}%
\contentsline {subsection}{\numberline {11.5.5}缺乏标准数据类型问题}{52}{subsection.11.5.5}%
\contentsline {section}{\numberline {11.6}LangChain 替代方案}{52}{section.11.6}%
\contentsline {subsection}{\numberline {11.6.1}LlamaIndex}{52}{subsection.11.6.1}%
\contentsline {subsection}{\numberline {11.6.2}Deepset Haystack}{52}{subsection.11.6.2}%
\contentsline {chapter}{\numberline {第十二章\hspace {.3em}}多轮对话中让AI保持长期记忆的8种优化方式篇}{53}{chapter.12}%
\contentsline {section}{\numberline {12.1}前言}{53}{section.12.1}%
\contentsline {section}{\numberline {12.2}Agent获取上下文对话信息的8种方式}{53}{section.12.2}%
\contentsline {subsection}{\numberline {12.2.1}获取全量历史对话}{53}{subsection.12.2.1}%
\contentsline {subsection}{\numberline {12.2.2}滑动窗口获取最近部分对话内容}{53}{subsection.12.2.2}%
\contentsline {subsection}{\numberline {12.2.3}获取历史对话中实体信息}{54}{subsection.12.2.3}%
\contentsline {subsection}{\numberline {12.2.4}利用知识图谱获取历史对话中的实体及其联系}{54}{subsection.12.2.4}%
\contentsline {subsection}{\numberline {12.2.5}对历史对话进行阶段性总结摘要}{55}{subsection.12.2.5}%
\contentsline {subsection}{\numberline {12.2.6}需要获取最新对话，又要兼顾较早历史对话}{55}{subsection.12.2.6}%
\contentsline {subsection}{\numberline {12.2.7}回溯最近和最关键的对话信息}{55}{subsection.12.2.7}%
\contentsline {subsection}{\numberline {12.2.8}基于向量检索对话信息}{55}{subsection.12.2.8}%
\contentsline {section}{\numberline {12.3}总结}{56}{section.12.3}%
\contentsline {chapter}{\numberline {第十三章\hspace {.3em}}基于LangChain RAG问答应用实战}{58}{chapter.13}%
\contentsline {section}{\numberline {13.1}前言}{58}{section.13.1}%
\contentsline {subsection}{\numberline {13.1.1}项目介绍}{58}{subsection.13.1.1}%
\contentsline {subsection}{\numberline {13.1.2}软件资源}{58}{subsection.13.1.2}%
\contentsline {section}{\numberline {13.2}环境搭建}{58}{section.13.2}%
\contentsline {subsection}{\numberline {13.2.1}环境配置}{58}{subsection.13.2.1}%
\contentsline {subsection}{\numberline {13.2.2}安装依赖}{58}{subsection.13.2.2}%
\contentsline {section}{\numberline {13.3}RAG问答应用实战}{59}{section.13.3}%
\contentsline {subsection}{\numberline {13.3.1}数据构建}{59}{subsection.13.3.1}%
\contentsline {subsection}{\numberline {13.3.2}本地数据加载}{59}{subsection.13.3.2}%
\contentsline {subsection}{\numberline {13.3.3}文档分割}{59}{subsection.13.3.3}%
\contentsline {subsection}{\numberline {13.3.4}向量化与数据入库}{60}{subsection.13.3.4}%
\contentsline {subsection}{\numberline {13.3.5}Prompt设计}{60}{subsection.13.3.5}%
\contentsline {subsection}{\numberline {13.3.6}RetrievalQAChain构建}{61}{subsection.13.3.6}%
\contentsline {subsection}{\numberline {13.3.7}高级用法}{61}{subsection.13.3.7}%
\contentsline {section}{\numberline {13.4}技术要点总结}{63}{section.13.4}%
\contentsline {subsection}{\numberline {13.4.1}核心组件}{63}{subsection.13.4.1}%
\contentsline {subsection}{\numberline {13.4.2}优化建议}{63}{subsection.13.4.2}%
\contentsline {subsection}{\numberline {13.4.3}扩展应用}{63}{subsection.13.4.3}%
\contentsline {chapter}{\numberline {第十四章\hspace {.3em}}基于LLM+向量库的文档对话经验面}{64}{chapter.14}%
\contentsline {section}{\numberline {14.1}基础理论}{64}{section.14.1}%
\contentsline {subsection}{\numberline {14.1.1}为什么大模型需要外挂(向量)知识库？}{64}{subsection.14.1.1}%
\contentsline {subsection}{\numberline {14.1.2}基于LLM+向量库的文档对话思路}{64}{subsection.14.1.2}%
\contentsline {subsection}{\numberline {14.1.3}核心技术：Embedding}{64}{subsection.14.1.3}%
\contentsline {subsection}{\numberline {14.1.4}Prompt模板构建}{65}{subsection.14.1.4}%
\contentsline {section}{\numberline {14.2}优化问题与解决方案}{65}{section.14.2}%
\contentsline {subsection}{\numberline {14.2.1}痛点1：文档切分粒度不好把控}{65}{subsection.14.2.1}%
\contentsline {subsubsection}{如何构建关键信息？}{65}{subsubsection*.28}%
\contentsline {subsection}{\numberline {14.2.2}痛点2：在垂直领域表现不佳}{66}{subsection.14.2.2}%
\contentsline {subsection}{\numberline {14.2.3}痛点3：LangChain内置问答分句效果不佳}{66}{subsection.14.2.3}%
\contentsline {subsection}{\numberline {14.2.4}痛点4：如何尽可能召回与query相关的Document}{66}{subsection.14.2.4}%
\contentsline {subsection}{\numberline {14.2.5}痛点5：如何让LLM基于query和context得到高质量的response}{67}{subsection.14.2.5}%
\contentsline {subsection}{\numberline {14.2.6}痛点6：Embedding模型在表示text chunks时偏差太大}{67}{subsection.14.2.6}%
\contentsline {subsection}{\numberline {14.2.7}痛点7：不同的prompt产生完全不同的效果}{67}{subsection.14.2.7}%
\contentsline {subsection}{\numberline {14.2.8}痛点8：LLM生成效果问题}{67}{subsection.14.2.8}%
\contentsline {subsection}{\numberline {14.2.9}痛点9：如何更高质量地召回context喂给LLM}{67}{subsection.14.2.9}%
\contentsline {section}{\numberline {14.3}工程实践与避坑指南}{67}{section.14.3}%
\contentsline {subsection}{\numberline {14.3.1}本地知识库问答系统（Langchain-chatGLM）}{67}{subsection.14.3.1}%
\contentsline {subsubsection}{环境配置问题解决}{67}{subsubsection*.29}%
\contentsline {subsubsection}{PDF加载问题解决}{68}{subsubsection*.30}%
\contentsline {subsubsection}{NLTK数据包问题解决}{68}{subsubsection*.31}%
\contentsline {subsubsection}{PaddleOCR错误解决}{68}{subsubsection*.32}%
\contentsline {subsubsection}{MOSS模型加载错误解决}{68}{subsubsection*.33}%
\contentsline {subsubsection}{MOSS提问错误解决}{68}{subsubsection*.34}%
\contentsline {section}{\numberline {14.4}技术要点总结}{69}{section.14.4}%
\contentsline {subsection}{\numberline {14.4.1}核心架构设计}{69}{subsection.14.4.1}%
\contentsline {subsection}{\numberline {14.4.2}关键优化建议}{69}{subsection.14.4.2}%
\contentsline {subsection}{\numberline {14.4.3}工程实践建议}{69}{subsection.14.4.3}%
\contentsline {chapter}{\numberline {第十五章\hspace {.3em}}大模型 RAG 经验面}{70}{chapter.15}%
\contentsline {section}{\numberline {15.1}LLMs 的不足与挑战}{70}{section.15.1}%
\contentsline {subsection}{\numberline {15.1.1}LLMs 存在的不足点}{70}{subsection.15.1.1}%
\contentsline {section}{\numberline {15.2}RAG 技术概述}{70}{section.15.2}%
\contentsline {subsection}{\numberline {15.2.1}什么是 RAG？}{70}{subsection.15.2.1}%
\contentsline {subsection}{\numberline {15.2.2}RAG 核心组件}{70}{subsection.15.2.2}%
\contentsline {subsubsection}{检索器模块（R）}{70}{subsubsection*.35}%
\contentsline {subsubsection}{生成器模块（G）}{71}{subsubsection*.36}%
\contentsline {section}{\numberline {15.3}RAG 的优势}{71}{section.15.3}%
\contentsline {section}{\numberline {15.4}RAG 与 SFT 对比}{72}{section.15.4}%
\contentsline {section}{\numberline {15.5}RAG 典型实现方法}{73}{section.15.5}%
\contentsline {subsection}{\numberline {15.5.1}数据索引构建}{73}{subsection.15.5.1}%
\contentsline {subsection}{\numberline {15.5.2}数据检索策略}{73}{subsection.15.5.2}%
\contentsline {subsection}{\numberline {15.5.3}文本生成与回复}{74}{subsection.15.5.3}%
\contentsline {section}{\numberline {15.6}RAG 典型案例}{74}{section.15.6}%
\contentsline {subsection}{\numberline {15.6.1}ChatPDF 及其复刻版}{74}{subsection.15.6.1}%
\contentsline {subsection}{\numberline {15.6.2}Baichuan 搜索增强系统}{74}{subsection.15.6.2}%
\contentsline {subsection}{\numberline {15.6.3}多模态检索增强模型}{75}{subsection.15.6.3}%
\contentsline {section}{\numberline {15.7}RAG 存在的问题与挑战}{75}{section.15.7}%
\contentsline {chapter}{\numberline {第十六章\hspace {.3em}}LLM文档对话PDF解析关键问题}{76}{chapter.16}%
\contentsline {section}{\numberline {16.1}PDF解析的必要性}{76}{section.16.1}%
\contentsline {subsection}{\numberline {16.1.1}为什么需要进行PDF解析？}{76}{subsection.16.1.1}%
\contentsline {subsection}{\numberline {16.1.2}PDF解析的重要性}{76}{subsection.16.1.2}%
\contentsline {section}{\numberline {16.2}PDF解析方法与区别}{76}{section.16.2}%
\contentsline {subsection}{\numberline {16.2.1}PDF解析的两条技术路线}{76}{subsection.16.2.1}%
\contentsline {section}{\numberline {16.3}PDF解析存在的问题}{77}{section.16.3}%
\contentsline {section}{\numberline {16.4}长文档关键信息提取方法}{77}{section.16.4}%
\contentsline {section}{\numberline {16.5}标题提取的重要性与方法}{77}{section.16.5}%
\contentsline {subsection}{\numberline {16.5.1}为什么要提取标题甚至是多级标题？}{77}{subsection.16.5.1}%
\contentsline {subsection}{\numberline {16.5.2}如何提取文章标题？}{78}{subsection.16.5.2}%
\contentsline {section}{\numberline {16.6}单双栏PDF的处理}{78}{section.16.6}%
\contentsline {subsection}{\numberline {16.6.1}区分单双栏PDF与重新排序}{78}{subsection.16.6.1}%
\contentsline {section}{\numberline {16.7}表格和图片数据提取}{79}{section.16.7}%
\contentsline {subsection}{\numberline {16.7.1}表格和图片数据提取思路}{79}{subsection.16.7.1}%
\contentsline {section}{\numberline {16.8}基于AI的文档解析优缺点}{79}{section.16.8}%
\contentsline {subsection}{\numberline {16.8.1}基于AI的文档解析优缺点分析}{79}{subsection.16.8.1}%
\contentsline {section}{\numberline {16.9}总结与建议}{79}{section.16.9}%
\contentsline {subsection}{\numberline {16.9.1}技术建议}{79}{subsection.16.9.1}%
\contentsline {subsection}{\numberline {16.9.2}实践要点总结}{79}{subsection.16.9.2}%
\contentsline {subsection}{\numberline {16.9.3}未来发展方向}{80}{subsection.16.9.3}%
\contentsline {chapter}{\numberline {第十七章\hspace {.3em}}大模型(LLMs)RAG版面分析表格识别方法篇}{81}{chapter.17}%
\contentsline {section}{\numberline {17.1}表格识别的必要性}{81}{section.17.1}%
\contentsline {subsection}{\numberline {17.1.1}为什么需要识别表格？}{81}{subsection.17.1.1}%
\contentsline {section}{\numberline {17.2}表格识别任务概述}{81}{section.17.2}%
\contentsline {subsection}{\numberline {17.2.1}表格识别任务定义}{81}{subsection.17.2.1}%
\contentsline {section}{\numberline {17.3}表格识别方法分类}{82}{section.17.3}%
\contentsline {subsection}{\numberline {17.3.1}传统方法}{82}{subsection.17.3.1}%
\contentsline {subsection}{\numberline {17.3.2}pdfplumber表格抽取}{82}{subsection.17.3.2}%
\contentsline {subsubsection}{pdfplumber表格抽取原理}{82}{subsubsection*.38}%
\contentsline {subsubsection}{pdfplumber常见的表格抽取模式}{82}{subsubsection*.39}%
\contentsline {subsection}{\numberline {17.3.3}深度学习方法-语义分割}{83}{subsection.17.3.3}%
\contentsline {subsubsection}{table-ocr/table-detect}{83}{subsubsection*.40}%
\contentsline {subsubsection}{腾讯表格图像识别}{83}{subsubsection*.41}%
\contentsline {subsubsection}{TableNet}{83}{subsubsection*.42}%
\contentsline {subsubsection}{CascadeTabNet}{83}{subsubsection*.43}%
\contentsline {subsubsection}{SPLERGE}{83}{subsubsection*.44}%
\contentsline {subsubsection}{DeepDeSRT}{83}{subsubsection*.45}%
\contentsline {section}{\numberline {17.4}方法比较与应用建议}{84}{section.17.4}%
\contentsline {subsection}{\numberline {17.4.1}各类方法优缺点比较}{84}{subsection.17.4.1}%
\contentsline {subsection}{\numberline {17.4.2}实际应用建议}{84}{subsection.17.4.2}%
\contentsline {section}{\numberline {17.5}技术挑战与发展趋势}{84}{section.17.5}%
\contentsline {subsection}{\numberline {17.5.1}当前主要挑战}{84}{subsection.17.5.1}%
\contentsline {subsection}{\numberline {17.5.2}未来发展趋势}{84}{subsection.17.5.2}%
\contentsline {chapter}{\numberline {第十八章\hspace {.3em}}大模型(LLMs)RAG版面分析-文本分块面}{85}{chapter.18}%
\contentsline {section}{\numberline {18.1}文本分块的必要性}{85}{section.18.1}%
\contentsline {subsection}{\numberline {18.1.1}为什么需要对文本分块？}{85}{subsection.18.1.1}%
\contentsline {section}{\numberline {18.2}常见的文本分块方法}{85}{section.18.2}%
\contentsline {subsection}{\numberline {18.2.1}一般的文本分块方法}{85}{subsection.18.2.1}%
\contentsline {subsection}{\numberline {18.2.2}正则拆分的文本分块方法}{86}{subsection.18.2.2}%
\contentsline {subsection}{\numberline {18.2.3}Spacy Text Splitter方法}{87}{subsection.18.2.3}%
\contentsline {subsection}{\numberline {18.2.4}基于langchain的CharacterTextSplitter方法}{87}{subsection.18.2.4}%
\contentsline {subsection}{\numberline {18.2.5}基于langchain的递归字符切分方法}{88}{subsection.18.2.5}%
\contentsline {subsection}{\numberline {18.2.6}HTML文本拆分方法}{89}{subsection.18.2.6}%
\contentsline {subsection}{\numberline {18.2.7}Markdown文本拆分方法}{90}{subsection.18.2.7}%
\contentsline {subsection}{\numberline {18.2.8}Python代码拆分方法}{91}{subsection.18.2.8}%
\contentsline {subsection}{\numberline {18.2.9}LaTex文本拆分方法}{91}{subsection.18.2.9}%
\contentsline {section}{\numberline {18.3}文本分块实践建议}{93}{section.18.3}%
\contentsline {subsection}{\numberline {18.3.1}分块策略选择}{93}{subsection.18.3.1}%
\contentsline {subsection}{\numberline {18.3.2}分块参数调优建议}{93}{subsection.18.3.2}%
\contentsline {subsection}{\numberline {18.3.3}不同文档类型的推荐分块方法}{93}{subsection.18.3.3}%
\contentsline {chapter}{\numberline {第十九章\hspace {.3em}}大模型外挂知识库优化：利用大模型辅助召回}{94}{chapter.19}%
\contentsline {section}{\numberline {19.1}引言：为什么需要大模型辅助召回？}{94}{section.19.1}%
\contentsline {section}{\numberline {19.2}策略一：HYDE（Hypothetical Document Embeddings）}{94}{section.19.2}%
\contentsline {subsection}{\numberline {19.2.1}HYDE 基本介绍}{94}{subsection.19.2.1}%
\contentsline {subsection}{\numberline {19.2.2}HYDE 思路详解}{94}{subsection.19.2.2}%
\contentsline {subsection}{\numberline {19.2.3}HYDE 存在的问题与局限性}{95}{subsection.19.2.3}%
\contentsline {section}{\numberline {19.3}策略二：FLARE（Forward-Looking Active REtrieval）}{95}{section.19.3}%
\contentsline {subsection}{\numberline {19.3.1}FLARE 基本介绍}{95}{subsection.19.3.1}%
\contentsline {subsection}{\numberline {19.3.2}为什么需要 FLARE？}{96}{subsection.19.3.2}%
\contentsline {subsection}{\numberline {19.3.3}FLARE 召回策略}{96}{subsection.19.3.3}%
\contentsline {subsubsection}{传统多次召回方案}{96}{subsubsection*.47}%
\contentsline {subsection}{\numberline {19.3.4}FLARE 策略1：主动召回标识}{96}{subsection.19.3.4}%
\contentsline {subsubsection}{策略1思路}{96}{subsubsection*.48}%
\contentsline {subsubsection}{策略1缺陷与解决方案}{96}{subsubsection*.49}%
\contentsline {subsection}{\numberline {19.3.5}FLARE 策略2：基于置信度的召回}{97}{subsection.19.3.5}%
\contentsline {subsubsection}{策略2思路}{97}{subsubsection*.50}%
\contentsline {section}{\numberline {19.4}技术对比与总结}{97}{section.19.4}%
\contentsline {subsection}{\numberline {19.4.1}方法优势比较}{97}{subsection.19.4.1}%
\contentsline {subsection}{\numberline {19.4.2}实践建议}{97}{subsection.19.4.2}%
\contentsline {subsection}{\numberline {19.4.3}未来发展方向}{98}{subsection.19.4.3}%
