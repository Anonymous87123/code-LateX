
\chapter{大模型 RAG 经验面}

\section{LLMs 的不足与挑战}

\subsection{LLMs 存在的不足点}
在LLM已经具备了较强能力的基础上，仍然存在以下问题：

\begin{itemize}
\item \textbf{幻觉问题}：LLM文本生成的底层原理是基于概率的token by token的形式，因此会不可避免地产生"一本正经的胡说八道"的情况

\item \textbf{时效性问题}：LLM的规模越大，大模型训练的成本越高，周期也就越长。那么具有时效性的数据也就无法参与训练，所以也就无法直接回答时效性相关的问题，例如"帮我推荐几部热映的电影?"

\item \textbf{数据安全问题}：通用的LLM没有企业内部数据和用户数据，那么企业想要在保证安全的前提下使用LLM，最好的方式就是把数据全部放在本地，企业数据的业务计算全部在本地完成。而在线的大模型仅仅完成一个归纳的功能
\end{itemize}

\section{RAG 技术概述}

\subsection{什么是 RAG？}
RAG（Retrieval Augmented Generation，检索增强生成），即LLM在回答问题或生成文本时，先会从大量文档中检索出相关的信息，然后基于这些信息生成回答或文本，从而提高预测质量。

\subsection{RAG 核心组件}

\subsubsection{检索器模块（R）}
在RAG技术中，"R"代表检索，其作用是从大量知识库中检索出最相关的前k个文档。构建高质量的检索器面临三个关键挑战：

\textbf{2.1.1 如何获得准确的语义表示？}
在RAG中，语义空间指的是查询和文档被映射的多维空间。构建准确语义空间的方法：

\begin{itemize}
\item \textbf{块优化}：处理外部文档的第一步是分块，以获得更细致的特征。选择分块策略时需要考虑被索引内容的特点、使用的嵌入模型及其最适块大小、用户查询的预期长度和复杂度

\item \textbf{微调嵌入模型}：在确定Chunk的适当大小后，通过嵌入模型将Chunk和查询嵌入。优秀的嵌入模型如UAE、Voyage、BGE等，它们在大规模语料库上预训练过
\end{itemize}

\textbf{2.1.2 如何协调查询和文档的语义空间？}
协调用户的查询与文档的语义空间的技术：

\begin{itemize}
\item \textbf{查询重写}：利用大语言模型的能力生成指导性伪文档，或将原始查询与伪文档结合形成新查询。多查询检索方法让大语言模型能够同时产生多个搜索查询

\item \textbf{嵌入变换}：通过在查询编码器后加入特殊适配器并微调，优化查询的嵌入表示。SANTA方法让检索系统能够理解并处理结构化的信息
\end{itemize}

\textbf{2.1.3 如何对齐检索模型的输出和大语言模型的偏好？}
对齐方法：

\begin{itemize}
\item \textbf{大语言模型的监督训练}：REPLUG使用检索模型和大语言模型计算检索到的文档的概率分布，然后通过计算KL散度进行监督训练

\item \textbf{适配器附加}：在检索模型上外部附加适配器来实现对齐，避免微调嵌入模型的挑战

\item \textbf{指令微调}：PKG通过指令微调将知识注入到白盒模型中，直接替换检索模块
\end{itemize}

\subsubsection{生成器模块（G）}
\textbf{2.2.1 生成器介绍}
\begin{itemize}
\item \textbf{作用}：将检索到的信息转化为自然流畅的文本。输入不仅包括传统的上下文信息，还有通过检索器得到的相关文本片段

\item \textbf{特点}：能够更深入地理解问题背后的上下文，并产生更加信息丰富的回答。根据检索到的文本来指导内容的生成，确保一致性
\end{itemize}

\textbf{2.2.2 后检索处理提升策略}
\begin{itemize}
\item \textbf{目的}：提高检索结果的质量，更好地满足用户需求或为后续任务做准备

\item \textbf{策略}：包括信息压缩和结果的重新排序
\end{itemize}

\textbf{2.2.3 生成器优化方法}
\begin{itemize}
\item \textbf{优化目的}：确保生成文本既流畅又能有效利用检索文档，更好地回应用户的查询

\item \textbf{方法}：对检索器找到的文档进行后续处理，微调方式与大语言模型的普通微调方法大体相同
\end{itemize}

\section{RAG 的优势}

使用RAG的好处包括：

\begin{itemize}
\item \textbf{可扩展性}：减少模型大小和训练成本，允许轻松扩展知识

\item \textbf{准确性}：通过引用信息来源，用户可以核实答案的准确性，增强对模型输出结果的信任

\item \textbf{可控性}：允许更新或定制知识

\item \textbf{可解释性}：检索到的项目作为模型预测中来源的参考

\item \textbf{多功能性}：可以针对多种任务进行微调和定制，包括QA、文本摘要、对话系统等

\item \textbf{及时性}：使用检索技术能识别到最新的信息，保持回答的及时性和准确性

\item \textbf{定制性}：通过索引与特定领域相关的文本语料库，为不同领域提供专业的知识支持

\item \textbf{安全性}：通过数据库中设置的角色和安全控制，实现对数据使用的更好控制
\end{itemize}

\section{RAG 与 SFT 对比}

\begin{table}[h]
\centering
\caption{RAG与SFT对比分析}
\begin{tabular}{@{}p{0.25\textwidth}p{0.35\textwidth}p{0.35\textwidth}@{}}
\toprule
\textbf{维度} & \textbf{RAG} & \textbf{SFT} \\
\midrule
数据 & 动态数据。RAG不断查询外部源，确保信息保持最新，而无需频繁的模型重新训练 & （相对）静态数据，并且在动态数据场景中可能很快就会过时。SFT也不能保证记住这些知识 \\
外部知识 & RAG擅长利用外部资源。通过在生成响应之前从知识源检索相关信息来增强LLM能力。它非常适合文档或其他结构化/非结构化数据库 & SFT可以对LLM进行微调以对齐预训练学到的外部知识，但对于频繁更改的数据源来说可能不太实用 \\
模型定制 & RAG主要关注信息检索，擅长整合外部知识，但可能无法完全定制模型的行为或写作风格 & SFT允许根据特定的语气或术语调整LLM的行为、写作风格或特定领域的知识 \\
减少幻觉 & RAG本质上不太容易产生幻觉，因为每个回答都建立在检索到的证据上 & SFT可以通过将模型基于特定领域的训练数据来帮助减少幻觉。但当面对不熟悉的输入时，它仍然可能产生幻觉 \\
透明度 & RAG系统通过将响应生成分解为不同的阶段来提供透明度，提供对数据检索的匹配度以提高对输出的信任 & SFT就像一个黑匣子，使得响应背后的推理更加不透明 \\
技术专长 & RAG需要高效的检索策略和大型数据库相关技术。另外还需要保持外部数据源集成以及数据更新 & SFT需要准备和整理高质量的训练数据集、定义微调目标以及相应的计算资源 \\
\bottomrule
\end{tabular}
\end{table}

两种方法并非非此即彼，合理的方式是结合业务需要与两种方法的优点，合理使用两种方法。

\section{RAG 典型实现方法}

RAG的实现主要包括三个主要步骤：数据索引、检索和生成。

\subsection{数据索引构建}

数据索引是一个离线的过程，主要是将私域数据向量化后构建索引并存入数据库的过程。

\textbf{Step1：数据提取}
\begin{itemize}
\item \textbf{数据获取}：包括多格式数据（PDF、word、markdown以及数据库和API等）加载、不同数据源获取等

\item \textbf{Doc类文档}：直接解析得到文本元素及其属性，用于后续切分的依据

\item \textbf{PDF类文档}：使用多个开源模型进行协同分析，如版面分析使用百度的PP-StructureV2

\item \textbf{PPT类文档}：将PPT转换成PDF形式，然后用处理PDF的方式来进行解析

\item \textbf{数据清洗}：对源数据进行去重、过滤、压缩和格式化等处理

\item \textbf{信息提取}：提取数据中关键信息，包括文件名、时间、章节title、图片等信息
\end{itemize}

\textbf{Step2：文本分割（Chunking）}
\begin{itemize}
\item \textbf{动机}：由于文本可能较长，或者仅有部分内容相关的情况下，需要对文本进行分块切分

\item \textbf{考虑因素}：embedding模型的Tokens限制情况；语义完整性对整体的检索效果的影响

\item \textbf{分块方式}：
\begin{itemize}
\item 句分割：以"句"的粒度进行切分，保留一个句子的完整语义
\item 固定大小的分块方式：根据embedding模型的token长度限制，将文本分割为固定长度
\item 基于意图的分块方式：句分割、递归分割、特殊分割
\end{itemize}

\item \textbf{常用工具}：langchain.text\_splitter库中的CharacterTextSplitter类
\end{itemize}

\textbf{Step3：向量化及创建索引}
\begin{itemize}
\item \textbf{向量化}：将文本、图像、音频和视频等转化为向量矩阵的过程

\item \textbf{常见embedding模型}：ChatGPT-Embedding、ERNIE-Embedding V1、M3E、BGE

\item \textbf{创建索引}：数据向量化后构建索引，并写入数据库的过程

\item \textbf{常用工具}：FAISS、Chromadb、ES、milvus等

\item \textbf{选择考虑}：根据业务场景、硬件、性能需求等多因素综合考虑
\end{itemize}

\subsection{数据检索策略}

\textbf{检索思路}：
\begin{itemize}
\item \textbf{元数据过滤}：通过元数据先进行过滤，提升效率和相关度

\item \textbf{图关系检索}：引入知识图谱，利用知识之间的关系做更准确的回答

\item \textbf{检索技术}：
\begin{itemize}
\item 向量化相似度检索：使用欧氏距离、曼哈顿距离、余弦等计算方式
\item 关键词检索：传统检索方式，元数据过滤也是一种
\item 全文检索、SQL检索：传统检索算法
\end{itemize}

\item \textbf{重排序}：根据相关度、匹配度等因素重新调整，得到更符合业务场景的排序

\item \textbf{查询轮换}：
\begin{itemize}
\item 子查询：使用各种查询策略，如树查询、向量查询、顺序查询chunks等
\item HyDE：生成相似的或更标准的prompt模板
\end{itemize}
\end{itemize}

\subsection{文本生成与回复}

文本生成就是将原始query和检索得到的文本组合起来输入模型得到结果的过程，本质上就是prompt engineering过程。

\begin{lstlisting}[language=Python]
from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnablePassthrough

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
rag_chain = {"context": retriever, "question": RunnablePassthrough()} | rag_prompt | llm
rag_chain.invoke("What is Task Decomposition?")
\end{lstlisting}

全流程框架如Langchain和LlamaIndex，都非常简单易用。

\section{RAG 典型案例}

\subsection{ChatPDF 及其复刻版}
ChatPDF的实现流程：
\begin{enumerate}
\item 读取PDF文件，转换为可处理的文本格式（如txt格式）
\item 对提取出来的文本进行清理和标准化（去除特殊字符、分段、分句等）
\item 使用OpenAI的Embeddings API将每个分段转换为向量
\item 将用户问题转换为向量，并与每个分段的向量进行比较，找到最相似的分段
\item 将最相似的分段与问题作为prompt，调用OpenAI的Completion API
\item 将ChatGPT生成的答案返回给用户
\end{enumerate}

\subsection{Baichuan 搜索增强系统}
百川大模型的搜索增强系统融合模块：
\begin{itemize}
\item \textbf{指令意图理解}：深入理解用户指令
\item \textbf{智能搜索}：精确驱动查询词的搜索
\item \textbf{结果增强}：结合大语言模型技术来优化模型结果生成的可靠性
\end{itemize}

通过这一系列协同作用，实现更精确、智能的模型结果回答，减少模型的幻觉。

\subsection{多模态检索增强模型}
RA-CM3是一个检索增强的多模态模型：
\begin{itemize}
\item 使用预训练的CLIP模型实现检索器（retriever）
\item 使用CM3 Transformer架构构成生成器（generator）
\item 检索器辅助模型从外部存储库中搜索有关提示文本的精确信息
\item 将该信息连同文本送入生成器中进行图像合成
\item 设计的模型的准确性大大提高
\end{itemize}

\section{RAG 存在的问题与挑战}

RAG技术目前存在以下问题：

\begin{itemize}
\item \textbf{检索效果依赖}：检索效果依赖embedding和检索算法。目前可能检索到无关信息，反而对输出有负面影响

\item \textbf{黑盒利用}：大模型如何利用检索到的信息仍是黑盒的。可能仍存在不准确（甚至生成的文本与检索信息相冲突）

\item \textbf{效率问题}：对所有任务都无差别检索k个文本片段，效率不高，同时会大大增加模型输入的长度

\item \textbf{引用和验证困难}：无法引用来源，也因此无法精准地查证事实，检索的真实性取决于数据源及检索算法
\end{itemize}

