
\chapter{大模型(LLMs)RAG版面分析表格识别方法篇}

\section{表格识别的必要性}

\subsection{为什么需要识别表格？}
表格的尺寸、类型和样式展现出多样化的特征，如背景填充的差异性、行列合并方法的多样性以及内容文本类型的不一致性等。同时，现有的文档资料不仅涵盖了现代电子文档，也包括历史的手写扫描文档，这些文档在样式设计、光照条件以及纹理特性等方面存在显著差异。因此，表格识别一直是文档识别领域的重大挑战。

表格类型示例包括：
\begin{itemize}
\item 有颜色背景的全线表
\item 少线表
\item 无线表
\item 有复杂表格线条样式的表格
\item 拍照得到的手写历史文档
\end{itemize}

\section{表格识别任务概述}

\subsection{表格识别任务定义}
表格识别包括表格检测和表格结构识别两个子任务。

表格识别过程可细分为两个关键步骤：

\textbf{表格定位（Table Localization）}：
\begin{itemize}
\item 涉及识别并划定表格的整体边界
\item 采用的技术手段包括目标检测算法，如YOLO、Faster RCNN或Mask RCNN
\item 有时借助生成对抗网络（GAN）来精确勾勒表格的外在轮廓
\end{itemize}

\textbf{表格元素解析与结构重建（Table Element Parsing and Structure Reconstruction）}：
\begin{itemize}
\item \textbf{表格单元格划分（Cell Detection）}：识别和区分表格内部的各个单元格
\item \textbf{表格结构理解（Table Structure Understanding）}：分析表格区域以提取数据内容及其内在逻辑关系
\end{itemize}

\section{表格识别方法分类}

\subsection{传统方法}
利用规则指导和图像处理技术，执行以下步骤识别结构：
\begin{enumerate}
\item 应用腐蚀与膨胀算法来细化和增强目标区域边界特征
\item 通过分析像素连通性，确定并标记图像中的各个显著区域
\item 实施线段检测和直线拟合技术，精确描绘图像内的线性结构元素
\item 计算线性结构之间的交点，构建可能的边框或连接关系网络
\item 合并初步检测到的边界框（猜测框），运用智能合并策略减少冗余并提高精度
\item 根据尺寸筛选优化，剔除不符合预期大小条件的候选区域
\end{enumerate}

\subsection{pdfplumber表格抽取}

\subsubsection{pdfplumber表格抽取原理}
\begin{enumerate}
\item 找到可见的或猜测出不可见的候选表格线
\item 根据候选表格线确定它们的交点，找到围成的最小单元格
\item 把连通的单元格整合到一起，生成检测出的表格对象
\end{enumerate}

\subsubsection{pdfplumber常见的表格抽取模式}

\textbf{lattice抽取线框类的表格}：
\begin{enumerate}
\item 把PDF页面转换成图像
\item 通过图像处理检测出水平方向和竖直方向的直线
\item 根据检测出的直线生成可能表格的bounding box
\item 确定表格各行、列的区域
\item 解析表格结构，填充单元格内容，形成表格对象
\end{enumerate}

\textbf{stream抽取非线框类的表格}：
\begin{enumerate}
\item 通过pdfminer获取连续字符串（串行）
\item 通过文本对齐的方式确定可能表格的bounding box（文本块）
\item 确定表格各行、列的区域
\item 解析表格结构，填充单元格内容，形成表格对象
\end{enumerate}

\subsection{深度学习方法-语义分割}

\subsubsection{table-ocr/table-detect}
\begin{itemize}
\item \textbf{table-ocr}：运用unet实现对文档表格的自动检测和表格重建
\item \textbf{table-detect}：使用YOLO进行表格检测，unet进行表格单元格定位
\end{itemize}

\subsubsection{腾讯表格图像识别}
\begin{itemize}
\item \textbf{思路}：图像分割，分割类别为4类（横向线、竖向线、横向不可见线、竖向不可见线）
\item \textbf{模型}：对比DeepLab系列、FCN、Unet、SegNet等，Unet收敛最快
\end{itemize}

\subsubsection{TableNet}
\begin{itemize}
\item \textbf{论文}：《TableNet: Deep Learning Model for End-to-end Table Detection and Tabular Data Extraction from Scanned Document Images》
\item \textbf{架构}：基于编码器-解码器模型，使用预训练VGG-19网络
\item \textbf{数据集}：马莫特数据集（包含中文页面）
\item \textbf{效果}：微调后模型的召回率0.9628、精度0.9697、F1得分0.9662
\end{itemize}

\subsubsection{CascadeTabNet}
\begin{itemize}
\item \textbf{方法}：基于端到端深度学习，使用级联掩码R-CNN HRNet模型
\item \textbf{优点}：
\begin{enumerate}
\item 提出级联网络进行表检测和结构识别
\item 端到端解决表格检测和识别两个子任务
\item 用实例分割提高表检测精度
\item 采用两阶段迁移学习策略，适用小数据集
\end{enumerate}
\end{itemize}

\subsubsection{SPLERGE}
\begin{itemize}
\item \textbf{论文名称}：Deep Splitting and Merging for Table Structure Decomposition
\item \textbf{思想}：先自顶向下、再自底向上的两阶段表格结构识别方法
\item \textbf{流程}：
\begin{itemize}
\item Split部分：把表格区域分割成网格状结构
\item Merge部分：对Split结果中的邻接网格对进行合并预测
\end{itemize}
\end{itemize}

\subsubsection{DeepDeSRT}
\begin{itemize}
\item \textbf{论文名称}：DeepDeSRT: Deep Learning for Detection and Structure Recognition of Tables in Document Images
\item \textbf{思路}：提供基于深度学习的表格检测和表结构识别解决方案
\item \textbf{结构}：
\begin{itemize}
\item 表格检测：使用快速RCNN作为基本框架
\item 结构识别：使用全连接网络与VGG-16权重提取行列信息
\end{itemize}
\item \textbf{数据集}：ICDAR 2013表竞争数据集
\end{itemize}

\section{方法比较与应用建议}

\subsection{各类方法优缺点比较}
\begin{itemize}
\item \textbf{传统方法}：计算量小，但对复杂表格效果有限
\item \textbf{pdfplumber}：适合规则表格，对扫描文档效果一般
\item \textbf{深度学习方法}：准确率高，但需要大量标注数据和计算资源
\end{itemize}

\subsection{实际应用建议}
\begin{enumerate}
\item 根据表格复杂程度选择合适的方法
\item 考虑计算资源和时间成本
\item 对于重要应用，建议采用深度学习方法
\item 可以组合使用多种方法提高准确率
\end{enumerate}

\section{技术挑战与发展趋势}

\subsection{当前主要挑战}
\begin{itemize}
\item 复杂表格结构的准确识别
\item 手写和历史文档的处理
\item 多语言表格的识别
\item 实时处理性能优化
\end{itemize}

\subsection{未来发展趋势}
\begin{itemize}
\item 更强大的端到端识别模型
\item 少样本和零样本学习技术
\item 多模态信息融合
\item 云端一体化解决方案
\end{itemize}


\chapter{大模型(LLMs)RAG版面分析-文本分块面}

\section{文本分块的必要性}

\subsection{为什么需要对文本分块？}
使用大型语言模型(LLM)时，切勿忽略文本分块的重要性，其对处理结果的好坏有重大影响。

考虑以下场景：你面临一个几百页的文档，其中充满了文字，你希望对其进行摘录和问答式处理。在这个流程中，最初的一步是提取文档的嵌入向量，但这样做会带来几个问题：

\begin{itemize}
\item \textbf{信息丢失的风险}：试图一次性提取整个文档的嵌入向量，虽然可以捕捉到整体的上下文，但也可能会忽略掉许多针对特定主题的重要信息，这可能会导致生成的信息不够精确或者有所缺失

\item \textbf{分块大小的限制}：在使用如OpenAI这样的模型时，分块大小是一个关键的限制因素。例如，GPT-4模型有一个32K的窗口大小限制。尽管这个限制在大多数情况下不是问题，但从一开始就考虑到分块大小是很重要的
\end{itemize}

因此，恰当地实施文本分块不仅能够提升文本的整体品质和可读性，还能够预防由于信息丢失或不当分块引起的问题。这就是为何在处理长篇文档时，采用文本分块而非直接处理整个文档至关重要的原因。

\section{常见的文本分块方法}

\subsection{一般的文本分块方法}
如果不借助任何包，直接按限制长度切分方案：

\begin{lstlisting}[language=Python]
text = "我是一个名为ChatGLM3-6B的人工智能助手，是基于清华大学KEG实验室和智谱AI公司于2023年共同训练的语言模型开发的。我的目标是通过回答用户提出的问题来帮助他们解决问题。由于我是一个计算机程序，所以我没有实际的存在，只能通过互联网来与用户交流。"

chunks = []
chunk_size = 128
for i in range(0, len(text), chunk_size):
    chunk = text[i:i + chunk_size]
    chunks.append(chunk)

chunks
\end{lstlisting}

输出结果：
\begin{verbatim}
['我是一个名为ChatGLM3-6B的人工智能助手，是基于清华大学KEG实验室和智谱AI公
司于2023年共同训练的语言模型开发的。我的目标是通过回答用户提出的问题来帮助他们
解决问题。由于我是一个计算机程序，所以我没有实际的存在，只能通过互联网',
'来与用户交流。']
\end{verbatim}

\subsection{正则拆分的文本分块方法}
\textbf{动机}：一般的文本分块方法能够按长度进行分割，但是对于一些长度偏长的句子，容易从中间切开

\textbf{方法}：在中文文本分块的场景中，正则表达式可以用来识别中文标点符号，从而将文本拆分成单独的句子。这种方法依赖于中文句号、"问号"、"感叹号"等标点符号作为句子结束的标志。

\textbf{特点}：虽然这种基于模式匹配的方法可能不如基于复杂语法和语义分析的方法精确，但它在大多数情况下足以满足基本的句子分割需求，并且实现起来更为简单直接。

\begin{lstlisting}[language=Python]
import re

def split_sentences(text):
    # 使用正则表达式匹配中文句子结束的标点符号
    sentence_delimiters = re.compile(u'[。?!;]|\n')
    sentences = sentence_delimiters.split(text)
    # 过滤掉空字符串
    sentences = [s.strip() for s in sentences if s.strip()]
    return sentences

text = "文本分块是自然语言处理(NLP)中的一项关键技术，其作用是将较长的文本切割成更小、更易于处理的片段。这种分割通常是基于单词的词性和语法结构，例如将文本拆分为名词短语、动词短语或其他语义单位。这样做有助于更高效地从文本中提取关键信息。"
sentences = split_sentences(text)
print(sentences)
\end{lstlisting}

输出结果：
\begin{lstlisting}
['文本分块是自然语言处理(NLP)中的一项关键技术,其作用是将较长的文本切割成更小、更易于处理的片段',
'这种分割通常是基于单词的词性和语法结构,例如将文本拆分为名词短语、动词短语或其他语义单位',
'这样做有助于更高效地从文本中提取关键信息']
\end{lstlisting}

在上面例子中，我们并没有采用任何特定的方式来分割句子。另外，还有许多其他的文本分块技术可以使用，例如词汇化(tokenizing)、词性标注(POS tagging)等。

\subsection{Spacy Text Splitter方法}
\textbf{介绍}：Spacy是一个用于执行自然语言处理(NLP)各种任务的库。它具有文本拆分器功能，能够在进行文本分割的同时，保留分割结果的上下文信息。

\begin{lstlisting}[language=Python]
import spacy

input_text = "文本分块是自然语言处理(NLP)中的一项关键技术，其作用是将较长的文本切割成更小、更易于处理的片段。这种分割通常是基于单词的词性和语法结构，例如将文本拆分为名词短语、动词短语或其他语义单位。这样做有助于更高效地从文本中提取关键信息。"
nlp = spacy.load("zh_core_web_sm")
doc = nlp(input_text)
for s in doc.sents:
    print(s)
\end{lstlisting}

输出结果：
\begin{lstlisting}
文本分块是自然语言处理(NLP)中的一项关键技术，其作用是将较长的文本切割成更小、更易于处理的片段。
这种分割通常是基于单词的词性和语法结构，例如将文本拆分为名词短语、动词短语或其他语义单位。
这样做有助于更高效地从文本中提取关键信息。
\end{lstlisting}

\subsection{基于langchain的CharacterTextSplitter方法}
使用CharacterTextSplitter，一般的设置参数为：chunk\_size、chunk\_overlap、separator和strip\_whitespace。

\begin{lstlisting}[language=Python]
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=35, 
    chunk_overlap=0,
    separator='', 
    strip_whitespace=False
)
text_splitter.create_documents([text])
\end{lstlisting}

输出结果：
\begin{lstlisting}
[Document(page_content='我是一个名为ChatGLM3-6B的人工智能助手，是基于清华大学'),
 Document(page_content='KEG实验室和智谱AI公司于2023年共同训练的语言模型开发'),
 Document(page_content='的。我的目标是通过回答用户提出的问题来帮助他们解决问题。由于我是一个计'),
 Document(page_content='算机程序，所以我没有实际的存在，只能通过互联网来与用户交流。')]
\end{lstlisting}

\subsection{基于langchain的递归字符切分方法}
使用RecursiveCharacterTextSplitter，一般的设置参数为：chunk\_size、chunk\_overlap。

\begin{lstlisting}[language=Python]
# input text
input_text = "文本分块是自然语言处理(NLP)中的一项关键技术，其作用是将较长的文本切割成更小、更易于处理的片段。这种分割通常是基于单词的词性和语法结构，例如将文本拆分为名词短语、动词短语或其他语义单位。这样做有助于更高效地从文本中提取关键信息。"

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,  # 设置所需的文本大小
    chunk_overlap=20
)
chunks = text_splitter.create_documents([input_text])
print(chunks)
\end{lstlisting}

输出结果：
\begin{lstlisting}
[Document(page_content='文本分块是自然语言处理(NLP)中的一项关键技术，其作用是将较长的文本切割成更小、更易于处理的片段。这种分割通常是基于单词的词性和语法结构，例如将文本拆分为名词短语、动词短语或其他语义单位。这样做有助'),
 Document(page_content='短语、动词短语或其他语义单位。这样做有助于更高效地从文本中提取关键信息。')]
\end{lstlisting}

与CharacterTextSplitter不同，RecursiveCharacterTextSplitter不需要设置分隔符，默认的几个分隔符如下：
\begin{verbatim}
"\n\n" - 两个换行符，一般认为是段落分隔符
"\n"   - 换行符
" "    - 空格
""     - 字符
\end{verbatim}

拆分器首先查找两个换行符（段落分隔符）。一旦段落被分割，它就会查看块的大小，如果块太大，那么它会被下一个分隔符分割。如果块仍然太大，那么它将移动到下一个块上，以此类推。

\subsection{HTML文本拆分方法}
\textbf{介绍}：HTML文本拆分器是一种结构感知的文本分块工具。它能够在HTML元素级别上进行文本拆分，并且会为每个分块添加与之相关的标题元数据。

\textbf{特点}：对HTML结构的敏感性，能够精准地处理和分析HTML文档中的内容。

\begin{lstlisting}[language=Python]
# input html string
html_string = """
<!DOCTYPE html>
<html>
<body>
<div>
<h1>Mobot</h1>
<p>一些关于Mobot的介绍文字。</p>
<div> <h2>Mobot主要部分</h2><p>有关Mobot的一些介绍文本。</p><h3>Mobot第1小节</h3><p>有关Mobot第一个子主题的一些文本。</p><h3>Mobot第2小节</h3><p>关于Mobot的第二个子主题的一些文字。</p></div><div><h2>Mobot</h2><p>关于Mobot的一些文字</p></div><br><p>关于Mobot的一些结论性文字</p></div></body></html>"""

headers_to_split_on = [("h1", "Header 1"), ("h2", "标题2"), ("h3", "标题3")]

from langchain.text_splitter import HTMLHeaderTextSplitter
html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
html_header_splits = html_splitter.split_text(html_string)
print(html_header_splits)
\end{lstlisting}

输出结果：
\begin{lstlisting}
[Document(page_content='Mobot'),
 Document(page_content='一些关于Mobot的介绍文字。\nMobot主Mobot第2小节', metadata={'Header 1':'Mobot'}),
 Document(page_content='有关Mobot的一些介绍文本。', metadata={'Header 1':'Mobot','标题 2':'Mobot主要部分'}),
 Document(page_content='有关Mobot第一个子主题的一些文本。', metadata={'Header 1':'Mobot','标题 2':'Mobot主要部分','标题 3':'Mobot第1小节'}),
 Document(page_content='关于Mobot的第二个子主题的一些文字。', metadata={'Header 1':'Mobot','标题 2':'Mobot主要部分','标题 3':'Mobot第2小节'}),
 Document(page_content='Mobot div>', metadata={'Header 1':'Mobot'}),
 Document(page_content='关于Mobot的一些文字\n关于Mobot的一些结论性文字', metadata={'Header 1':'Mobot','标题 2':'Mobot'})]
\end{lstlisting}

仅提取在header\_to\_split\_on参数中指定的HTML标题。

\subsection{Markdown文本拆分方法}
\textbf{介绍}：Markdown文本拆分是一种根据Markdown的语法规则（例如标题、Bash代码块、图片和列表）进行文本分块的方法。

\textbf{特点}：具有对结构的敏感性，能够基于Markdown文档的结构特点进行有效的文本分割。

\begin{lstlisting}[language=Python]
markdown_text = '# Mobot\n\n## Stone\n\n这是python\n这是\n\n## markdown\n\n这是中文文本拆分'

from langchain.text_splitter import MarkdownHeaderTextSplitter

headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
md_header_splits = markdown_splitter.split_text(markdown_text)
print(md_header_splits)
\end{lstlisting}

输出结果：
\begin{lstlisting}
[Document(page_content='这是python\n这是', metadata={'Header 1':'Mobot','Header 2':'Stone'}),
 Document(page_content='这是中文文本拆分', metadata={'Header 1':'Mobot','Header 2':'markdown'})]
\end{lstlisting}

MarkdownHeaderTextSplitter能够根据设定的headers\_to\_split\_on参数，将Markdown文本进行拆分。这一功能使得用户可以便捷地根据指定的标题将Markdown文件分割成不同部分，从而提高编辑和管理的效率。

\subsection{Python代码拆分方法}
\begin{lstlisting}[language=Python]
python_text = """
class Person:
    def __init__(self, name, age):
        self.name = name
        self.age = age

p1 = Person("John", 36)

for i in range(10):
    print(i)
"""

from langchain.text_splitter import PythonCodeTextSplitter
python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)
python_splitter.create_documents([python_text])
\end{lstlisting}

输出结果：
\begin{lstlisting}
[Document(page_content='class Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age'),
 Document(page_content='p1 = Person("John", 36)\n\nfor i in range(10):\n    print(i)')]
\end{lstlisting}

\subsection{LaTex文本拆分方法}
LaTex文本拆分工具是一种专用于代码分块的工具。它通过解析LaTex命令来创建各个块，这些块按照逻辑组织，如章节和小节等。这种方式能够产生更加准确且与上下文相关的分块结果，从而有效地提升LaTex文档的组织和处理效率。

\begin{lstlisting}[language=Python]
# input Latex string
latex_text = """
\documentclass{article}
\begin{document}
\maketitle
\section{Introduction}
大型语言模型(LLM)是一种机器学习模型，可以在大量文本数据上进行训练，以生成类似人类的语言。近年来，法学硕士在各种自然语言处理任务中取得了重大进展，包括语言翻译、文本生成和情感分析。
\subsection{法学硕士的历史}
最早的法学硕士是在20世纪80年代开发的和20世纪90年代，但它们受到可处理的数据量和当时可用的计算能力的限制。然而，在过去的十年中，硬件和软件的进步使得在海量数据集上训练法学硕士成为可能，从而导致
\subsection{LLM的应用}
LLM在工业界有许多应用，包括聊天机器人、内容创建和虚拟助理。它们还可以在学术界用于语言学、心理学和计算语言学的研究。
\end{document}
"""

from langchain.text_splitter import LatexTextSplitter
latex_splitter = LatexTextSplitter(chunk_size=100, chunk_overlap=0)
latex_splits = latex_splitter.create_documents([latex_text])
print(latex_splits)
\end{lstlisting}

输出结果：
\begin{lstlisting}
[Document(page_content='\\documentclass{article}\\begin{document}\\maketitle\\section{Introduction} 大型语言模型(LLM)'),
 Document(page_content='是一种机器学习模型，可以在大量文本数据上进行训练，以生成类似人类的语言。近年来,法学硕士在各种自然语言处理任务中取得了重大进展,包括语言翻译、文本生成和情感分析。\\subsection{法学硕士的历史}'),
 Document(page_content='}最早的法学硕士是在'),
 Document(page_content='20世纪80年代开发的和20世纪90'),
 Document(page_content='年代，但它们受到可处理的数据量和当时可用的计算能力的限制。然而，在过去的十年中，硬件和软件的进步使得在海量数据集上训练法学硕士成为可能，从而导致\\subsection{LLM的应用}LLM'),
 Document(page_content='在工业界有许多应用，包括聊天机器人、内容创建和虚拟助理。它们还可以在学术界用于语言学、心理学和计算语言学的研究。\\end{document}')]
\end{lstlisting}

在上述示例中，我们注意到代码分割时的重叠部分设置为0。这是因为在处理代码分割过程中，任何重叠的代码都可能完全改变其原有含义。因此，为了保持代码的原始意图和准确性，避免产生误解或错误，设置重叠部分为0是必要的。

\section{文本分块实践建议}

\subsection{分块策略选择}
当你决定使用哪种分块器处理数据时，重要的一步是提取数据嵌入并将其存储在向量数据库(Vector DB)中。上面的例子中使用文本分块器结合LanceDB来存储数据块及其对应的嵌入。

LanceDB是一个无需配置、开源且无服务器的向量数据库，其数据持久化在硬盘驱动器上，允许用户在不超出预算的情况下实现扩展。此外，LanceDB与Python数据生态系统兼容，因此你可以将其与现有的数据工具（如pandas、pyarrow等）结合使用。

\subsection{分块参数调优建议}
\begin{itemize}
\item \textbf{chunk\_size选择}：根据具体任务和模型限制调整，一般建议在128-1024之间
\item \textbf{chunk\_overlap设置}：对于连续文本建议设置10-20\%的重叠，对于代码建议设置为0
\item \textbf{分隔符选择}：根据文档类型选择合适的分隔符
\end{itemize}

\subsection{不同文档类型的推荐分块方法}
\begin{itemize}
\item \textbf{普通文本}：RecursiveCharacterTextSplitter
\item \textbf{HTML文档}：HTMLHeaderTextSplitter  
\item \textbf{Markdown文档}：MarkdownHeaderTextSplitter
\item \textbf{代码文件}：专用代码拆分器（PythonCodeTextSplitter等）
\item \textbf{学术论文}：LatexTextSplitter
\end{itemize}

