\chapter{基础知识}
\section{大模型基础概念}

\textbf{大模型}：一般指1亿以上参数模型，但标准一直升级，目前已有万亿参数以上的模型。

\textbf{大语言模型(Large Language Model, LLM)}：针对语言的大模型。

参数规模：\textbf{175B、60B、540B等}，这些一般指参数的个数，B是Billion/十亿的意思，175B是1750亿参数，这是ChatGPT大约的参数规模。

\section{单双向注意力}
\subsection{核心概念：“阅读”和“写作”}
理解单双向注意力是掌握大模型架构差异的关键。我们可以用两个生动的比喻来理解：

\textbf{双向注意力：像阅读侦探小说}:想象你在阅读一本悬疑小说。为了理解复杂的情节，你会\textbf{随意地前后翻看}。当看到最后一章揭示凶手时，你可能会翻回前面的章节，查看某个角色的不在场证明是否有漏洞。这就是"双向"的精髓——任何一个部分的信息都可以参考全文的任何其他部分，获得全局的、最充分的理解。

\textbf{单向注意力：像写作小说续集}:现在你要为这本小说写续集。你只能\textbf{从左到右一个一个词地写}。在写下"侦探"这个词时，你只能基于前面已经写好的"突然，门开了，走进来一位…"来构思，你\textbf{不能提前知道或使用}后面将要写出的"掏出了手枪"这个词。这就是"单向"或"因果"的本质——每个新词只能基于它之前的所有词来生成。

\subsection{技术深度解析}
双向注意力机制:
\begin{itemize}
\item \textbf{目标}：深度\textbf{理解}和\textbf{编码}输入信息
\item \textbf{工作原理}：模型同时处理整个句子的所有词。当理解某个词（如代词"它"）时，可以\textbf{同时关注}该词左右两侧的所有上下文，从而准确判断指代关系
\item \textbf{优势}：文本理解能力极强，能把握复杂语义关系和指代消解
\item \textbf{局限}：不适合直接用于生成任务，否则会"作弊"（提前看到答案）
\item \textbf{典型代表}：BERT模型，主要用于文本分类、情感分析等理解型任务
\end{itemize}

单向注意力机制:
\begin{itemize}
\item \textbf{目标}：序列\textbf{生成}
\item \textbf{工作原理}：模型以自回归方式工作，每次只能基于当前和之前的词预测下一个词，严格遵循因果律
\item \textbf{优势}：天然适合文本生成任务，训练目标与实际应用完全一致，Zero-shot能力强，容易涌现新能力
\item \textbf{局限}：在纯理解任务上可能不如双向模型深入
\item \textbf{典型代表}：GPT系列、LLaMA系列
\end{itemize}

\subsection{单双向注意力对比总结}
\begin{table}[h]
\centering
\caption{单双向注意力机制对比}
\begin{tabular}{p{0.3\textwidth}p{0.3\textwidth}p{0.3\textwidth}}
\toprule
\textbf{特性} & \textbf{双向注意力} & \textbf{单向注意力（因果注意力）} \\
\midrule
\textbf{核心目标} & 理解与分析 & 生成与创作 \\
\textbf{信息流动} & 全局、无方向限制 & 从左到右、严格因果 \\
\textbf{形象比喻} & 阅读分析文章 & 写作口述文章 \\
\textbf{主要优势} & 深层语义理解、分类任务强 & 文本生成、零样本能力、涌现能力 \\
\textbf{主要局限} & 不直接适用于生成 & 理解任务可能缺少全局上下文 \\
\textbf{典型架构} & Encoder（编码器） & Decoder（解码器） \\
\textbf{代表模型} & BERT & GPT、LLaMA系列 \\
\bottomrule
\end{tabular}
\end{table}


\section{主流开源模型体系}
\subsection{三种主流体系}
\begin{itemize}
\item \textbf{Prefix Decoder系}
    \begin{itemize}
    \item 介绍：输入双向注意力，输出单向注意力
    \item 代表模型：ChatGLM、ChatGLM2、U-PaLM
    \end{itemize}

\item \textbf{Causal Decoder系}
    \begin{itemize}
    \item 介绍：从左到右的单向注意力
    \item 代表模型：LLaMA-7B、LLaMa衍生物
    \end{itemize}

\item \textbf{Encoder-Decoder系}
    \begin{itemize}
    \item 介绍：输入双向注意力，输出单向注意力
    \item 代表模型：T5、Flan-T5、BART y1y2
    \end{itemize}
\end{itemize}

\section{三种Decoder架构区别}
\subsection{核心区别}
主要区别在于attention mask不同：

\subsection{Encoder-Decoder架构}
\begin{itemize}
\item 在输入上采用双向注意力，对问题的编码理解更充分
\item 适用任务：在偏理解的NLP任务上效果好
\item 缺点：在长文本生成任务上效果差，训练效率低
\end{itemize}

\subsection{Causal Decoder架构}
\begin{itemize}
\item 自回归语言模型，预训练和下游应用是完全一致的，严格遵守只有后面的token才能看到前面的token的规则
\item 适用任务：文本生成任务效果好
\item 优点：训练效率高，zero-shot能力更强，具有涌现能力
\end{itemize}

\subsection{Prefix Decoder架构}
\begin{itemize}
\item 特点：prefix部分的token互相能看到，是Causal Decoder和Encoder-Decoder的折中
\item 缺点：训练效率低
\end{itemize}

\section{大模型训练目标}
\subsection{语言模型}
根据已有词预测下一个词，训练目标为最大似然函数：
\[
\mathcal{L}_{LM}(x)=\sum_{i=1}^{n}\log P(x_{i}|x_{<i})
\]
训练效率：Prefix Decoder $<$ Causal Decoder\\
Causal Decoder结构会在所有token上计算损失，而Prefix Decoder只会在输出上计算损失。

\subsection{去噪自编码器}
随机替换掉一些文本段，训练语言模型去恢复被打乱的文本段。目标函数为：
\[
\mathcal{L}_{DAE}(x)=\log P(\tilde{x}|x_{/\tilde{x}})
\]
去噪自编码器的实现难度更高。采用去噪自编码器作为训练目标的任务有GLM-130B、T5。

\section{涌现能力分析}
根据前人分析和论文总结，大致是2个猜想：
\begin{itemize}
\item 任务的评价指标不够平滑
\item 复杂任务vs子任务：假设某个任务T有5个子任务Sub-T构成，每个sub-T随着模型增长，指标从40\%提升到60\%，但是最终任务的指标只从1.1\%提升到了7\%，也就是说宏观上看到了涌现现象，但是子任务效果其实是平滑增长的
\end{itemize}

\section{Decoder Only架构优势}
\begin{itemize}
\item decoder-only结构模型在没有任何微调数据的情况下，zero-shot的表现能力最好
\item 而encoder-decoder则需要在一定量的标注数据上做multitask-finetuning才能够激发最佳性能
\item 目前的Large LM的训练范式还是在大规模语料上做自监督学习，zero-shot性能更好的decoder-only架构才能更好的利用这些无标注的数据
\item 大模型使用decoder-only架构除了训练效率和工程实现上的优势外，在理论上因为Encoder的双向注意力会存在低秩的问题，这可能会削弱模型的表达能力
\item 就生成任务而言，引入双向注意力并无实质的好处
\item Encoder-decoder模型架构之所以能够在某些场景下表现更好，大概是因为它多了一倍参数
\item 在同等参数量、同等推理成本下，Decoder-only架构就是最优的选择
\end{itemize}

\section{大模型优缺点分析}
\subsection{优点}
\begin{itemize}
\item 可以利用大量的无标注数据来训练一个通用的模型，然后再用少量的有标注数据来微调模型，以适应特定的任务。这种预训练和微调的方法可以减少数据标注的成本和时间，提高模型的泛化能力
\item 可以利用生成式人工智能技术来产生新颖和有价值的内容，例如图像、文本、音乐等。这种生成能力可以帮助用户在创意、娱乐、教育等领域获得更好的体验和效果
\item 可以利用涌现能力(Emergent Capabilities)来完成一些之前无法完成或者很难完成的任务，例如数学应用题、常识推理、符号操作等。这种涌现能力可以反映模型的智能水平和推理能力
\end{itemize}

\subsection{缺点}
\begin{itemize}
\item 需要消耗大量的计算资源和存储资源来训练和运行，这会增加经济和环境的负担。据估计，训练一个GPT-3模型需要消耗约30万美元，并产生约284吨二氧化碳排放
\item 需要面对数据质量和安全性的问题，例如数据偏见、数据泄露、数据滥用等。这些问题可能会导致模型产生不准确或不道德的输出，并影响用户或社会的利益
\item 需要考虑可解释性、可靠性、可持续性等方面的挑战，例如如何理解和控制模型的行为、如何保证模型的正确性和稳定性、如何平衡模型的效益和风险等。这些挑战需要多方面的研究和合作，以确保大模型能够健康地发展
\end{itemize}